{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable, grad\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "mnist_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "data_loader = DataLoader(mnist_data, batch_size=1000, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 784),\n",
    "            # Tanh activation because MNIST is normalized between -1 and 1\n",
    "            nn.Tanh()  \n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        img = self.model(noise)\n",
    "        # Reshape to 2D image for MNIST\n",
    "        img = img.view(img.size(0), 28, 28)  \n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "            # No Sigmoid or Tanh at the output for WGAN-GP\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 10  # Penalty coefficient\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Ensure alpha is shaped correctly for broadcasting\n",
    "    alpha = torch.rand(batch_size, 1, device=real_samples.device)\n",
    "    alpha = alpha.expand(batch_size, real_samples.nelement() // batch_size).contiguous().view(batch_size, -1)\n",
    "\n",
    "    # Ensure real_samples and fake_samples are flat\n",
    "    real_samples_flat = real_samples.view(batch_size, -1)\n",
    "    fake_samples_flat = fake_samples.view(batch_size, -1)\n",
    "\n",
    "    # Calculate interpolates\n",
    "    interpolates = (alpha * real_samples_flat + (1 - alpha) * fake_samples_flat).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    \n",
    "    fake = Variable(torch.Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False).to(real_samples.device)\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    # Flatten the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)  \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define the optimisers\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to track progress\n",
    "avg_d_loss = 0\n",
    "avg_g_loss = 0\n",
    "n_batches = len(data_loader)\n",
    "\n",
    "# Define the number of critic updates per generator update\n",
    "n_critic = 5  \n",
    "\n",
    "for epoch in range(200):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.view(imgs.size(0), -1)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        for _ in range(n_critic):  # Update the discriminator n_critic times\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = torch.randn(imgs.shape[0], 100)  # Ensure noise_dim matches generator input\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            # Real images\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            # Fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "            \n",
    "            # Wasserstein GAN loss w/ gradient penalty\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            avg_d_loss += d_loss.item() / n_critic  # Average over the n_critic updates\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        if i % n_critic == 0:  # Update the generator every n_critic steps\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            avg_g_loss += g_loss.item()\n",
    "\n",
    "        # Prints progress within the epoch\n",
    "        if (i+1) % 100 == 0:  # Print every 100 steps\n",
    "            print(f\"Epoch [{epoch+1}/{200}], Step [{i+1}/{n_batches}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Prints average loss per epoch\n",
    "    avg_d_loss /= n_batches\n",
    "    avg_g_loss /= n_batches\n",
    "    print(f\"Epoch [{epoch+1}/{200}] completed. Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "    # Resets average losses for the next epoch\n",
    "    avg_d_loss = 0\n",
    "    avg_g_loss = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise generations\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_noise = torch.randn(1000, 100)\n",
    "    generated_images = generator(test_noise).view(-1, 28, 28).cpu().data.numpy()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(4, 4), sharey=True, sharex=True)\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(generated_images[i], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the model\n",
    "MODEL_PATH = '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/Saved_Generator_States/generator_state_dict.pth'\n",
    "\n",
    "# Save the generator's state dictionary\n",
    "torch.save(generator.state_dict(), MODEL_PATH)\n",
    "\n",
    "# # Instantiate the model\n",
    "# generator = Generator()\n",
    "\n",
    "# # Load the state dictionary\n",
    "# generator.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "# # Set the model to evaluation mode if you are doing inference only\n",
    "# generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimiser state\n",
    "OPTIMIZER_PATH = '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/Saved_Optimiser_States/optimizer_G_state_dict.pth'\n",
    "torch.save(optimizer_G.state_dict(), OPTIMIZER_PATH)\n",
    "\n",
    "# To load the optimiser state, first instantiate the optimiser, then load the state dictionary\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "optimizer_G.load_state_dict(torch.load(OPTIMIZER_PATH))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
