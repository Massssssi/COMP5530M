{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Development - Kochems Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from data_reader import DataReader\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ask_bid_int(dataset):\n",
    "    mask_ask = np.char.endswith(dataset[:,:,2], 'ask')\n",
    "    mask_bid = np.char.endswith(dataset[:,:,2], 'bid')\n",
    "    dataset[:,:,2][mask_ask] = '1'\n",
    "    dataset[:,:,2][mask_bid] = '0'\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "def get_dataset_max_price(dataset):\n",
    "    last_row_prices = dataset[:, 0, 0]\n",
    "    max_val = np.max(last_row_prices)\n",
    "    return max_val\n",
    "\n",
    "def get_dataset_min_price(dataset):\n",
    "    first_row_prices = dataset[:, -1, 0]\n",
    "    min_val = np.min(first_row_prices)\n",
    "    return min_val\n",
    "\n",
    "def make_histogram_from_dataset(dataset, bin_width = 0.5):\n",
    "    X_train = []\n",
    "    hist_max = get_dataset_max_price(dataset)\n",
    "    hist_min = get_dataset_min_price(dataset)\n",
    "    print(\"range: \", hist_min, \" \", hist_max)\n",
    "    num_bins = int(np.ceil((hist_max-hist_min) / bin_width))\n",
    "    bins = np.linspace(hist_min, hist_max, num_bins)\n",
    "    for i in range(len(dataset)):\n",
    "        orderbook = dataset[i];\n",
    "        price = orderbook[:,0]\n",
    "        quantity = orderbook[:,1]\n",
    "        quantity[orderbook[:, 2] == 0] *= -1\n",
    "        hist, bin_edges = np.histogram(price, bins=bins, weights=quantity)\n",
    "        X_train.append(hist)\n",
    "    X_train = np.array(X_train)\n",
    "    return X_train\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3639, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "data_reader = DataReader(\"orderbook_snapshots.csv\", rows_per_orderbook=100)\n",
    "data_reader.read_csv()\n",
    "X_train_raw = data_reader.get_data()\n",
    "X_train_raw = convert_ask_bid_int(X_train_raw)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:  2286.1   2316.7\n",
      "(3639, 61)\n"
     ]
    }
   ],
   "source": [
    "X_train = make_histogram_from_dataset(X_train_raw, bin_width=0.5)\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "# print(X_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries and Load Data (Use my script that transforms the Orderbook csv file into 1-row-per-timestamp format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the data\n",
    "\n",
    "# Input and output file names\n",
    "# TODO (CHANGE THESE FROM MINE TO YOURS)\n",
    "input_filename = '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/training_data/orderbook_snapshots.csv'\n",
    "output_filename = '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/training_data/reformatted.csv'\n",
    "\n",
    "# Initialise a dictionary to hold the data in correct order\n",
    "orders = defaultdict(lambda: {'ask_price': None, 'ask_qty': None, 'bid_price': None, 'bid_qty': None})\n",
    "\n",
    "# Read original csv and populate the orders dictionary\n",
    "with open(input_filename, mode='r', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        timestamp, _, price, quantity, order_type = row\n",
    "        if order_type == '0_ask':\n",
    "            orders[timestamp]['ask_price'] = price\n",
    "            orders[timestamp]['ask_qty'] = quantity\n",
    "        elif order_type == '0_bid':\n",
    "            orders[timestamp]['bid_price'] = price\n",
    "            orders[timestamp]['bid_qty'] = quantity\n",
    "\n",
    "# Write the data to a new csv file\n",
    "with open(output_filename, mode='w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Timestamp', 'Best Ask Price Level', 'Best Ask Quantity', 'Best Bid Price Level', 'Best Bid Quantity'])\n",
    "    for timestamp, order in orders.items():\n",
    "        writer.writerow([timestamp, order['ask_price'], order['ask_qty'], order['bid_price'], order['bid_qty']])\n",
    "\n",
    "\n",
    "data_path = output_filename\n",
    "columns = ['timestamp', 'best_ask_price', 'best_ask_qty', 'best_bid_price', 'best_bid_qty']\n",
    "data = pd.read_csv(data_path, header=None, names=columns, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to a numerical format\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "data['timestamp'] = (data['timestamp'] - data['timestamp'].min()).dt.total_seconds()\n",
    "\n",
    "# Normalize the price and quantity columns\n",
    "scaler = MinMaxScaler()\n",
    "data[['best_ask_price', 'best_ask_qty', 'best_bid_price', 'best_bid_qty']] = \\\n",
    "    scaler.fit_transform(data[['best_ask_price', 'best_ask_qty', 'best_bid_price', 'best_bid_qty']])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.tensor(data.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. GAN Architecture (Generator & Discriminator Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 128),  # Assuming the noise vector size is 2 and conditional input size is 1\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 4)  # Output size is 4 (best_ask_price, best_ask_qty, best_bid_price, best_bid_qty)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, price_level):\n",
    "        # Given that price_level is already [batch_size, 1], no need to unsqueeze\n",
    "        # print(\"Noise shape:\", noise.shape)\n",
    "        # print(\"Price level shape before concat:\", price_level.shape)  # Adjusted print statement for clarity\n",
    "        \n",
    "        # Concatenate noise and price_level directly\n",
    "        x = torch.cat([noise, price_level], dim=1)  # Both tensors should now be compatible for concat\n",
    "        \n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, snapshot):\n",
    "        return self.model(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Instantiate Models & Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define the optimisers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Step [1/149], D Loss: 1.3965988159179688, G Loss: 0.7215413451194763\n",
      "Epoch [1/200], Step [1601/149], D Loss: 1.4688373804092407, G Loss: 0.5217849016189575\n",
      "Epoch [1/200], Step [3201/149], D Loss: 1.3653583526611328, G Loss: 0.9481761455535889\n",
      "Epoch [1/200], Step [4801/149], D Loss: 1.3138500452041626, G Loss: 1.0709651708602905\n",
      "Epoch [1/200], Step [6401/149], D Loss: 1.1183561086654663, G Loss: 1.2066458463668823\n",
      "Epoch [1/200], Step [8001/149], D Loss: 1.4968788623809814, G Loss: 0.562454879283905\n",
      "Epoch [2/200], Step [1/149], D Loss: 1.1029284000396729, G Loss: 0.7625342607498169\n",
      "Epoch [2/200], Step [1601/149], D Loss: 1.0214810371398926, G Loss: 0.7176451086997986\n",
      "Epoch [2/200], Step [3201/149], D Loss: 1.2565090656280518, G Loss: 0.7303485870361328\n",
      "Epoch [2/200], Step [4801/149], D Loss: 1.4943289756774902, G Loss: 0.712454080581665\n",
      "Epoch [2/200], Step [6401/149], D Loss: 1.3722593784332275, G Loss: 0.9504902958869934\n",
      "Epoch [2/200], Step [8001/149], D Loss: 1.2274872064590454, G Loss: 1.0198336839675903\n",
      "Epoch [3/200], Step [1/149], D Loss: 1.5418180227279663, G Loss: 0.8265911936759949\n",
      "Epoch [3/200], Step [1601/149], D Loss: 1.4887192249298096, G Loss: 0.5643711090087891\n",
      "Epoch [3/200], Step [3201/149], D Loss: 1.3858758211135864, G Loss: 0.5473932027816772\n",
      "Epoch [3/200], Step [4801/149], D Loss: 1.4481775760650635, G Loss: 0.6243922114372253\n",
      "Epoch [3/200], Step [6401/149], D Loss: 1.4883657693862915, G Loss: 0.6696312427520752\n",
      "Epoch [3/200], Step [8001/149], D Loss: 1.3403637409210205, G Loss: 0.7970128655433655\n",
      "Epoch [4/200], Step [1/149], D Loss: 1.4273743629455566, G Loss: 0.9959892630577087\n",
      "Epoch [4/200], Step [1601/149], D Loss: 1.4438573122024536, G Loss: 0.6129456758499146\n",
      "Epoch [4/200], Step [3201/149], D Loss: 1.3306294679641724, G Loss: 0.5887195467948914\n",
      "Epoch [4/200], Step [4801/149], D Loss: 1.4354270696640015, G Loss: 0.6233205199241638\n",
      "Epoch [4/200], Step [6401/149], D Loss: 1.4999144077301025, G Loss: 0.6605672240257263\n",
      "Epoch [4/200], Step [8001/149], D Loss: 1.3350226879119873, G Loss: 0.8522425889968872\n",
      "Epoch [5/200], Step [1/149], D Loss: 1.4737422466278076, G Loss: 0.9733959436416626\n",
      "Epoch [5/200], Step [1601/149], D Loss: 1.478318214416504, G Loss: 0.5921642780303955\n",
      "Epoch [5/200], Step [3201/149], D Loss: 1.3378909826278687, G Loss: 0.5731551051139832\n",
      "Epoch [5/200], Step [4801/149], D Loss: 1.4183785915374756, G Loss: 0.6120321750640869\n",
      "Epoch [5/200], Step [6401/149], D Loss: 1.5058419704437256, G Loss: 0.6447444558143616\n",
      "Epoch [5/200], Step [8001/149], D Loss: 1.3527216911315918, G Loss: 0.8327416777610779\n",
      "Epoch [6/200], Step [1/149], D Loss: 1.47100031375885, G Loss: 0.9431843757629395\n",
      "Epoch [6/200], Step [1601/149], D Loss: 1.4715261459350586, G Loss: 0.6070951223373413\n",
      "Epoch [6/200], Step [3201/149], D Loss: 1.3480422496795654, G Loss: 0.5770220756530762\n",
      "Epoch [6/200], Step [4801/149], D Loss: 1.413792610168457, G Loss: 0.6122045516967773\n",
      "Epoch [6/200], Step [6401/149], D Loss: 1.476217269897461, G Loss: 0.6588889956474304\n",
      "Epoch [6/200], Step [8001/149], D Loss: 1.364916443824768, G Loss: 0.8023185729980469\n",
      "Epoch [7/200], Step [1/149], D Loss: 1.483957052230835, G Loss: 0.87701416015625\n",
      "Epoch [7/200], Step [1601/149], D Loss: 1.4377105236053467, G Loss: 0.6220413446426392\n",
      "Epoch [7/200], Step [3201/149], D Loss: 1.3387470245361328, G Loss: 0.5870442986488342\n",
      "Epoch [7/200], Step [4801/149], D Loss: 1.431351661682129, G Loss: 0.6111010909080505\n",
      "Epoch [7/200], Step [6401/149], D Loss: 1.4587819576263428, G Loss: 0.6857094764709473\n",
      "Epoch [7/200], Step [8001/149], D Loss: 1.34393310546875, G Loss: 0.850683867931366\n",
      "Epoch [8/200], Step [1/149], D Loss: 1.5239211320877075, G Loss: 0.8618802428245544\n",
      "Epoch [8/200], Step [1601/149], D Loss: 1.4480547904968262, G Loss: 0.6034820675849915\n",
      "Epoch [8/200], Step [3201/149], D Loss: 1.3106417655944824, G Loss: 0.5871778130531311\n",
      "Epoch [8/200], Step [4801/149], D Loss: 1.4583897590637207, G Loss: 0.5832162499427795\n",
      "Epoch [8/200], Step [6401/149], D Loss: 1.4957709312438965, G Loss: 0.6533610820770264\n",
      "Epoch [8/200], Step [8001/149], D Loss: 1.32562255859375, G Loss: 0.9007506370544434\n",
      "Epoch [9/200], Step [1/149], D Loss: 1.5347459316253662, G Loss: 0.918877363204956\n",
      "Epoch [9/200], Step [1601/149], D Loss: 1.470545768737793, G Loss: 0.6057355403900146\n",
      "Epoch [9/200], Step [3201/149], D Loss: 1.3206958770751953, G Loss: 0.5899497866630554\n",
      "Epoch [9/200], Step [4801/149], D Loss: 1.4319143295288086, G Loss: 0.5894498825073242\n",
      "Epoch [9/200], Step [6401/149], D Loss: 1.4931738376617432, G Loss: 0.6188607215881348\n",
      "Epoch [9/200], Step [8001/149], D Loss: 1.3597227334976196, G Loss: 0.7938252687454224\n",
      "Epoch [10/200], Step [1/149], D Loss: 1.4549593925476074, G Loss: 0.9065290689468384\n",
      "Epoch [10/200], Step [1601/149], D Loss: 1.457045078277588, G Loss: 0.6389849185943604\n",
      "Epoch [10/200], Step [3201/149], D Loss: 1.3259196281433105, G Loss: 0.6285527348518372\n",
      "Epoch [10/200], Step [4801/149], D Loss: 1.4286541938781738, G Loss: 0.6167551875114441\n",
      "Epoch [10/200], Step [6401/149], D Loss: 1.4627625942230225, G Loss: 0.6530729532241821\n",
      "Epoch [10/200], Step [8001/149], D Loss: 1.3116159439086914, G Loss: 0.8535208106040955\n",
      "Epoch [11/200], Step [1/149], D Loss: 1.531496524810791, G Loss: 0.8022162318229675\n",
      "Epoch [11/200], Step [1601/149], D Loss: 1.4125772714614868, G Loss: 0.6285322904586792\n",
      "Epoch [11/200], Step [3201/149], D Loss: 1.2915986776351929, G Loss: 0.6104676723480225\n",
      "Epoch [11/200], Step [4801/149], D Loss: 1.4670252799987793, G Loss: 0.5881510376930237\n",
      "Epoch [11/200], Step [6401/149], D Loss: 1.4839155673980713, G Loss: 0.6710330247879028\n",
      "Epoch [11/200], Step [8001/149], D Loss: 1.317456603050232, G Loss: 0.8857527375221252\n",
      "Epoch [12/200], Step [1/149], D Loss: 1.5259833335876465, G Loss: 0.880484938621521\n",
      "Epoch [12/200], Step [1601/149], D Loss: 1.4566421508789062, G Loss: 0.6323205828666687\n",
      "Epoch [12/200], Step [3201/149], D Loss: 1.3395237922668457, G Loss: 0.6076258420944214\n",
      "Epoch [12/200], Step [4801/149], D Loss: 1.4097908735275269, G Loss: 0.6164442300796509\n",
      "Epoch [12/200], Step [6401/149], D Loss: 1.4433071613311768, G Loss: 0.6485795974731445\n",
      "Epoch [12/200], Step [8001/149], D Loss: 1.3950121402740479, G Loss: 0.7153177857398987\n",
      "Epoch [13/200], Step [1/149], D Loss: 1.4331660270690918, G Loss: 0.7881066203117371\n",
      "Epoch [13/200], Step [1601/149], D Loss: 1.3911700248718262, G Loss: 0.6649895906448364\n",
      "Epoch [13/200], Step [3201/149], D Loss: 1.3437072038650513, G Loss: 0.617764949798584\n",
      "Epoch [13/200], Step [4801/149], D Loss: 1.4599905014038086, G Loss: 0.6332014203071594\n",
      "Epoch [13/200], Step [6401/149], D Loss: 1.3676059246063232, G Loss: 0.79413902759552\n",
      "Epoch [13/200], Step [8001/149], D Loss: 1.3124639987945557, G Loss: 0.8740310072898865\n",
      "Epoch [14/200], Step [1/149], D Loss: 1.6023106575012207, G Loss: 0.7893394231796265\n",
      "Epoch [14/200], Step [1601/149], D Loss: 1.462815523147583, G Loss: 0.6052623391151428\n",
      "Epoch [14/200], Step [3201/149], D Loss: 1.3210210800170898, G Loss: 0.6043576002120972\n",
      "Epoch [14/200], Step [4801/149], D Loss: 1.3984217643737793, G Loss: 0.6147395968437195\n",
      "Epoch [14/200], Step [6401/149], D Loss: 1.4708210229873657, G Loss: 0.6303961277008057\n",
      "Epoch [14/200], Step [8001/149], D Loss: 1.400323510169983, G Loss: 0.7054380178451538\n",
      "Epoch [15/200], Step [1/149], D Loss: 1.4239590167999268, G Loss: 0.780381441116333\n",
      "Epoch [15/200], Step [1601/149], D Loss: 1.3821845054626465, G Loss: 0.6875575184822083\n",
      "Epoch [15/200], Step [3201/149], D Loss: 1.36204195022583, G Loss: 0.6335021257400513\n",
      "Epoch [15/200], Step [4801/149], D Loss: 1.4557604789733887, G Loss: 0.6226423978805542\n",
      "Epoch [15/200], Step [6401/149], D Loss: 1.3833179473876953, G Loss: 0.7361797094345093\n",
      "Epoch [15/200], Step [8001/149], D Loss: 1.3059382438659668, G Loss: 0.8567302823066711\n",
      "Epoch [16/200], Step [1/149], D Loss: 1.5747486352920532, G Loss: 0.8141366243362427\n",
      "Epoch [16/200], Step [1601/149], D Loss: 1.5000603199005127, G Loss: 0.6101431846618652\n",
      "Epoch [16/200], Step [3201/149], D Loss: 1.3568358421325684, G Loss: 0.6086845993995667\n",
      "Epoch [16/200], Step [4801/149], D Loss: 1.3873789310455322, G Loss: 0.626798152923584\n",
      "Epoch [16/200], Step [6401/149], D Loss: 1.4395904541015625, G Loss: 0.6267406344413757\n",
      "Epoch [16/200], Step [8001/149], D Loss: 1.3986272811889648, G Loss: 0.657067596912384\n",
      "Epoch [17/200], Step [1/149], D Loss: 1.3423542976379395, G Loss: 0.7068725228309631\n",
      "Epoch [17/200], Step [1601/149], D Loss: 1.3442639112472534, G Loss: 0.6695070862770081\n",
      "Epoch [17/200], Step [3201/149], D Loss: 1.35548996925354, G Loss: 0.6283640265464783\n",
      "Epoch [17/200], Step [4801/149], D Loss: 1.5049957036972046, G Loss: 0.6267751455307007\n",
      "Epoch [17/200], Step [6401/149], D Loss: 1.368227243423462, G Loss: 0.827235758304596\n",
      "Epoch [17/200], Step [8001/149], D Loss: 1.264064073562622, G Loss: 0.9858680963516235\n",
      "Epoch [18/200], Step [1/149], D Loss: 1.7025939226150513, G Loss: 0.8099085092544556\n",
      "Epoch [18/200], Step [1601/149], D Loss: 1.5110617876052856, G Loss: 0.6168042421340942\n",
      "Epoch [18/200], Step [3201/149], D Loss: 1.3725277185440063, G Loss: 0.6089739203453064\n",
      "Epoch [18/200], Step [4801/149], D Loss: 1.3728196620941162, G Loss: 0.6337468028068542\n",
      "Epoch [18/200], Step [6401/149], D Loss: 1.418806552886963, G Loss: 0.6385369300842285\n",
      "Epoch [18/200], Step [8001/149], D Loss: 1.3960769176483154, G Loss: 0.652847409248352\n",
      "Epoch [19/200], Step [1/149], D Loss: 1.3189496994018555, G Loss: 0.6813632249832153\n",
      "Epoch [19/200], Step [1601/149], D Loss: 1.3008248805999756, G Loss: 0.6622641682624817\n",
      "Epoch [19/200], Step [3201/149], D Loss: 1.3210620880126953, G Loss: 0.6242218017578125\n",
      "Epoch [19/200], Step [4801/149], D Loss: 1.5545284748077393, G Loss: 0.6042529940605164\n",
      "Epoch [19/200], Step [6401/149], D Loss: 1.4421459436416626, G Loss: 0.7531909346580505\n",
      "Epoch [19/200], Step [8001/149], D Loss: 1.2526836395263672, G Loss: 1.008701205253601\n",
      "Epoch [20/200], Step [1/149], D Loss: 1.5668888092041016, G Loss: 0.9087615013122559\n",
      "Epoch [20/200], Step [1601/149], D Loss: 1.512918472290039, G Loss: 0.6486574411392212\n",
      "Epoch [20/200], Step [3201/149], D Loss: 1.409918189048767, G Loss: 0.5964313745498657\n",
      "Epoch [20/200], Step [4801/149], D Loss: 1.398332118988037, G Loss: 0.6132727861404419\n",
      "Epoch [20/200], Step [6401/149], D Loss: 1.4005897045135498, G Loss: 0.6448513865470886\n",
      "Epoch [20/200], Step [8001/149], D Loss: 1.3834433555603027, G Loss: 0.6710485219955444\n",
      "Epoch [21/200], Step [1/149], D Loss: 1.375509262084961, G Loss: 0.686729907989502\n",
      "Epoch [21/200], Step [1601/149], D Loss: 1.3507403135299683, G Loss: 0.660871684551239\n",
      "Epoch [21/200], Step [3201/149], D Loss: 1.3328328132629395, G Loss: 0.6438469886779785\n",
      "Epoch [21/200], Step [4801/149], D Loss: 1.415615200996399, G Loss: 0.6699957251548767\n",
      "Epoch [21/200], Step [6401/149], D Loss: 1.455322027206421, G Loss: 0.6838375329971313\n",
      "Epoch [21/200], Step [8001/149], D Loss: 1.4046461582183838, G Loss: 0.7249125242233276\n",
      "Epoch [22/200], Step [1/149], D Loss: 1.4161962270736694, G Loss: 0.7553114891052246\n",
      "Epoch [22/200], Step [1601/149], D Loss: 1.3936669826507568, G Loss: 0.6857185959815979\n",
      "Epoch [22/200], Step [3201/149], D Loss: 1.3704440593719482, G Loss: 0.6502003073692322\n",
      "Epoch [22/200], Step [4801/149], D Loss: 1.406599998474121, G Loss: 0.6699519753456116\n",
      "Epoch [22/200], Step [6401/149], D Loss: 1.3400146961212158, G Loss: 0.7710791826248169\n",
      "Epoch [22/200], Step [8001/149], D Loss: 1.3209991455078125, G Loss: 0.8237006068229675\n",
      "Epoch [23/200], Step [1/149], D Loss: 1.6307461261749268, G Loss: 0.7562380433082581\n",
      "Epoch [23/200], Step [1601/149], D Loss: 1.5026071071624756, G Loss: 0.6179263591766357\n",
      "Epoch [23/200], Step [3201/149], D Loss: 1.3694396018981934, G Loss: 0.623333215713501\n",
      "Epoch [23/200], Step [4801/149], D Loss: 1.3763411045074463, G Loss: 0.6539796590805054\n",
      "Epoch [23/200], Step [6401/149], D Loss: 1.409441590309143, G Loss: 0.6657615303993225\n",
      "Epoch [23/200], Step [8001/149], D Loss: 1.389836072921753, G Loss: 0.6729652285575867\n",
      "Epoch [24/200], Step [1/149], D Loss: 1.3005883693695068, G Loss: 0.6865507364273071\n",
      "Epoch [24/200], Step [1601/149], D Loss: 1.2753243446350098, G Loss: 0.6826423406600952\n",
      "Epoch [24/200], Step [3201/149], D Loss: 1.3115358352661133, G Loss: 0.6457648277282715\n",
      "Epoch [24/200], Step [4801/149], D Loss: 1.6039807796478271, G Loss: 0.5895705223083496\n",
      "Epoch [24/200], Step [6401/149], D Loss: 1.5145728588104248, G Loss: 0.6938948631286621\n",
      "Epoch [24/200], Step [8001/149], D Loss: 1.240918755531311, G Loss: 1.0231671333312988\n",
      "Epoch [25/200], Step [1/149], D Loss: 1.5967986583709717, G Loss: 0.9343599081039429\n",
      "Epoch [25/200], Step [1601/149], D Loss: 1.5065979957580566, G Loss: 0.6880799531936646\n",
      "Epoch [25/200], Step [3201/149], D Loss: 1.3868415355682373, G Loss: 0.651179850101471\n",
      "Epoch [25/200], Step [4801/149], D Loss: 1.3684742450714111, G Loss: 0.667597770690918\n",
      "Epoch [25/200], Step [6401/149], D Loss: 1.4117125272750854, G Loss: 0.6455265283584595\n",
      "Epoch [25/200], Step [8001/149], D Loss: 1.4027516841888428, G Loss: 0.6520152688026428\n",
      "Epoch [26/200], Step [1/149], D Loss: 1.3817553520202637, G Loss: 0.6818264126777649\n",
      "Epoch [26/200], Step [1601/149], D Loss: 1.3222923278808594, G Loss: 0.7046970725059509\n",
      "Epoch [26/200], Step [3201/149], D Loss: 1.269425630569458, G Loss: 0.7178625464439392\n",
      "Epoch [26/200], Step [4801/149], D Loss: 1.4597362279891968, G Loss: 0.6552413702011108\n",
      "Epoch [26/200], Step [6401/149], D Loss: 1.5061640739440918, G Loss: 0.6447175145149231\n",
      "Epoch [26/200], Step [8001/149], D Loss: 1.383597731590271, G Loss: 0.713477611541748\n",
      "Epoch [27/200], Step [1/149], D Loss: 1.3881977796554565, G Loss: 0.7349668741226196\n",
      "Epoch [27/200], Step [1601/149], D Loss: 1.4067648649215698, G Loss: 0.6785761117935181\n",
      "Epoch [27/200], Step [3201/149], D Loss: 1.354532241821289, G Loss: 0.6964863538742065\n",
      "Epoch [27/200], Step [4801/149], D Loss: 1.377671480178833, G Loss: 0.7300055623054504\n",
      "Epoch [27/200], Step [6401/149], D Loss: 1.348895788192749, G Loss: 0.7616895437240601\n",
      "Epoch [27/200], Step [8001/149], D Loss: 1.3095755577087402, G Loss: 0.8329752683639526\n",
      "Epoch [28/200], Step [1/149], D Loss: 1.5647896528244019, G Loss: 0.8016772270202637\n",
      "Epoch [28/200], Step [1601/149], D Loss: 1.5114037990570068, G Loss: 0.5966358184814453\n",
      "Epoch [28/200], Step [3201/149], D Loss: 1.3882341384887695, G Loss: 0.5965020656585693\n",
      "Epoch [28/200], Step [4801/149], D Loss: 1.3913861513137817, G Loss: 0.6408225297927856\n",
      "Epoch [28/200], Step [6401/149], D Loss: 1.4000120162963867, G Loss: 0.6729995608329773\n",
      "Epoch [28/200], Step [8001/149], D Loss: 1.3939776420593262, G Loss: 0.6785956025123596\n",
      "Epoch [29/200], Step [1/149], D Loss: 1.3683881759643555, G Loss: 0.6799033880233765\n",
      "Epoch [29/200], Step [1601/149], D Loss: 1.3162235021591187, G Loss: 0.6750450730323792\n",
      "Epoch [29/200], Step [3201/149], D Loss: 1.252336025238037, G Loss: 0.6834279894828796\n",
      "Epoch [29/200], Step [4801/149], D Loss: 1.470410943031311, G Loss: 0.6345171332359314\n",
      "Epoch [29/200], Step [6401/149], D Loss: 1.5565605163574219, G Loss: 0.6214238405227661\n",
      "Epoch [29/200], Step [8001/149], D Loss: 1.4429757595062256, G Loss: 0.7010422945022583\n",
      "Epoch [30/200], Step [1/149], D Loss: 1.3879852294921875, G Loss: 0.7848912477493286\n",
      "Epoch [30/200], Step [1601/149], D Loss: 1.4067513942718506, G Loss: 0.7192354202270508\n",
      "Epoch [30/200], Step [3201/149], D Loss: 1.3810144662857056, G Loss: 0.687508225440979\n",
      "Epoch [30/200], Step [4801/149], D Loss: 1.3718174695968628, G Loss: 0.7254804372787476\n",
      "Epoch [30/200], Step [6401/149], D Loss: 1.296417474746704, G Loss: 0.8163216710090637\n",
      "Epoch [30/200], Step [8001/149], D Loss: 1.2938389778137207, G Loss: 0.8399668335914612\n",
      "Epoch [31/200], Step [1/149], D Loss: 1.6828093528747559, G Loss: 0.7119760513305664\n",
      "Epoch [31/200], Step [1601/149], D Loss: 1.5574554204940796, G Loss: 0.5988433361053467\n",
      "Epoch [31/200], Step [3201/149], D Loss: 1.4170182943344116, G Loss: 0.6021131873130798\n",
      "Epoch [31/200], Step [4801/149], D Loss: 1.392045259475708, G Loss: 0.6348680853843689\n",
      "Epoch [31/200], Step [6401/149], D Loss: 1.3958474397659302, G Loss: 0.6599682569503784\n",
      "Epoch [31/200], Step [8001/149], D Loss: 1.392859697341919, G Loss: 0.6667724251747131\n",
      "Epoch [32/200], Step [1/149], D Loss: 1.3406256437301636, G Loss: 0.6772695779800415\n",
      "Epoch [32/200], Step [1601/149], D Loss: 1.2456138134002686, G Loss: 0.697235107421875\n",
      "Epoch [32/200], Step [3201/149], D Loss: 1.120230793952942, G Loss: 0.7555123567581177\n",
      "Epoch [32/200], Step [4801/149], D Loss: 1.4791855812072754, G Loss: 0.6396067142486572\n",
      "Epoch [32/200], Step [6401/149], D Loss: 1.7080817222595215, G Loss: 0.5573114156723022\n",
      "Epoch [32/200], Step [8001/149], D Loss: 1.5032031536102295, G Loss: 0.6692301630973816\n",
      "Epoch [33/200], Step [1/149], D Loss: 1.3977104425430298, G Loss: 0.7986136078834534\n",
      "Epoch [33/200], Step [1601/149], D Loss: 1.3957014083862305, G Loss: 0.778986930847168\n",
      "Epoch [33/200], Step [3201/149], D Loss: 1.3672603368759155, G Loss: 0.7402229309082031\n",
      "Epoch [33/200], Step [4801/149], D Loss: 1.308480978012085, G Loss: 0.7976090908050537\n",
      "Epoch [33/200], Step [6401/149], D Loss: 1.197083592414856, G Loss: 0.9275482892990112\n",
      "Epoch [33/200], Step [8001/149], D Loss: 1.287535309791565, G Loss: 0.8651909828186035\n",
      "Epoch [34/200], Step [1/149], D Loss: 2.0689926147460938, G Loss: 0.593264102935791\n",
      "Epoch [34/200], Step [1601/149], D Loss: 1.5763734579086304, G Loss: 0.5611493587493896\n",
      "Epoch [34/200], Step [3201/149], D Loss: 1.239896297454834, G Loss: 0.684290885925293\n",
      "Epoch [34/200], Step [4801/149], D Loss: 1.348398208618164, G Loss: 0.6918371915817261\n",
      "Epoch [34/200], Step [6401/149], D Loss: 1.563849687576294, G Loss: 0.6257311701774597\n",
      "Epoch [34/200], Step [8001/149], D Loss: 1.4465630054473877, G Loss: 0.6332095861434937\n",
      "Epoch [35/200], Step [1/149], D Loss: 1.3243217468261719, G Loss: 0.6723967790603638\n",
      "Epoch [35/200], Step [1601/149], D Loss: 1.3052592277526855, G Loss: 0.6515328288078308\n",
      "Epoch [35/200], Step [3201/149], D Loss: 1.3447012901306152, G Loss: 0.6389113664627075\n",
      "Epoch [35/200], Step [4801/149], D Loss: 1.552889108657837, G Loss: 0.6857062578201294\n",
      "Epoch [35/200], Step [6401/149], D Loss: 1.3765758275985718, G Loss: 0.8274673819541931\n",
      "Epoch [35/200], Step [8001/149], D Loss: 1.2015372514724731, G Loss: 1.087018370628357\n",
      "Epoch [36/200], Step [1/149], D Loss: 1.7840540409088135, G Loss: 0.9597988128662109\n",
      "Epoch [36/200], Step [1601/149], D Loss: 1.5680474042892456, G Loss: 0.52691251039505\n",
      "Epoch [36/200], Step [3201/149], D Loss: 1.3948776721954346, G Loss: 0.5442676544189453\n",
      "Epoch [36/200], Step [4801/149], D Loss: 1.4135645627975464, G Loss: 0.5953665971755981\n",
      "Epoch [36/200], Step [6401/149], D Loss: 1.42789888381958, G Loss: 0.6397625803947449\n",
      "Epoch [36/200], Step [8001/149], D Loss: 1.3986785411834717, G Loss: 0.6659609079360962\n",
      "Epoch [37/200], Step [1/149], D Loss: 1.412973165512085, G Loss: 0.687526524066925\n",
      "Epoch [37/200], Step [1601/149], D Loss: 1.3698451519012451, G Loss: 0.6535035371780396\n",
      "Epoch [37/200], Step [3201/149], D Loss: 1.3214517831802368, G Loss: 0.6403684616088867\n",
      "Epoch [37/200], Step [4801/149], D Loss: 1.4140160083770752, G Loss: 0.6545084714889526\n",
      "Epoch [37/200], Step [6401/149], D Loss: 1.4205207824707031, G Loss: 0.7017449140548706\n",
      "Epoch [37/200], Step [8001/149], D Loss: 1.3861851692199707, G Loss: 0.7240492701530457\n",
      "Epoch [38/200], Step [1/149], D Loss: 1.4087563753128052, G Loss: 0.7104340195655823\n",
      "Epoch [38/200], Step [1601/149], D Loss: 1.3341262340545654, G Loss: 0.7016466856002808\n",
      "Epoch [38/200], Step [3201/149], D Loss: 1.3224753141403198, G Loss: 0.6741757988929749\n",
      "Epoch [38/200], Step [4801/149], D Loss: 1.474610686302185, G Loss: 0.6361019015312195\n",
      "Epoch [38/200], Step [6401/149], D Loss: 1.3868813514709473, G Loss: 0.7264517545700073\n",
      "Epoch [38/200], Step [8001/149], D Loss: 1.3059611320495605, G Loss: 0.8271707892417908\n",
      "Epoch [39/200], Step [1/149], D Loss: 1.4853181838989258, G Loss: 0.8298024535179138\n",
      "Epoch [39/200], Step [1601/149], D Loss: 1.4689667224884033, G Loss: 0.6712263822555542\n",
      "Epoch [39/200], Step [3201/149], D Loss: 1.4208241701126099, G Loss: 0.6226058006286621\n",
      "Epoch [39/200], Step [4801/149], D Loss: 1.3882708549499512, G Loss: 0.6471772193908691\n",
      "Epoch [39/200], Step [6401/149], D Loss: 1.376509428024292, G Loss: 0.6743394732475281\n",
      "Epoch [39/200], Step [8001/149], D Loss: 1.3847821950912476, G Loss: 0.6851692795753479\n",
      "Epoch [40/200], Step [1/149], D Loss: 1.3768844604492188, G Loss: 0.682315468788147\n",
      "Epoch [40/200], Step [1601/149], D Loss: 1.3180056810379028, G Loss: 0.6739283204078674\n",
      "Epoch [40/200], Step [3201/149], D Loss: 1.2800781726837158, G Loss: 0.6855067014694214\n",
      "Epoch [40/200], Step [4801/149], D Loss: 1.5758211612701416, G Loss: 0.6281942129135132\n",
      "Epoch [40/200], Step [6401/149], D Loss: 1.4127562046051025, G Loss: 0.7843189239501953\n",
      "Epoch [40/200], Step [8001/149], D Loss: 1.2773916721343994, G Loss: 0.9962762594223022\n",
      "Epoch [41/200], Step [1/149], D Loss: 1.5504155158996582, G Loss: 1.022916316986084\n",
      "Epoch [41/200], Step [1601/149], D Loss: 1.5113162994384766, G Loss: 0.5921700596809387\n",
      "Epoch [41/200], Step [3201/149], D Loss: 1.4409329891204834, G Loss: 0.5384741425514221\n",
      "Epoch [41/200], Step [4801/149], D Loss: 1.3953008651733398, G Loss: 0.6030900478363037\n",
      "Epoch [41/200], Step [6401/149], D Loss: 1.3934801816940308, G Loss: 0.6480050683021545\n",
      "Epoch [41/200], Step [8001/149], D Loss: 1.3907783031463623, G Loss: 0.6626846194267273\n",
      "Epoch [42/200], Step [1/149], D Loss: 1.3227462768554688, G Loss: 0.6713134050369263\n",
      "Epoch [42/200], Step [1601/149], D Loss: 1.221840500831604, G Loss: 0.6806163787841797\n",
      "Epoch [42/200], Step [3201/149], D Loss: 1.238893985748291, G Loss: 0.6472541689872742\n",
      "Epoch [42/200], Step [4801/149], D Loss: 1.703045129776001, G Loss: 0.5797711610794067\n",
      "Epoch [42/200], Step [6401/149], D Loss: 1.5197811126708984, G Loss: 0.7474660873413086\n",
      "Epoch [42/200], Step [8001/149], D Loss: 1.3164626359939575, G Loss: 0.9862659573554993\n",
      "Epoch [43/200], Step [1/149], D Loss: 1.4698030948638916, G Loss: 1.0794957876205444\n",
      "Epoch [43/200], Step [1601/149], D Loss: 1.4443998336791992, G Loss: 0.7363449335098267\n",
      "Epoch [43/200], Step [3201/149], D Loss: 1.4028469324111938, G Loss: 0.6284193992614746\n",
      "Epoch [43/200], Step [4801/149], D Loss: 1.35971999168396, G Loss: 0.64215087890625\n",
      "Epoch [43/200], Step [6401/149], D Loss: 1.2915725708007812, G Loss: 0.7007935643196106\n",
      "Epoch [43/200], Step [8001/149], D Loss: 1.3231735229492188, G Loss: 0.7108367681503296\n",
      "Epoch [44/200], Step [1/149], D Loss: 1.6414101123809814, G Loss: 0.6483016610145569\n",
      "Epoch [44/200], Step [1601/149], D Loss: 1.5995674133300781, G Loss: 0.5609828233718872\n",
      "Epoch [44/200], Step [3201/149], D Loss: 1.3608388900756836, G Loss: 0.6610501408576965\n",
      "Epoch [44/200], Step [4801/149], D Loss: 1.3028178215026855, G Loss: 0.7682346105575562\n",
      "Epoch [44/200], Step [6401/149], D Loss: 1.5324711799621582, G Loss: 0.6333405375480652\n",
      "Epoch [44/200], Step [8001/149], D Loss: 1.4436910152435303, G Loss: 0.6143221855163574\n",
      "Epoch [45/200], Step [1/149], D Loss: 1.328984022140503, G Loss: 0.6671572923660278\n",
      "Epoch [45/200], Step [1601/149], D Loss: 1.314530372619629, G Loss: 0.6634238958358765\n",
      "Epoch [45/200], Step [3201/149], D Loss: 1.439940094947815, G Loss: 0.5942819118499756\n",
      "Epoch [45/200], Step [4801/149], D Loss: 1.4340362548828125, G Loss: 0.7505688071250916\n",
      "Epoch [45/200], Step [6401/149], D Loss: 1.2102116346359253, G Loss: 1.061464786529541\n",
      "Epoch [45/200], Step [8001/149], D Loss: 1.1910035610198975, G Loss: 1.0224636793136597\n",
      "Epoch [46/200], Step [1/149], D Loss: 2.1054396629333496, G Loss: 0.6974055767059326\n",
      "Epoch [46/200], Step [1601/149], D Loss: 1.6079466342926025, G Loss: 0.5389872789382935\n",
      "Epoch [46/200], Step [3201/149], D Loss: 1.339683175086975, G Loss: 0.6036011576652527\n",
      "Epoch [46/200], Step [4801/149], D Loss: 1.374420166015625, G Loss: 0.6611348986625671\n",
      "Epoch [46/200], Step [6401/149], D Loss: 1.4664602279663086, G Loss: 0.6606549024581909\n",
      "Epoch [46/200], Step [8001/149], D Loss: 1.4064821004867554, G Loss: 0.6689956188201904\n",
      "Epoch [47/200], Step [1/149], D Loss: 1.3361492156982422, G Loss: 0.6940338611602783\n",
      "Epoch [47/200], Step [1601/149], D Loss: 1.2806423902511597, G Loss: 0.6733477115631104\n",
      "Epoch [47/200], Step [3201/149], D Loss: 1.3130440711975098, G Loss: 0.6711370944976807\n",
      "Epoch [47/200], Step [4801/149], D Loss: 1.6141328811645508, G Loss: 0.6723752617835999\n",
      "Epoch [47/200], Step [6401/149], D Loss: 1.2930949926376343, G Loss: 0.9355615973472595\n",
      "Epoch [47/200], Step [8001/149], D Loss: 1.1681010723114014, G Loss: 1.0362838506698608\n",
      "Epoch [48/200], Step [1/149], D Loss: 1.949753761291504, G Loss: 1.0750879049301147\n",
      "Epoch [48/200], Step [1601/149], D Loss: 1.583233118057251, G Loss: 0.4918054938316345\n",
      "Epoch [48/200], Step [3201/149], D Loss: 1.3828351497650146, G Loss: 0.49689117074012756\n",
      "Epoch [48/200], Step [4801/149], D Loss: 1.4093351364135742, G Loss: 0.59733647108078\n",
      "Epoch [48/200], Step [6401/149], D Loss: 1.4510862827301025, G Loss: 0.645329475402832\n",
      "Epoch [48/200], Step [8001/149], D Loss: 1.3737528324127197, G Loss: 0.7155951261520386\n",
      "Epoch [49/200], Step [1/149], D Loss: 1.3493144512176514, G Loss: 0.759074866771698\n",
      "Epoch [49/200], Step [1601/149], D Loss: 1.322031855583191, G Loss: 0.6140238046646118\n",
      "Epoch [49/200], Step [3201/149], D Loss: 1.3555808067321777, G Loss: 0.5959296822547913\n",
      "Epoch [49/200], Step [4801/149], D Loss: 1.4875906705856323, G Loss: 0.6998799443244934\n",
      "Epoch [49/200], Step [6401/149], D Loss: 1.3328642845153809, G Loss: 0.8517988920211792\n",
      "Epoch [49/200], Step [8001/149], D Loss: 1.2725014686584473, G Loss: 0.8987311124801636\n",
      "Epoch [50/200], Step [1/149], D Loss: 1.6722241640090942, G Loss: 0.9048636555671692\n",
      "Epoch [50/200], Step [1601/149], D Loss: 1.5152966976165771, G Loss: 0.5650950074195862\n",
      "Epoch [50/200], Step [3201/149], D Loss: 1.3856804370880127, G Loss: 0.5855569839477539\n",
      "Epoch [50/200], Step [4801/149], D Loss: 1.3901060819625854, G Loss: 0.633399486541748\n",
      "Epoch [50/200], Step [6401/149], D Loss: 1.4022181034088135, G Loss: 0.6717725992202759\n",
      "Epoch [50/200], Step [8001/149], D Loss: 1.3851313591003418, G Loss: 0.6909018158912659\n",
      "Epoch [51/200], Step [1/149], D Loss: 1.2858449220657349, G Loss: 0.6852620840072632\n",
      "Epoch [51/200], Step [1601/149], D Loss: 1.2337682247161865, G Loss: 0.6757580041885376\n",
      "Epoch [51/200], Step [3201/149], D Loss: 1.2203514575958252, G Loss: 0.7110686302185059\n",
      "Epoch [51/200], Step [4801/149], D Loss: 1.6298866271972656, G Loss: 0.6490662693977356\n",
      "Epoch [51/200], Step [6401/149], D Loss: 1.5362863540649414, G Loss: 0.6870847940444946\n",
      "Epoch [51/200], Step [8001/149], D Loss: 1.3704999685287476, G Loss: 0.8503875136375427\n",
      "Epoch [52/200], Step [1/149], D Loss: 1.4349839687347412, G Loss: 0.9694658517837524\n",
      "Epoch [52/200], Step [1601/149], D Loss: 1.4093880653381348, G Loss: 0.694795548915863\n",
      "Epoch [52/200], Step [3201/149], D Loss: 1.390209436416626, G Loss: 0.6300636529922485\n",
      "Epoch [52/200], Step [4801/149], D Loss: 1.3582465648651123, G Loss: 0.6739324927330017\n",
      "Epoch [52/200], Step [6401/149], D Loss: 1.342595100402832, G Loss: 0.7071471810340881\n",
      "Epoch [52/200], Step [8001/149], D Loss: 1.3938357830047607, G Loss: 0.7001815438270569\n",
      "Epoch [53/200], Step [1/149], D Loss: 1.537750482559204, G Loss: 0.6472130417823792\n",
      "Epoch [53/200], Step [1601/149], D Loss: 1.2874692678451538, G Loss: 0.665905237197876\n",
      "Epoch [53/200], Step [3201/149], D Loss: 1.302060842514038, G Loss: 0.6129229068756104\n",
      "Epoch [53/200], Step [4801/149], D Loss: 1.480035424232483, G Loss: 0.6481372117996216\n",
      "Epoch [53/200], Step [6401/149], D Loss: 1.4931808710098267, G Loss: 0.6734296083450317\n",
      "Epoch [53/200], Step [8001/149], D Loss: 1.394759178161621, G Loss: 0.7534188032150269\n",
      "Epoch [54/200], Step [1/149], D Loss: 1.4061270952224731, G Loss: 0.8912853002548218\n",
      "Epoch [54/200], Step [1601/149], D Loss: 1.3949098587036133, G Loss: 0.7405748963356018\n",
      "Epoch [54/200], Step [3201/149], D Loss: 1.385688066482544, G Loss: 0.6682170629501343\n",
      "Epoch [54/200], Step [4801/149], D Loss: 1.3530775308609009, G Loss: 0.6831996440887451\n",
      "Epoch [54/200], Step [6401/149], D Loss: 1.288428783416748, G Loss: 0.7545943856239319\n",
      "Epoch [54/200], Step [8001/149], D Loss: 1.4068081378936768, G Loss: 0.6874968409538269\n",
      "Epoch [55/200], Step [1/149], D Loss: 1.6389148235321045, G Loss: 0.6348311901092529\n",
      "Epoch [55/200], Step [1601/149], D Loss: 1.3489806652069092, G Loss: 0.6302233934402466\n",
      "Epoch [55/200], Step [3201/149], D Loss: 1.2961668968200684, G Loss: 0.6169225573539734\n",
      "Epoch [55/200], Step [4801/149], D Loss: 1.5056190490722656, G Loss: 0.6068350076675415\n",
      "Epoch [55/200], Step [6401/149], D Loss: 1.5222721099853516, G Loss: 0.6361756324768066\n",
      "Epoch [55/200], Step [8001/149], D Loss: 1.3628579378128052, G Loss: 0.7730745673179626\n",
      "Epoch [56/200], Step [1/149], D Loss: 1.4114785194396973, G Loss: 0.9753830432891846\n",
      "Epoch [56/200], Step [1601/149], D Loss: 1.4292666912078857, G Loss: 0.6913599371910095\n",
      "Epoch [56/200], Step [3201/149], D Loss: 1.3991634845733643, G Loss: 0.6313039064407349\n",
      "Epoch [56/200], Step [4801/149], D Loss: 1.389470100402832, G Loss: 0.6433966755867004\n",
      "Epoch [56/200], Step [6401/149], D Loss: 1.3780484199523926, G Loss: 0.668514609336853\n",
      "Epoch [56/200], Step [8001/149], D Loss: 1.3962668180465698, G Loss: 0.6712379455566406\n",
      "Epoch [57/200], Step [1/149], D Loss: 1.3450841903686523, G Loss: 0.6782369613647461\n",
      "Epoch [57/200], Step [1601/149], D Loss: 1.2511355876922607, G Loss: 0.687057614326477\n",
      "Epoch [57/200], Step [3201/149], D Loss: 1.2296239137649536, G Loss: 0.6607535481452942\n",
      "Epoch [57/200], Step [4801/149], D Loss: 1.6004576683044434, G Loss: 0.5697988271713257\n",
      "Epoch [57/200], Step [6401/149], D Loss: 1.584080696105957, G Loss: 0.6503250598907471\n",
      "Epoch [57/200], Step [8001/149], D Loss: 1.2882983684539795, G Loss: 1.0262303352355957\n",
      "Epoch [58/200], Step [1/149], D Loss: 1.5984132289886475, G Loss: 1.0873956680297852\n",
      "Epoch [58/200], Step [1601/149], D Loss: 1.4900968074798584, G Loss: 0.6199430227279663\n",
      "Epoch [58/200], Step [3201/149], D Loss: 1.3695833683013916, G Loss: 0.5967963337898254\n",
      "Epoch [58/200], Step [4801/149], D Loss: 1.3894777297973633, G Loss: 0.6338449716567993\n",
      "Epoch [58/200], Step [6401/149], D Loss: 1.4126321077346802, G Loss: 0.6480965614318848\n",
      "Epoch [58/200], Step [8001/149], D Loss: 1.3875958919525146, G Loss: 0.6683402061462402\n",
      "Epoch [59/200], Step [1/149], D Loss: 1.295494556427002, G Loss: 0.6864177584648132\n",
      "Epoch [59/200], Step [1601/149], D Loss: 1.2731971740722656, G Loss: 0.6764848828315735\n",
      "Epoch [59/200], Step [3201/149], D Loss: 1.3298461437225342, G Loss: 0.6416130661964417\n",
      "Epoch [59/200], Step [4801/149], D Loss: 1.555138349533081, G Loss: 0.6434212923049927\n",
      "Epoch [59/200], Step [6401/149], D Loss: 1.456611156463623, G Loss: 0.7301954030990601\n",
      "Epoch [59/200], Step [8001/149], D Loss: 1.3379738330841064, G Loss: 0.8797595500946045\n",
      "Epoch [60/200], Step [1/149], D Loss: 1.4793397188186646, G Loss: 1.0452266931533813\n",
      "Epoch [60/200], Step [1601/149], D Loss: 1.4411534070968628, G Loss: 0.6842372417449951\n",
      "Epoch [60/200], Step [3201/149], D Loss: 1.3975400924682617, G Loss: 0.6080668568611145\n",
      "Epoch [60/200], Step [4801/149], D Loss: 1.3835347890853882, G Loss: 0.6223447322845459\n",
      "Epoch [60/200], Step [6401/149], D Loss: 1.3347859382629395, G Loss: 0.6804825067520142\n",
      "Epoch [60/200], Step [8001/149], D Loss: 1.386243224143982, G Loss: 0.6810822486877441\n",
      "Epoch [61/200], Step [1/149], D Loss: 1.46700918674469, G Loss: 0.6732726693153381\n",
      "Epoch [61/200], Step [1601/149], D Loss: 1.3585264682769775, G Loss: 0.6356230974197388\n",
      "Epoch [61/200], Step [3201/149], D Loss: 1.2029093503952026, G Loss: 0.6947226524353027\n",
      "Epoch [61/200], Step [4801/149], D Loss: 1.5004202127456665, G Loss: 0.6543682813644409\n",
      "Epoch [61/200], Step [6401/149], D Loss: 1.5489610433578491, G Loss: 0.6587448120117188\n",
      "Epoch [61/200], Step [8001/149], D Loss: 1.4135655164718628, G Loss: 0.7914682626724243\n",
      "Epoch [62/200], Step [1/149], D Loss: 1.455993413925171, G Loss: 1.085301399230957\n",
      "Epoch [62/200], Step [1601/149], D Loss: 1.4013559818267822, G Loss: 0.6979875564575195\n",
      "Epoch [62/200], Step [3201/149], D Loss: 1.3884633779525757, G Loss: 0.613558828830719\n",
      "Epoch [62/200], Step [4801/149], D Loss: 1.362915277481079, G Loss: 0.6498955488204956\n",
      "Epoch [62/200], Step [6401/149], D Loss: 1.2774924039840698, G Loss: 0.7481534481048584\n",
      "Epoch [62/200], Step [8001/149], D Loss: 1.3108510971069336, G Loss: 0.7907139658927917\n",
      "Epoch [63/200], Step [1/149], D Loss: 1.7105920314788818, G Loss: 0.6936588287353516\n",
      "Epoch [63/200], Step [1601/149], D Loss: 1.4909557104110718, G Loss: 0.5847516655921936\n",
      "Epoch [63/200], Step [3201/149], D Loss: 1.2904036045074463, G Loss: 0.6263507008552551\n",
      "Epoch [63/200], Step [4801/149], D Loss: 1.3822166919708252, G Loss: 0.654212236404419\n",
      "Epoch [63/200], Step [6401/149], D Loss: 1.448254108428955, G Loss: 0.6538955569267273\n",
      "Epoch [63/200], Step [8001/149], D Loss: 1.4155771732330322, G Loss: 0.6671547293663025\n",
      "Epoch [64/200], Step [1/149], D Loss: 1.2972201108932495, G Loss: 0.7180414199829102\n",
      "Epoch [64/200], Step [1601/149], D Loss: 1.3525886535644531, G Loss: 0.6194349527359009\n",
      "Epoch [64/200], Step [3201/149], D Loss: 1.4176347255706787, G Loss: 0.6812302470207214\n",
      "Epoch [64/200], Step [4801/149], D Loss: 1.3399572372436523, G Loss: 0.959054172039032\n",
      "Epoch [64/200], Step [6401/149], D Loss: 1.1009085178375244, G Loss: 1.164565086364746\n",
      "Epoch [64/200], Step [8001/149], D Loss: 1.2852219343185425, G Loss: 0.7992515563964844\n",
      "Epoch [65/200], Step [1/149], D Loss: 2.125256299972534, G Loss: 0.5783776640892029\n",
      "Epoch [65/200], Step [1601/149], D Loss: 1.387986660003662, G Loss: 0.5866537690162659\n",
      "Epoch [65/200], Step [3201/149], D Loss: 1.2369616031646729, G Loss: 0.6417903304100037\n",
      "Epoch [65/200], Step [4801/149], D Loss: 1.493952751159668, G Loss: 0.6921389698982239\n",
      "Epoch [65/200], Step [6401/149], D Loss: 1.5443074703216553, G Loss: 0.6605553030967712\n",
      "Epoch [65/200], Step [8001/149], D Loss: 1.348198652267456, G Loss: 0.7761792540550232\n",
      "Epoch [66/200], Step [1/149], D Loss: 1.4582356214523315, G Loss: 0.8815962672233582\n",
      "Epoch [66/200], Step [1601/149], D Loss: 1.3780443668365479, G Loss: 0.7254530787467957\n",
      "Epoch [66/200], Step [3201/149], D Loss: 1.34882652759552, G Loss: 0.6520966291427612\n",
      "Epoch [66/200], Step [4801/149], D Loss: 1.3893301486968994, G Loss: 0.6551705598831177\n",
      "Epoch [66/200], Step [6401/149], D Loss: 1.3377420902252197, G Loss: 0.7185481190681458\n",
      "Epoch [66/200], Step [8001/149], D Loss: 1.3289997577667236, G Loss: 0.7599843144416809\n",
      "Epoch [67/200], Step [1/149], D Loss: 1.5309189558029175, G Loss: 0.7753602266311646\n",
      "Epoch [67/200], Step [1601/149], D Loss: 1.4696191549301147, G Loss: 0.586405336856842\n",
      "Epoch [67/200], Step [3201/149], D Loss: 1.3128986358642578, G Loss: 0.6208789944648743\n",
      "Epoch [67/200], Step [4801/149], D Loss: 1.3796225786209106, G Loss: 0.6622589826583862\n",
      "Epoch [67/200], Step [6401/149], D Loss: 1.4205280542373657, G Loss: 0.6597164869308472\n",
      "Epoch [67/200], Step [8001/149], D Loss: 1.4049577713012695, G Loss: 0.6568740606307983\n",
      "Epoch [68/200], Step [1/149], D Loss: 1.3178777694702148, G Loss: 0.6267247200012207\n",
      "Epoch [68/200], Step [1601/149], D Loss: 1.3556556701660156, G Loss: 0.6391217112541199\n",
      "Epoch [68/200], Step [3201/149], D Loss: 1.4251583814620972, G Loss: 0.6599621772766113\n",
      "Epoch [68/200], Step [4801/149], D Loss: 1.4449408054351807, G Loss: 0.7606925964355469\n",
      "Epoch [68/200], Step [6401/149], D Loss: 1.30181884765625, G Loss: 0.8759574294090271\n",
      "Epoch [68/200], Step [8001/149], D Loss: 1.264220952987671, G Loss: 0.9329615831375122\n",
      "Epoch [69/200], Step [1/149], D Loss: 1.614811897277832, G Loss: 0.976834774017334\n",
      "Epoch [69/200], Step [1601/149], D Loss: 1.50339937210083, G Loss: 0.5787544846534729\n",
      "Epoch [69/200], Step [3201/149], D Loss: 1.4095697402954102, G Loss: 0.5577797293663025\n",
      "Epoch [69/200], Step [4801/149], D Loss: 1.3941378593444824, G Loss: 0.6011201739311218\n",
      "Epoch [69/200], Step [6401/149], D Loss: 1.397277593612671, G Loss: 0.6361957788467407\n",
      "Epoch [69/200], Step [8001/149], D Loss: 1.38663649559021, G Loss: 0.6616305708885193\n",
      "Epoch [70/200], Step [1/149], D Loss: 1.284462571144104, G Loss: 0.6706146001815796\n",
      "Epoch [70/200], Step [1601/149], D Loss: 1.2325013875961304, G Loss: 0.6810770034790039\n",
      "Epoch [70/200], Step [3201/149], D Loss: 1.292222023010254, G Loss: 0.6508104205131531\n",
      "Epoch [70/200], Step [4801/149], D Loss: 1.5943429470062256, G Loss: 0.650722086429596\n",
      "Epoch [70/200], Step [6401/149], D Loss: 1.4657526016235352, G Loss: 0.7475149631500244\n",
      "Epoch [70/200], Step [8001/149], D Loss: 1.293226957321167, G Loss: 0.9731557965278625\n",
      "Epoch [71/200], Step [1/149], D Loss: 1.4975745677947998, G Loss: 1.1220651865005493\n",
      "Epoch [71/200], Step [1601/149], D Loss: 1.4415967464447021, G Loss: 0.6645973324775696\n",
      "Epoch [71/200], Step [3201/149], D Loss: 1.3998193740844727, G Loss: 0.595431387424469\n",
      "Epoch [71/200], Step [4801/149], D Loss: 1.3964990377426147, G Loss: 0.6103588342666626\n",
      "Epoch [71/200], Step [6401/149], D Loss: 1.3811211585998535, G Loss: 0.6432342529296875\n",
      "Epoch [71/200], Step [8001/149], D Loss: 1.3966808319091797, G Loss: 0.6604432463645935\n",
      "Epoch [72/200], Step [1/149], D Loss: 1.3505738973617554, G Loss: 0.6740714311599731\n",
      "Epoch [72/200], Step [1601/149], D Loss: 1.269477128982544, G Loss: 0.6839278340339661\n",
      "Epoch [72/200], Step [3201/149], D Loss: 1.2319802045822144, G Loss: 0.6927626132965088\n",
      "Epoch [72/200], Step [4801/149], D Loss: 1.549116849899292, G Loss: 0.6448878049850464\n",
      "Epoch [72/200], Step [6401/149], D Loss: 1.5734364986419678, G Loss: 0.6199654340744019\n",
      "Epoch [72/200], Step [8001/149], D Loss: 1.3789396286010742, G Loss: 0.8246198892593384\n",
      "Epoch [73/200], Step [1/149], D Loss: 1.4097278118133545, G Loss: 1.1932945251464844\n",
      "Epoch [73/200], Step [1601/149], D Loss: 1.4292912483215332, G Loss: 0.6688295006752014\n",
      "Epoch [73/200], Step [3201/149], D Loss: 1.398207426071167, G Loss: 0.5891101360321045\n",
      "Epoch [73/200], Step [4801/149], D Loss: 1.3973920345306396, G Loss: 0.6093660593032837\n",
      "Epoch [73/200], Step [6401/149], D Loss: 1.3613555431365967, G Loss: 0.662992000579834\n",
      "Epoch [73/200], Step [8001/149], D Loss: 1.3824031352996826, G Loss: 0.6870306134223938\n",
      "Epoch [74/200], Step [1/149], D Loss: 1.426644206047058, G Loss: 0.6958997845649719\n",
      "Epoch [74/200], Step [1601/149], D Loss: 1.3488078117370605, G Loss: 0.6645375490188599\n",
      "Epoch [74/200], Step [3201/149], D Loss: 1.3046348094940186, G Loss: 0.655359148979187\n",
      "Epoch [74/200], Step [4801/149], D Loss: 1.4176934957504272, G Loss: 0.6632381081581116\n",
      "Epoch [74/200], Step [6401/149], D Loss: 1.4351708889007568, G Loss: 0.6679253578186035\n",
      "Epoch [74/200], Step [8001/149], D Loss: 1.4050401449203491, G Loss: 0.7074241638183594\n",
      "Epoch [75/200], Step [1/149], D Loss: 1.380589485168457, G Loss: 0.7608461380004883\n",
      "Epoch [75/200], Step [1601/149], D Loss: 1.3801867961883545, G Loss: 0.7089661955833435\n",
      "Epoch [75/200], Step [3201/149], D Loss: 1.3823871612548828, G Loss: 0.6889857053756714\n",
      "Epoch [75/200], Step [4801/149], D Loss: 1.3252806663513184, G Loss: 0.7482832074165344\n",
      "Epoch [75/200], Step [6401/149], D Loss: 1.263242483139038, G Loss: 0.7952120304107666\n",
      "Epoch [75/200], Step [8001/149], D Loss: 1.332077145576477, G Loss: 0.7761805057525635\n",
      "Epoch [76/200], Step [1/149], D Loss: 1.7148244380950928, G Loss: 0.7282307744026184\n",
      "Epoch [76/200], Step [1601/149], D Loss: 1.4030191898345947, G Loss: 0.6339787244796753\n",
      "Epoch [76/200], Step [3201/149], D Loss: 1.309066891670227, G Loss: 0.6356348395347595\n",
      "Epoch [76/200], Step [4801/149], D Loss: 1.395301103591919, G Loss: 0.659294843673706\n",
      "Epoch [76/200], Step [6401/149], D Loss: 1.42128586769104, G Loss: 0.6622153520584106\n",
      "Epoch [76/200], Step [8001/149], D Loss: 1.4024620056152344, G Loss: 0.6654284596443176\n",
      "Epoch [77/200], Step [1/149], D Loss: 1.310848355293274, G Loss: 0.6649805903434753\n",
      "Epoch [77/200], Step [1601/149], D Loss: 1.33127760887146, G Loss: 0.6539483070373535\n",
      "Epoch [77/200], Step [3201/149], D Loss: 1.3984920978546143, G Loss: 0.6374480724334717\n",
      "Epoch [77/200], Step [4801/149], D Loss: 1.466755747795105, G Loss: 0.6822075843811035\n",
      "Epoch [77/200], Step [6401/149], D Loss: 1.3262653350830078, G Loss: 0.7935740947723389\n",
      "Epoch [77/200], Step [8001/149], D Loss: 1.2303171157836914, G Loss: 0.9308102130889893\n",
      "Epoch [78/200], Step [1/149], D Loss: 1.591318130493164, G Loss: 1.0289409160614014\n",
      "Epoch [78/200], Step [1601/149], D Loss: 1.5682222843170166, G Loss: 0.6002684235572815\n",
      "Epoch [78/200], Step [3201/149], D Loss: 1.4227867126464844, G Loss: 0.6116729974746704\n",
      "Epoch [78/200], Step [4801/149], D Loss: 1.4087715148925781, G Loss: 0.6249114274978638\n",
      "Epoch [78/200], Step [6401/149], D Loss: 1.4074430465698242, G Loss: 0.6527310013771057\n",
      "Epoch [78/200], Step [8001/149], D Loss: 1.3872120380401611, G Loss: 0.6722703576087952\n",
      "Epoch [79/200], Step [1/149], D Loss: 1.2908425331115723, G Loss: 0.6790229082107544\n",
      "Epoch [79/200], Step [1601/149], D Loss: 1.1727120876312256, G Loss: 0.7425100803375244\n",
      "Epoch [79/200], Step [3201/149], D Loss: 1.1153168678283691, G Loss: 0.7602882981300354\n",
      "Epoch [79/200], Step [4801/149], D Loss: 1.8179422616958618, G Loss: 0.5703544616699219\n",
      "Epoch [79/200], Step [6401/149], D Loss: 1.717162847518921, G Loss: 0.583127498626709\n",
      "Epoch [79/200], Step [8001/149], D Loss: 1.3644883632659912, G Loss: 0.8782352209091187\n",
      "Epoch [80/200], Step [1/149], D Loss: 1.4349671602249146, G Loss: 1.2174657583236694\n",
      "Epoch [80/200], Step [1601/149], D Loss: 1.4738054275512695, G Loss: 0.6361978054046631\n",
      "Epoch [80/200], Step [3201/149], D Loss: 1.4054019451141357, G Loss: 0.5871628522872925\n",
      "Epoch [80/200], Step [4801/149], D Loss: 1.366323471069336, G Loss: 0.6423203349113464\n",
      "Epoch [80/200], Step [6401/149], D Loss: 1.356993317604065, G Loss: 0.6672791242599487\n",
      "Epoch [80/200], Step [8001/149], D Loss: 1.3866777420043945, G Loss: 0.671760082244873\n",
      "Epoch [81/200], Step [1/149], D Loss: 1.4380319118499756, G Loss: 0.6979290246963501\n",
      "Epoch [81/200], Step [1601/149], D Loss: 1.3366410732269287, G Loss: 0.6483843326568604\n",
      "Epoch [81/200], Step [3201/149], D Loss: 1.255996584892273, G Loss: 0.6576690673828125\n",
      "Epoch [81/200], Step [4801/149], D Loss: 1.4631210565567017, G Loss: 0.6420372724533081\n",
      "Epoch [81/200], Step [6401/149], D Loss: 1.4870197772979736, G Loss: 0.6572520136833191\n",
      "Epoch [81/200], Step [8001/149], D Loss: 1.4019410610198975, G Loss: 0.7254403829574585\n",
      "Epoch [82/200], Step [1/149], D Loss: 1.4062347412109375, G Loss: 0.7825772166252136\n",
      "Epoch [82/200], Step [1601/149], D Loss: 1.3878695964813232, G Loss: 0.7269971370697021\n",
      "Epoch [82/200], Step [3201/149], D Loss: 1.3792260885238647, G Loss: 0.6844618320465088\n",
      "Epoch [82/200], Step [4801/149], D Loss: 1.3271889686584473, G Loss: 0.7211529612541199\n",
      "Epoch [82/200], Step [6401/149], D Loss: 1.1798306703567505, G Loss: 0.8643536567687988\n",
      "Epoch [82/200], Step [8001/149], D Loss: 1.2752940654754639, G Loss: 0.8174915313720703\n",
      "Epoch [83/200], Step [1/149], D Loss: 1.9644831418991089, G Loss: 0.5662772059440613\n",
      "Epoch [83/200], Step [1601/149], D Loss: 1.3647427558898926, G Loss: 0.6602684855461121\n",
      "Epoch [83/200], Step [3201/149], D Loss: 1.2785989046096802, G Loss: 0.6743436455726624\n",
      "Epoch [83/200], Step [4801/149], D Loss: 1.4622383117675781, G Loss: 0.6925508379936218\n",
      "Epoch [83/200], Step [6401/149], D Loss: 1.5073919296264648, G Loss: 0.6480625867843628\n",
      "Epoch [83/200], Step [8001/149], D Loss: 1.421013355255127, G Loss: 0.6705988645553589\n",
      "Epoch [84/200], Step [1/149], D Loss: 1.403883695602417, G Loss: 0.7847979068756104\n",
      "Epoch [84/200], Step [1601/149], D Loss: 1.3658688068389893, G Loss: 0.7671038508415222\n",
      "Epoch [84/200], Step [3201/149], D Loss: 1.3888624906539917, G Loss: 0.6739181876182556\n",
      "Epoch [84/200], Step [4801/149], D Loss: 1.3269349336624146, G Loss: 0.6996120810508728\n",
      "Epoch [84/200], Step [6401/149], D Loss: 1.2234848737716675, G Loss: 0.8000197410583496\n",
      "Epoch [84/200], Step [8001/149], D Loss: 1.2928032875061035, G Loss: 0.7934474349021912\n",
      "Epoch [85/200], Step [1/149], D Loss: 1.7816388607025146, G Loss: 0.6703640818595886\n",
      "Epoch [85/200], Step [1601/149], D Loss: 1.483007550239563, G Loss: 0.5861856937408447\n",
      "Epoch [85/200], Step [3201/149], D Loss: 1.3739479780197144, G Loss: 0.600911557674408\n",
      "Epoch [85/200], Step [4801/149], D Loss: 1.3972806930541992, G Loss: 0.6567748785018921\n",
      "Epoch [85/200], Step [6401/149], D Loss: 1.4082026481628418, G Loss: 0.6886830925941467\n",
      "Epoch [85/200], Step [8001/149], D Loss: 1.3655232191085815, G Loss: 0.7073181867599487\n",
      "Epoch [86/200], Step [1/149], D Loss: 1.2896487712860107, G Loss: 0.6959855556488037\n",
      "Epoch [86/200], Step [1601/149], D Loss: 1.2566211223602295, G Loss: 0.6953815817832947\n",
      "Epoch [86/200], Step [3201/149], D Loss: 1.2784405946731567, G Loss: 0.702645480632782\n",
      "Epoch [86/200], Step [4801/149], D Loss: 1.566615343093872, G Loss: 0.6564350128173828\n",
      "Epoch [86/200], Step [6401/149], D Loss: 1.483870029449463, G Loss: 0.6888211965560913\n",
      "Epoch [86/200], Step [8001/149], D Loss: 1.3374531269073486, G Loss: 0.849507212638855\n",
      "Epoch [87/200], Step [1/149], D Loss: 1.4272637367248535, G Loss: 1.0764892101287842\n",
      "Epoch [87/200], Step [1601/149], D Loss: 1.4067704677581787, G Loss: 0.6871742606163025\n",
      "Epoch [87/200], Step [3201/149], D Loss: 1.407926082611084, G Loss: 0.6101326942443848\n",
      "Epoch [87/200], Step [4801/149], D Loss: 1.3220899105072021, G Loss: 0.6472324728965759\n",
      "Epoch [87/200], Step [6401/149], D Loss: 1.230391502380371, G Loss: 0.7309312224388123\n",
      "Epoch [87/200], Step [8001/149], D Loss: 1.460073709487915, G Loss: 0.6218922138214111\n",
      "Epoch [88/200], Step [1/149], D Loss: 1.645380973815918, G Loss: 0.6788333058357239\n",
      "Epoch [88/200], Step [1601/149], D Loss: 1.3565826416015625, G Loss: 0.6718590259552002\n",
      "Epoch [88/200], Step [3201/149], D Loss: 1.27787184715271, G Loss: 0.637123703956604\n",
      "Epoch [88/200], Step [4801/149], D Loss: 1.395901083946228, G Loss: 0.7262916564941406\n",
      "Epoch [88/200], Step [6401/149], D Loss: 1.4751594066619873, G Loss: 0.6705181002616882\n",
      "Epoch [88/200], Step [8001/149], D Loss: 1.4686295986175537, G Loss: 0.6525511741638184\n",
      "Epoch [89/200], Step [1/149], D Loss: 1.4106659889221191, G Loss: 0.836609423160553\n",
      "Epoch [89/200], Step [1601/149], D Loss: 1.3701293468475342, G Loss: 0.6912795305252075\n",
      "Epoch [89/200], Step [3201/149], D Loss: 1.3712737560272217, G Loss: 0.6426079273223877\n",
      "Epoch [89/200], Step [4801/149], D Loss: 1.3769328594207764, G Loss: 0.6919804811477661\n",
      "Epoch [89/200], Step [6401/149], D Loss: 1.2235500812530518, G Loss: 0.8388634920120239\n",
      "Epoch [89/200], Step [8001/149], D Loss: 1.2767448425292969, G Loss: 0.812350869178772\n",
      "Epoch [90/200], Step [1/149], D Loss: 1.9330897331237793, G Loss: 0.7405654191970825\n",
      "Epoch [90/200], Step [1601/149], D Loss: 1.536451816558838, G Loss: 0.5343098044395447\n",
      "Epoch [90/200], Step [3201/149], D Loss: 1.3428287506103516, G Loss: 0.5958437919616699\n",
      "Epoch [90/200], Step [4801/149], D Loss: 1.386318325996399, G Loss: 0.6639322638511658\n",
      "Epoch [90/200], Step [6401/149], D Loss: 1.4164878129959106, G Loss: 0.6825010776519775\n",
      "Epoch [90/200], Step [8001/149], D Loss: 1.3863003253936768, G Loss: 0.6847225427627563\n",
      "Epoch [91/200], Step [1/149], D Loss: 1.184852123260498, G Loss: 0.7070527076721191\n",
      "Epoch [91/200], Step [1601/149], D Loss: 1.178208589553833, G Loss: 0.7002604007720947\n",
      "Epoch [91/200], Step [3201/149], D Loss: 1.3150277137756348, G Loss: 0.6576961874961853\n",
      "Epoch [91/200], Step [4801/149], D Loss: 1.6680092811584473, G Loss: 0.6825445890426636\n",
      "Epoch [91/200], Step [6401/149], D Loss: 1.5257165431976318, G Loss: 0.7358667254447937\n",
      "Epoch [91/200], Step [8001/149], D Loss: 1.2258591651916504, G Loss: 1.1268377304077148\n",
      "Epoch [92/200], Step [1/149], D Loss: 1.542568325996399, G Loss: 1.2146514654159546\n",
      "Epoch [92/200], Step [1601/149], D Loss: 1.508634090423584, G Loss: 0.5546941757202148\n",
      "Epoch [92/200], Step [3201/149], D Loss: 1.3844025135040283, G Loss: 0.5432857871055603\n",
      "Epoch [92/200], Step [4801/149], D Loss: 1.3908329010009766, G Loss: 0.6078534722328186\n",
      "Epoch [92/200], Step [6401/149], D Loss: 1.3929905891418457, G Loss: 0.6476181149482727\n",
      "Epoch [92/200], Step [8001/149], D Loss: 1.3970661163330078, G Loss: 0.6631970405578613\n",
      "Epoch [93/200], Step [1/149], D Loss: 1.2316536903381348, G Loss: 0.6889073848724365\n",
      "Epoch [93/200], Step [1601/149], D Loss: 1.1424570083618164, G Loss: 0.6912964582443237\n",
      "Epoch [93/200], Step [3201/149], D Loss: 1.3586246967315674, G Loss: 0.6073004603385925\n",
      "Epoch [93/200], Step [4801/149], D Loss: 1.7495359182357788, G Loss: 0.7040683031082153\n",
      "Epoch [93/200], Step [6401/149], D Loss: 1.3054938316345215, G Loss: 1.1084402799606323\n",
      "Epoch [93/200], Step [8001/149], D Loss: 1.1842646598815918, G Loss: 1.1634249687194824\n",
      "Epoch [94/200], Step [1/149], D Loss: 1.7675715684890747, G Loss: 1.100021243095398\n",
      "Epoch [94/200], Step [1601/149], D Loss: 1.5199928283691406, G Loss: 0.5210638046264648\n",
      "Epoch [94/200], Step [3201/149], D Loss: 1.3633157014846802, G Loss: 0.5216389894485474\n",
      "Epoch [94/200], Step [4801/149], D Loss: 1.4047943353652954, G Loss: 0.5740455389022827\n",
      "Epoch [94/200], Step [6401/149], D Loss: 1.4080113172531128, G Loss: 0.6426785588264465\n",
      "Epoch [94/200], Step [8001/149], D Loss: 1.374882698059082, G Loss: 0.6949430704116821\n",
      "Epoch [95/200], Step [1/149], D Loss: 1.2572393417358398, G Loss: 0.6621010303497314\n",
      "Epoch [95/200], Step [1601/149], D Loss: 1.2984364032745361, G Loss: 0.6432615518569946\n",
      "Epoch [95/200], Step [3201/149], D Loss: 1.4221184253692627, G Loss: 0.7449808120727539\n",
      "Epoch [95/200], Step [4801/149], D Loss: 1.4691460132598877, G Loss: 0.8071976900100708\n",
      "Epoch [95/200], Step [6401/149], D Loss: 1.1748623847961426, G Loss: 1.087607741355896\n",
      "Epoch [95/200], Step [8001/149], D Loss: 1.1986843347549438, G Loss: 0.9653865098953247\n",
      "Epoch [96/200], Step [1/149], D Loss: 2.0412962436676025, G Loss: 0.8781458139419556\n",
      "Epoch [96/200], Step [1601/149], D Loss: 1.490009069442749, G Loss: 0.4772118031978607\n",
      "Epoch [96/200], Step [3201/149], D Loss: 1.2720308303833008, G Loss: 0.5241758227348328\n",
      "Epoch [96/200], Step [4801/149], D Loss: 1.334906816482544, G Loss: 0.663268506526947\n",
      "Epoch [96/200], Step [6401/149], D Loss: 1.4952850341796875, G Loss: 0.64741450548172\n",
      "Epoch [96/200], Step [8001/149], D Loss: 1.4630506038665771, G Loss: 0.6683080792427063\n",
      "Epoch [97/200], Step [1/149], D Loss: 1.3787133693695068, G Loss: 0.7325916290283203\n",
      "Epoch [97/200], Step [1601/149], D Loss: 1.3641935586929321, G Loss: 0.7108293175697327\n",
      "Epoch [97/200], Step [3201/149], D Loss: 1.3784334659576416, G Loss: 0.7233330011367798\n",
      "Epoch [97/200], Step [4801/149], D Loss: 1.3097550868988037, G Loss: 0.8186229467391968\n",
      "Epoch [97/200], Step [6401/149], D Loss: 1.1490137577056885, G Loss: 0.9408019781112671\n",
      "Epoch [97/200], Step [8001/149], D Loss: 1.3550002574920654, G Loss: 0.713607668876648\n",
      "Epoch [98/200], Step [1/149], D Loss: 2.2147135734558105, G Loss: 0.7475928664207458\n",
      "Epoch [98/200], Step [1601/149], D Loss: 1.4494596719741821, G Loss: 0.5437889099121094\n",
      "Epoch [98/200], Step [3201/149], D Loss: 1.2110397815704346, G Loss: 0.6077611446380615\n",
      "Epoch [98/200], Step [4801/149], D Loss: 1.3406593799591064, G Loss: 0.6834768652915955\n",
      "Epoch [98/200], Step [6401/149], D Loss: 1.515152931213379, G Loss: 0.6252313256263733\n",
      "Epoch [98/200], Step [8001/149], D Loss: 1.4950098991394043, G Loss: 0.6052169799804688\n",
      "Epoch [99/200], Step [1/149], D Loss: 1.3345338106155396, G Loss: 0.6210696697235107\n",
      "Epoch [99/200], Step [1601/149], D Loss: 1.375270128250122, G Loss: 0.7226554155349731\n",
      "Epoch [99/200], Step [3201/149], D Loss: 1.4152780771255493, G Loss: 0.7435153722763062\n",
      "Epoch [99/200], Step [4801/149], D Loss: 1.331317663192749, G Loss: 0.8888272047042847\n",
      "Epoch [99/200], Step [6401/149], D Loss: 1.1472861766815186, G Loss: 1.0378234386444092\n",
      "Epoch [99/200], Step [8001/149], D Loss: 1.2516968250274658, G Loss: 0.8976612687110901\n",
      "Epoch [100/200], Step [1/149], D Loss: 2.1136138439178467, G Loss: 0.882898211479187\n",
      "Epoch [100/200], Step [1601/149], D Loss: 1.5190420150756836, G Loss: 0.49933385848999023\n",
      "Epoch [100/200], Step [3201/149], D Loss: 1.2627419233322144, G Loss: 0.5486451387405396\n",
      "Epoch [100/200], Step [4801/149], D Loss: 1.3357875347137451, G Loss: 0.629291296005249\n",
      "Epoch [100/200], Step [6401/149], D Loss: 1.4356212615966797, G Loss: 0.645133912563324\n",
      "Epoch [100/200], Step [8001/149], D Loss: 1.402129888534546, G Loss: 0.6488911509513855\n",
      "Epoch [101/200], Step [1/149], D Loss: 1.2678163051605225, G Loss: 0.5865850448608398\n",
      "Epoch [101/200], Step [1601/149], D Loss: 1.3116378784179688, G Loss: 0.6575400233268738\n",
      "Epoch [101/200], Step [3201/149], D Loss: 1.4387201070785522, G Loss: 0.6998792886734009\n",
      "Epoch [101/200], Step [4801/149], D Loss: 1.463104486465454, G Loss: 0.850001335144043\n",
      "Epoch [101/200], Step [6401/149], D Loss: 1.2346820831298828, G Loss: 1.043473243713379\n",
      "Epoch [101/200], Step [8001/149], D Loss: 1.1972873210906982, G Loss: 1.0588176250457764\n",
      "Epoch [102/200], Step [1/149], D Loss: 1.8135268688201904, G Loss: 1.2566088438034058\n",
      "Epoch [102/200], Step [1601/149], D Loss: 1.6028380393981934, G Loss: 0.5378388166427612\n",
      "Epoch [102/200], Step [3201/149], D Loss: 1.3874495029449463, G Loss: 0.5515310168266296\n",
      "Epoch [102/200], Step [4801/149], D Loss: 1.3212319612503052, G Loss: 0.6685847043991089\n",
      "Epoch [102/200], Step [6401/149], D Loss: 1.4909776449203491, G Loss: 0.6233557462692261\n",
      "Epoch [102/200], Step [8001/149], D Loss: 1.335782527923584, G Loss: 0.721653938293457\n",
      "Epoch [103/200], Step [1/149], D Loss: 1.2383846044540405, G Loss: 0.6169325113296509\n",
      "Epoch [103/200], Step [1601/149], D Loss: 1.181007981300354, G Loss: 0.6721609830856323\n",
      "Epoch [103/200], Step [3201/149], D Loss: 1.3285934925079346, G Loss: 0.7147930264472961\n",
      "Epoch [103/200], Step [4801/149], D Loss: 1.7049087285995483, G Loss: 0.7104698419570923\n",
      "Epoch [103/200], Step [6401/149], D Loss: 1.2805542945861816, G Loss: 0.9812078475952148\n",
      "Epoch [103/200], Step [8001/149], D Loss: 1.1567466259002686, G Loss: 1.1309360265731812\n",
      "Epoch [104/200], Step [1/149], D Loss: 1.683127522468567, G Loss: 1.386812448501587\n",
      "Epoch [104/200], Step [1601/149], D Loss: 1.4917832612991333, G Loss: 0.5480731725692749\n",
      "Epoch [104/200], Step [3201/149], D Loss: 1.4477777481079102, G Loss: 0.520554780960083\n",
      "Epoch [104/200], Step [4801/149], D Loss: 1.3795547485351562, G Loss: 0.5617517828941345\n",
      "Epoch [104/200], Step [6401/149], D Loss: 1.317072868347168, G Loss: 0.6345703601837158\n",
      "Epoch [104/200], Step [8001/149], D Loss: 1.4419225454330444, G Loss: 0.653461217880249\n",
      "Epoch [105/200], Step [1/149], D Loss: 1.5202977657318115, G Loss: 0.6919715404510498\n",
      "Epoch [105/200], Step [1601/149], D Loss: 1.2876068353652954, G Loss: 0.6334013938903809\n",
      "Epoch [105/200], Step [3201/149], D Loss: 1.1064040660858154, G Loss: 0.7115533947944641\n",
      "Epoch [105/200], Step [4801/149], D Loss: 1.3138344287872314, G Loss: 0.7842556834220886\n",
      "Epoch [105/200], Step [6401/149], D Loss: 1.4843692779541016, G Loss: 0.6670631170272827\n",
      "Epoch [105/200], Step [8001/149], D Loss: 1.4872123003005981, G Loss: 0.6048049330711365\n",
      "Epoch [106/200], Step [1/149], D Loss: 1.2370893955230713, G Loss: 0.573021411895752\n",
      "Epoch [106/200], Step [1601/149], D Loss: 1.2944056987762451, G Loss: 0.663368821144104\n",
      "Epoch [106/200], Step [3201/149], D Loss: 1.520732045173645, G Loss: 0.6772686839103699\n",
      "Epoch [106/200], Step [4801/149], D Loss: 1.52821946144104, G Loss: 0.8691743016242981\n",
      "Epoch [106/200], Step [6401/149], D Loss: 1.2054091691970825, G Loss: 1.0586861371994019\n",
      "Epoch [106/200], Step [8001/149], D Loss: 1.098224401473999, G Loss: 1.1239093542099\n",
      "Epoch [107/200], Step [1/149], D Loss: 1.7803864479064941, G Loss: 1.4482431411743164\n",
      "Epoch [107/200], Step [1601/149], D Loss: 1.521395206451416, G Loss: 0.6057021021842957\n",
      "Epoch [107/200], Step [3201/149], D Loss: 1.4283485412597656, G Loss: 0.5326899886131287\n",
      "Epoch [107/200], Step [4801/149], D Loss: 1.4080820083618164, G Loss: 0.5643717050552368\n",
      "Epoch [107/200], Step [6401/149], D Loss: 1.3635518550872803, G Loss: 0.6099992394447327\n",
      "Epoch [107/200], Step [8001/149], D Loss: 1.4393432140350342, G Loss: 0.6432775259017944\n",
      "Epoch [108/200], Step [1/149], D Loss: 1.2252637147903442, G Loss: 0.6890534162521362\n",
      "Epoch [108/200], Step [1601/149], D Loss: 1.0275169610977173, G Loss: 0.7518509030342102\n",
      "Epoch [108/200], Step [3201/149], D Loss: 0.9508789777755737, G Loss: 0.8709447383880615\n",
      "Epoch [108/200], Step [4801/149], D Loss: 1.6597895622253418, G Loss: 0.7351153492927551\n",
      "Epoch [108/200], Step [6401/149], D Loss: 1.8176671266555786, G Loss: 0.5743367075920105\n",
      "Epoch [108/200], Step [8001/149], D Loss: 1.4819228649139404, G Loss: 0.7582452893257141\n",
      "Epoch [109/200], Step [1/149], D Loss: 1.4433598518371582, G Loss: 0.9822143912315369\n",
      "Epoch [109/200], Step [1601/149], D Loss: 1.3389556407928467, G Loss: 0.8424178957939148\n",
      "Epoch [109/200], Step [3201/149], D Loss: 1.3725495338439941, G Loss: 0.6671542525291443\n",
      "Epoch [109/200], Step [4801/149], D Loss: 1.2179702520370483, G Loss: 0.7834092378616333\n",
      "Epoch [109/200], Step [6401/149], D Loss: 1.043186068534851, G Loss: 0.9448004961013794\n",
      "Epoch [109/200], Step [8001/149], D Loss: 1.1842679977416992, G Loss: 0.9198369383811951\n",
      "Epoch [110/200], Step [1/149], D Loss: 2.318877696990967, G Loss: 1.0025782585144043\n",
      "Epoch [110/200], Step [1601/149], D Loss: 1.6832095384597778, G Loss: 0.49934229254722595\n",
      "Epoch [110/200], Step [3201/149], D Loss: 1.4395489692687988, G Loss: 0.4897429943084717\n",
      "Epoch [110/200], Step [4801/149], D Loss: 1.4023995399475098, G Loss: 0.5803242921829224\n",
      "Epoch [110/200], Step [6401/149], D Loss: 1.3772939443588257, G Loss: 0.6403753161430359\n",
      "Epoch [110/200], Step [8001/149], D Loss: 1.3891385793685913, G Loss: 0.6669934391975403\n",
      "Epoch [111/200], Step [1/149], D Loss: 1.14857816696167, G Loss: 0.6569715738296509\n",
      "Epoch [111/200], Step [1601/149], D Loss: 0.9466438293457031, G Loss: 0.8356075286865234\n",
      "Epoch [111/200], Step [3201/149], D Loss: 0.8576692342758179, G Loss: 1.0003612041473389\n",
      "Epoch [111/200], Step [4801/149], D Loss: 1.8075978755950928, G Loss: 0.65013587474823\n",
      "Epoch [111/200], Step [6401/149], D Loss: 1.7987720966339111, G Loss: 0.5714176297187805\n",
      "Epoch [111/200], Step [8001/149], D Loss: 1.5748358964920044, G Loss: 0.6421477794647217\n",
      "Epoch [112/200], Step [1/149], D Loss: 1.3723523616790771, G Loss: 0.8250555396080017\n",
      "Epoch [112/200], Step [1601/149], D Loss: 1.3858311176300049, G Loss: 0.7960822582244873\n",
      "Epoch [112/200], Step [3201/149], D Loss: 1.3644826412200928, G Loss: 0.7350257039070129\n",
      "Epoch [112/200], Step [4801/149], D Loss: 1.25495445728302, G Loss: 0.8038302063941956\n",
      "Epoch [112/200], Step [6401/149], D Loss: 1.0601170063018799, G Loss: 0.9365952610969543\n",
      "Epoch [112/200], Step [8001/149], D Loss: 1.3101458549499512, G Loss: 0.758953332901001\n",
      "Epoch [113/200], Step [1/149], D Loss: 2.472799777984619, G Loss: 0.5459820032119751\n",
      "Epoch [113/200], Step [1601/149], D Loss: 1.377587080001831, G Loss: 0.6515501737594604\n",
      "Epoch [113/200], Step [3201/149], D Loss: 1.2047449350357056, G Loss: 0.640911877155304\n",
      "Epoch [113/200], Step [4801/149], D Loss: 1.4139407873153687, G Loss: 0.6993740200996399\n",
      "Epoch [113/200], Step [6401/149], D Loss: 1.5241000652313232, G Loss: 0.6393943428993225\n",
      "Epoch [113/200], Step [8001/149], D Loss: 1.4727140665054321, G Loss: 0.6101306676864624\n",
      "Epoch [114/200], Step [1/149], D Loss: 1.3212249279022217, G Loss: 0.6236522197723389\n",
      "Epoch [114/200], Step [1601/149], D Loss: 1.3661245107650757, G Loss: 0.7491667866706848\n",
      "Epoch [114/200], Step [3201/149], D Loss: 1.3525354862213135, G Loss: 0.7925184965133667\n",
      "Epoch [114/200], Step [4801/149], D Loss: 1.2985632419586182, G Loss: 0.7750447988510132\n",
      "Epoch [114/200], Step [6401/149], D Loss: 1.1540772914886475, G Loss: 0.8822677135467529\n",
      "Epoch [114/200], Step [8001/149], D Loss: 1.3122519254684448, G Loss: 0.778990626335144\n",
      "Epoch [115/200], Step [1/149], D Loss: 2.104785919189453, G Loss: 0.7239351272583008\n",
      "Epoch [115/200], Step [1601/149], D Loss: 1.4751445055007935, G Loss: 0.5752010941505432\n",
      "Epoch [115/200], Step [3201/149], D Loss: 1.1818547248840332, G Loss: 0.6631501913070679\n",
      "Epoch [115/200], Step [4801/149], D Loss: 1.3727998733520508, G Loss: 0.7291297912597656\n",
      "Epoch [115/200], Step [6401/149], D Loss: 1.4699455499649048, G Loss: 0.6852190494537354\n",
      "Epoch [115/200], Step [8001/149], D Loss: 1.3978244066238403, G Loss: 0.6557230949401855\n",
      "Epoch [116/200], Step [1/149], D Loss: 1.1528677940368652, G Loss: 0.5992087721824646\n",
      "Epoch [116/200], Step [1601/149], D Loss: 1.2931735515594482, G Loss: 0.5709540843963623\n",
      "Epoch [116/200], Step [3201/149], D Loss: 1.4830355644226074, G Loss: 0.615582287311554\n",
      "Epoch [116/200], Step [4801/149], D Loss: 1.5886080265045166, G Loss: 0.7984187602996826\n",
      "Epoch [116/200], Step [6401/149], D Loss: 1.323788046836853, G Loss: 0.9559663534164429\n",
      "Epoch [116/200], Step [8001/149], D Loss: 1.165015697479248, G Loss: 1.1661620140075684\n",
      "Epoch [117/200], Step [1/149], D Loss: 1.482325553894043, G Loss: 1.66795015335083\n",
      "Epoch [117/200], Step [1601/149], D Loss: 1.420912742614746, G Loss: 0.6710248589515686\n",
      "Epoch [117/200], Step [3201/149], D Loss: 1.4730312824249268, G Loss: 0.5451682209968567\n",
      "Epoch [117/200], Step [4801/149], D Loss: 1.3747538328170776, G Loss: 0.5604826807975769\n",
      "Epoch [117/200], Step [6401/149], D Loss: 1.279442310333252, G Loss: 0.6577827334403992\n",
      "Epoch [117/200], Step [8001/149], D Loss: 1.4412777423858643, G Loss: 0.6470395922660828\n",
      "Epoch [118/200], Step [1/149], D Loss: 1.6453568935394287, G Loss: 0.6438890695571899\n",
      "Epoch [118/200], Step [1601/149], D Loss: 1.315077781677246, G Loss: 0.6271588206291199\n",
      "Epoch [118/200], Step [3201/149], D Loss: 1.1145377159118652, G Loss: 0.6898844242095947\n",
      "Epoch [118/200], Step [4801/149], D Loss: 1.3215502500534058, G Loss: 0.7497001886367798\n",
      "Epoch [118/200], Step [6401/149], D Loss: 1.4748516082763672, G Loss: 0.6686371564865112\n",
      "Epoch [118/200], Step [8001/149], D Loss: 1.4372581243515015, G Loss: 0.6277858018875122\n",
      "Epoch [119/200], Step [1/149], D Loss: 1.2551414966583252, G Loss: 0.4992707669734955\n",
      "Epoch [119/200], Step [1601/149], D Loss: 1.336496114730835, G Loss: 0.5680310726165771\n",
      "Epoch [119/200], Step [3201/149], D Loss: 1.5252866744995117, G Loss: 0.66583651304245\n",
      "Epoch [119/200], Step [4801/149], D Loss: 1.53871750831604, G Loss: 0.8801950216293335\n",
      "Epoch [119/200], Step [6401/149], D Loss: 1.2780400514602661, G Loss: 1.0258136987686157\n",
      "Epoch [119/200], Step [8001/149], D Loss: 1.1806261539459229, G Loss: 1.1380599737167358\n",
      "Epoch [120/200], Step [1/149], D Loss: 1.5425305366516113, G Loss: 1.5649677515029907\n",
      "Epoch [120/200], Step [1601/149], D Loss: 1.5263869762420654, G Loss: 0.6068296432495117\n",
      "Epoch [120/200], Step [3201/149], D Loss: 1.2622472047805786, G Loss: 0.7378869652748108\n",
      "Epoch [120/200], Step [4801/149], D Loss: 1.308826208114624, G Loss: 0.6196281313896179\n",
      "Epoch [120/200], Step [6401/149], D Loss: 1.4143786430358887, G Loss: 0.6038364768028259\n",
      "Epoch [120/200], Step [8001/149], D Loss: 1.4132641553878784, G Loss: 0.6628840565681458\n",
      "Epoch [121/200], Step [1/149], D Loss: 1.4186221361160278, G Loss: 0.6893360614776611\n",
      "Epoch [121/200], Step [1601/149], D Loss: 1.1963043212890625, G Loss: 0.655593752861023\n",
      "Epoch [121/200], Step [3201/149], D Loss: 1.0318009853363037, G Loss: 0.7599966526031494\n",
      "Epoch [121/200], Step [4801/149], D Loss: 1.6779303550720215, G Loss: 0.6295652389526367\n",
      "Epoch [121/200], Step [6401/149], D Loss: 1.745848298072815, G Loss: 0.5946676731109619\n",
      "Epoch [121/200], Step [8001/149], D Loss: 1.3618710041046143, G Loss: 0.9001670479774475\n",
      "Epoch [122/200], Step [1/149], D Loss: 1.4090882539749146, G Loss: 1.4467613697052002\n",
      "Epoch [122/200], Step [1601/149], D Loss: 1.4051072597503662, G Loss: 0.6283055543899536\n",
      "Epoch [122/200], Step [3201/149], D Loss: 1.394963026046753, G Loss: 0.5897897481918335\n",
      "Epoch [122/200], Step [4801/149], D Loss: 1.369434118270874, G Loss: 0.640466034412384\n",
      "Epoch [122/200], Step [6401/149], D Loss: 1.3148415088653564, G Loss: 0.6946800947189331\n",
      "Epoch [122/200], Step [8001/149], D Loss: 1.3960052728652954, G Loss: 0.6848041415214539\n",
      "Epoch [123/200], Step [1/149], D Loss: 1.5287938117980957, G Loss: 0.7025740742683411\n",
      "Epoch [123/200], Step [1601/149], D Loss: 1.3141961097717285, G Loss: 0.6266942024230957\n",
      "Epoch [123/200], Step [3201/149], D Loss: 1.184029221534729, G Loss: 0.676960825920105\n",
      "Epoch [123/200], Step [4801/149], D Loss: 1.4592862129211426, G Loss: 0.6759828329086304\n",
      "Epoch [123/200], Step [6401/149], D Loss: 1.52409029006958, G Loss: 0.6509891748428345\n",
      "Epoch [123/200], Step [8001/149], D Loss: 1.4787578582763672, G Loss: 0.6763880848884583\n",
      "Epoch [124/200], Step [1/149], D Loss: 1.359433650970459, G Loss: 0.7747686505317688\n",
      "Epoch [124/200], Step [1601/149], D Loss: 1.370659351348877, G Loss: 0.7404034733772278\n",
      "Epoch [124/200], Step [3201/149], D Loss: 1.3739078044891357, G Loss: 0.7255260348320007\n",
      "Epoch [124/200], Step [4801/149], D Loss: 1.2814936637878418, G Loss: 0.8102807402610779\n",
      "Epoch [124/200], Step [6401/149], D Loss: 1.1299736499786377, G Loss: 0.9453606605529785\n",
      "Epoch [124/200], Step [8001/149], D Loss: 1.3015179634094238, G Loss: 0.8197404742240906\n",
      "Epoch [125/200], Step [1/149], D Loss: 2.384603977203369, G Loss: 0.6266649961471558\n",
      "Epoch [125/200], Step [1601/149], D Loss: 1.402141809463501, G Loss: 0.5484235882759094\n",
      "Epoch [125/200], Step [3201/149], D Loss: 1.2203700542449951, G Loss: 0.6202672123908997\n",
      "Epoch [125/200], Step [4801/149], D Loss: 1.371716022491455, G Loss: 0.6776490807533264\n",
      "Epoch [125/200], Step [6401/149], D Loss: 1.4413203001022339, G Loss: 0.6605581045150757\n",
      "Epoch [125/200], Step [8001/149], D Loss: 1.3973815441131592, G Loss: 0.6557309627532959\n",
      "Epoch [126/200], Step [1/149], D Loss: 1.2089781761169434, G Loss: 0.5806660652160645\n",
      "Epoch [126/200], Step [1601/149], D Loss: 1.36607027053833, G Loss: 0.5551550388336182\n",
      "Epoch [126/200], Step [3201/149], D Loss: 1.5491139888763428, G Loss: 0.673565149307251\n",
      "Epoch [126/200], Step [4801/149], D Loss: 1.468228816986084, G Loss: 0.9356485605239868\n",
      "Epoch [126/200], Step [6401/149], D Loss: 1.1759072542190552, G Loss: 1.146836519241333\n",
      "Epoch [126/200], Step [8001/149], D Loss: 1.1817116737365723, G Loss: 1.0670925378799438\n",
      "Epoch [127/200], Step [1/149], D Loss: 1.8852142095565796, G Loss: 1.1831293106079102\n",
      "Epoch [127/200], Step [1601/149], D Loss: 1.5332504510879517, G Loss: 0.5148351788520813\n",
      "Epoch [127/200], Step [3201/149], D Loss: 1.3077419996261597, G Loss: 0.5308877229690552\n",
      "Epoch [127/200], Step [4801/149], D Loss: 1.3626065254211426, G Loss: 0.6116840243339539\n",
      "Epoch [127/200], Step [6401/149], D Loss: 1.4246189594268799, G Loss: 0.6506032943725586\n",
      "Epoch [127/200], Step [8001/149], D Loss: 1.4083151817321777, G Loss: 0.6632459163665771\n",
      "Epoch [128/200], Step [1/149], D Loss: 1.317226529121399, G Loss: 0.550584614276886\n",
      "Epoch [128/200], Step [1601/149], D Loss: 1.3622040748596191, G Loss: 0.7287706136703491\n",
      "Epoch [128/200], Step [3201/149], D Loss: 1.5018763542175293, G Loss: 0.7350800633430481\n",
      "Epoch [128/200], Step [4801/149], D Loss: 1.3755213022232056, G Loss: 0.8879355192184448\n",
      "Epoch [128/200], Step [6401/149], D Loss: 1.1169840097427368, G Loss: 1.1072595119476318\n",
      "Epoch [128/200], Step [8001/149], D Loss: 1.225682020187378, G Loss: 0.9008009433746338\n",
      "Epoch [129/200], Step [1/149], D Loss: 1.9661431312561035, G Loss: 1.1786774396896362\n",
      "Epoch [129/200], Step [1601/149], D Loss: 1.5756490230560303, G Loss: 0.4510766565799713\n",
      "Epoch [129/200], Step [3201/149], D Loss: 1.233474612236023, G Loss: 0.5492924451828003\n",
      "Epoch [129/200], Step [4801/149], D Loss: 1.3232241868972778, G Loss: 0.6838039755821228\n",
      "Epoch [129/200], Step [6401/149], D Loss: 1.5076823234558105, G Loss: 0.631005048751831\n",
      "Epoch [129/200], Step [8001/149], D Loss: 1.4774597883224487, G Loss: 0.6156268119812012\n",
      "Epoch [130/200], Step [1/149], D Loss: 1.4420156478881836, G Loss: 0.5223312377929688\n",
      "Epoch [130/200], Step [1601/149], D Loss: 1.3915820121765137, G Loss: 0.8808063268661499\n",
      "Epoch [130/200], Step [3201/149], D Loss: 1.2967289686203003, G Loss: 0.8369640111923218\n",
      "Epoch [130/200], Step [4801/149], D Loss: 1.3048255443572998, G Loss: 0.8175655603408813\n",
      "Epoch [130/200], Step [6401/149], D Loss: 1.1684472560882568, G Loss: 0.9408962726593018\n",
      "Epoch [130/200], Step [8001/149], D Loss: 1.349916934967041, G Loss: 0.7270616292953491\n",
      "Epoch [131/200], Step [1/149], D Loss: 2.063563823699951, G Loss: 0.8611732721328735\n",
      "Epoch [131/200], Step [1601/149], D Loss: 1.404710292816162, G Loss: 0.4648345112800598\n",
      "Epoch [131/200], Step [3201/149], D Loss: 1.1882295608520508, G Loss: 0.5580428838729858\n",
      "Epoch [131/200], Step [4801/149], D Loss: 1.3424304723739624, G Loss: 0.6587701439857483\n",
      "Epoch [131/200], Step [6401/149], D Loss: 1.458796739578247, G Loss: 0.6446222066879272\n",
      "Epoch [131/200], Step [8001/149], D Loss: 1.4501519203186035, G Loss: 0.6241834163665771\n",
      "Epoch [132/200], Step [1/149], D Loss: 1.3167235851287842, G Loss: 0.5300901532173157\n",
      "Epoch [132/200], Step [1601/149], D Loss: 1.3589065074920654, G Loss: 0.7174133062362671\n",
      "Epoch [132/200], Step [3201/149], D Loss: 1.4156068563461304, G Loss: 0.7530827522277832\n",
      "Epoch [132/200], Step [4801/149], D Loss: 1.412029504776001, G Loss: 0.8494031429290771\n",
      "Epoch [132/200], Step [6401/149], D Loss: 1.2014036178588867, G Loss: 1.0314006805419922\n",
      "Epoch [132/200], Step [8001/149], D Loss: 1.2333199977874756, G Loss: 0.9839209318161011\n",
      "Epoch [133/200], Step [1/149], D Loss: 1.9975647926330566, G Loss: 1.3189451694488525\n",
      "Epoch [133/200], Step [1601/149], D Loss: 1.4797406196594238, G Loss: 0.4993551969528198\n",
      "Epoch [133/200], Step [3201/149], D Loss: 1.2801202535629272, G Loss: 0.5244898200035095\n",
      "Epoch [133/200], Step [4801/149], D Loss: 1.3476850986480713, G Loss: 0.6257343292236328\n",
      "Epoch [133/200], Step [6401/149], D Loss: 1.413341760635376, G Loss: 0.6484019756317139\n",
      "Epoch [133/200], Step [8001/149], D Loss: 1.386824369430542, G Loss: 0.6411936283111572\n",
      "Epoch [134/200], Step [1/149], D Loss: 1.2262080907821655, G Loss: 0.5016255378723145\n",
      "Epoch [134/200], Step [1601/149], D Loss: 1.296151876449585, G Loss: 0.6428984999656677\n",
      "Epoch [134/200], Step [3201/149], D Loss: 1.4902952909469604, G Loss: 0.7383504509925842\n",
      "Epoch [134/200], Step [4801/149], D Loss: 1.5319103002548218, G Loss: 0.8494840860366821\n",
      "Epoch [134/200], Step [6401/149], D Loss: 1.3015565872192383, G Loss: 0.9862856268882751\n",
      "Epoch [134/200], Step [8001/149], D Loss: 1.174946665763855, G Loss: 1.2224104404449463\n",
      "Epoch [135/200], Step [1/149], D Loss: 1.6228668689727783, G Loss: 1.9748116731643677\n",
      "Epoch [135/200], Step [1601/149], D Loss: 1.4012553691864014, G Loss: 0.5854849815368652\n",
      "Epoch [135/200], Step [3201/149], D Loss: 1.4264159202575684, G Loss: 0.5634571313858032\n",
      "Epoch [135/200], Step [4801/149], D Loss: 1.3756959438323975, G Loss: 0.5578415393829346\n",
      "Epoch [135/200], Step [6401/149], D Loss: 1.3150296211242676, G Loss: 0.6391789317131042\n",
      "Epoch [135/200], Step [8001/149], D Loss: 1.4644683599472046, G Loss: 0.624447226524353\n",
      "Epoch [136/200], Step [1/149], D Loss: 1.4779751300811768, G Loss: 0.7631192207336426\n",
      "Epoch [136/200], Step [1601/149], D Loss: 1.2561619281768799, G Loss: 0.60759437084198\n",
      "Epoch [136/200], Step [3201/149], D Loss: 1.1131333112716675, G Loss: 0.6484020352363586\n",
      "Epoch [136/200], Step [4801/149], D Loss: 1.312631368637085, G Loss: 0.7429747581481934\n",
      "Epoch [136/200], Step [6401/149], D Loss: 1.4445918798446655, G Loss: 0.6730133891105652\n",
      "Epoch [136/200], Step [8001/149], D Loss: 1.4159815311431885, G Loss: 0.6197578310966492\n",
      "Epoch [137/200], Step [1/149], D Loss: 1.3953866958618164, G Loss: 0.38736778497695923\n",
      "Epoch [137/200], Step [1601/149], D Loss: 1.380713939666748, G Loss: 0.612661600112915\n",
      "Epoch [137/200], Step [3201/149], D Loss: 1.5378475189208984, G Loss: 0.7568223476409912\n",
      "Epoch [137/200], Step [4801/149], D Loss: 1.515520691871643, G Loss: 0.8827652931213379\n",
      "Epoch [137/200], Step [6401/149], D Loss: 1.2347772121429443, G Loss: 1.0724272727966309\n",
      "Epoch [137/200], Step [8001/149], D Loss: 1.1625323295593262, G Loss: 1.2006101608276367\n",
      "Epoch [138/200], Step [1/149], D Loss: 1.6716639995574951, G Loss: 1.9385771751403809\n",
      "Epoch [138/200], Step [1601/149], D Loss: 1.4413689374923706, G Loss: 0.5628995299339294\n",
      "Epoch [138/200], Step [3201/149], D Loss: 1.415975570678711, G Loss: 0.5596360564231873\n",
      "Epoch [138/200], Step [4801/149], D Loss: 1.376704454421997, G Loss: 0.5647497773170471\n",
      "Epoch [138/200], Step [6401/149], D Loss: 1.345201849937439, G Loss: 0.6315467357635498\n",
      "Epoch [138/200], Step [8001/149], D Loss: 1.4379693269729614, G Loss: 0.6482534408569336\n",
      "Epoch [139/200], Step [1/149], D Loss: 1.3371909856796265, G Loss: 0.7748839855194092\n",
      "Epoch [139/200], Step [1601/149], D Loss: 1.1928386688232422, G Loss: 0.6466711759567261\n",
      "Epoch [139/200], Step [3201/149], D Loss: 1.041590929031372, G Loss: 0.743087112903595\n",
      "Epoch [139/200], Step [4801/149], D Loss: 1.3607168197631836, G Loss: 0.8140518069267273\n",
      "Epoch [139/200], Step [6401/149], D Loss: 1.4962520599365234, G Loss: 0.6664736866950989\n",
      "Epoch [139/200], Step [8001/149], D Loss: 1.5137028694152832, G Loss: 0.5886510610580444\n",
      "Epoch [140/200], Step [1/149], D Loss: 1.5601884126663208, G Loss: 0.3692167103290558\n",
      "Epoch [140/200], Step [1601/149], D Loss: 1.4042788743972778, G Loss: 0.7637363076210022\n",
      "Epoch [140/200], Step [3201/149], D Loss: 1.406844139099121, G Loss: 0.8966883420944214\n",
      "Epoch [140/200], Step [4801/149], D Loss: 1.3160645961761475, G Loss: 0.9099043607711792\n",
      "Epoch [140/200], Step [6401/149], D Loss: 1.1302905082702637, G Loss: 0.9572570323944092\n",
      "Epoch [140/200], Step [8001/149], D Loss: 1.2216639518737793, G Loss: 0.9057323336601257\n",
      "Epoch [141/200], Step [1/149], D Loss: 2.20003342628479, G Loss: 1.1652953624725342\n",
      "Epoch [141/200], Step [1601/149], D Loss: 1.4461033344268799, G Loss: 0.5318264961242676\n",
      "Epoch [141/200], Step [3201/149], D Loss: 1.3122680187225342, G Loss: 0.5330610275268555\n",
      "Epoch [141/200], Step [4801/149], D Loss: 1.3683037757873535, G Loss: 0.6252525448799133\n",
      "Epoch [141/200], Step [6401/149], D Loss: 1.3845465183258057, G Loss: 0.6535425186157227\n",
      "Epoch [141/200], Step [8001/149], D Loss: 1.3590764999389648, G Loss: 0.6734816431999207\n",
      "Epoch [142/200], Step [1/149], D Loss: 1.022585391998291, G Loss: 0.6330623030662537\n",
      "Epoch [142/200], Step [1601/149], D Loss: 0.9953318238258362, G Loss: 0.7147011756896973\n",
      "Epoch [142/200], Step [3201/149], D Loss: 1.2167868614196777, G Loss: 0.718366801738739\n",
      "Epoch [142/200], Step [4801/149], D Loss: 1.6763019561767578, G Loss: 0.8010219931602478\n",
      "Epoch [142/200], Step [6401/149], D Loss: 1.5760847330093384, G Loss: 0.7953659892082214\n",
      "Epoch [142/200], Step [8001/149], D Loss: 1.408430576324463, G Loss: 0.9517127275466919\n",
      "Epoch [143/200], Step [1/149], D Loss: 1.405777096748352, G Loss: 1.9975008964538574\n",
      "Epoch [143/200], Step [1601/149], D Loss: 1.3615670204162598, G Loss: 0.6518716216087341\n",
      "Epoch [143/200], Step [3201/149], D Loss: 1.390155553817749, G Loss: 0.6755767464637756\n",
      "Epoch [143/200], Step [4801/149], D Loss: 1.2422634363174438, G Loss: 0.6859010457992554\n",
      "Epoch [143/200], Step [6401/149], D Loss: 1.1046961545944214, G Loss: 0.795563817024231\n",
      "Epoch [143/200], Step [8001/149], D Loss: 1.329596757888794, G Loss: 0.7336277961730957\n",
      "Epoch [144/200], Step [1/149], D Loss: 2.1332972049713135, G Loss: 0.9640885591506958\n",
      "Epoch [144/200], Step [1601/149], D Loss: 1.5654534101486206, G Loss: 0.4934319257736206\n",
      "Epoch [144/200], Step [3201/149], D Loss: 1.2020950317382812, G Loss: 0.6085548996925354\n",
      "Epoch [144/200], Step [4801/149], D Loss: 1.331886887550354, G Loss: 0.6755438446998596\n",
      "Epoch [144/200], Step [6401/149], D Loss: 1.4094338417053223, G Loss: 0.6668676733970642\n",
      "Epoch [144/200], Step [8001/149], D Loss: 1.3724485635757446, G Loss: 0.6552119255065918\n",
      "Epoch [145/200], Step [1/149], D Loss: 1.1368238925933838, G Loss: 0.5490393042564392\n",
      "Epoch [145/200], Step [1601/149], D Loss: 1.206054449081421, G Loss: 0.5690270662307739\n",
      "Epoch [145/200], Step [3201/149], D Loss: 1.3709100484848022, G Loss: 0.6652811169624329\n",
      "Epoch [145/200], Step [4801/149], D Loss: 1.572319746017456, G Loss: 0.8338034152984619\n",
      "Epoch [145/200], Step [6401/149], D Loss: 1.4587562084197998, G Loss: 0.8532217144966125\n",
      "Epoch [145/200], Step [8001/149], D Loss: 1.3311893939971924, G Loss: 0.9937008619308472\n",
      "Epoch [146/200], Step [1/149], D Loss: 1.4355157613754272, G Loss: 1.5370652675628662\n",
      "Epoch [146/200], Step [1601/149], D Loss: 1.3335447311401367, G Loss: 0.7754528522491455\n",
      "Epoch [146/200], Step [3201/149], D Loss: 1.376391887664795, G Loss: 0.6833704113960266\n",
      "Epoch [146/200], Step [4801/149], D Loss: 1.2524853944778442, G Loss: 0.6796057820320129\n",
      "Epoch [146/200], Step [6401/149], D Loss: 1.168651819229126, G Loss: 0.7516961097717285\n",
      "Epoch [146/200], Step [8001/149], D Loss: 1.4245119094848633, G Loss: 0.6716336607933044\n",
      "Epoch [147/200], Step [1/149], D Loss: 1.8871936798095703, G Loss: 0.7840981483459473\n",
      "Epoch [147/200], Step [1601/149], D Loss: 1.423317551612854, G Loss: 0.5546111464500427\n",
      "Epoch [147/200], Step [3201/149], D Loss: 1.3427231311798096, G Loss: 0.5483134984970093\n",
      "Epoch [147/200], Step [4801/149], D Loss: 1.3010847568511963, G Loss: 0.7983052134513855\n",
      "Epoch [147/200], Step [6401/149], D Loss: 1.3681721687316895, G Loss: 0.7576147317886353\n",
      "Epoch [147/200], Step [8001/149], D Loss: 1.4331941604614258, G Loss: 0.6205193400382996\n",
      "Epoch [148/200], Step [1/149], D Loss: 1.4410552978515625, G Loss: 0.5344849824905396\n",
      "Epoch [148/200], Step [1601/149], D Loss: 1.3375129699707031, G Loss: 0.84943687915802\n",
      "Epoch [148/200], Step [3201/149], D Loss: 1.3541775941848755, G Loss: 0.7330645322799683\n",
      "Epoch [148/200], Step [4801/149], D Loss: 1.3379836082458496, G Loss: 0.7767953872680664\n",
      "Epoch [148/200], Step [6401/149], D Loss: 1.270388126373291, G Loss: 0.7976805567741394\n",
      "Epoch [148/200], Step [8001/149], D Loss: 1.3699569702148438, G Loss: 0.74947589635849\n",
      "Epoch [149/200], Step [1/149], D Loss: 1.7602789402008057, G Loss: 0.8040233254432678\n",
      "Epoch [149/200], Step [1601/149], D Loss: 1.2491819858551025, G Loss: 0.5731848478317261\n",
      "Epoch [149/200], Step [3201/149], D Loss: 1.1355968713760376, G Loss: 0.621823251247406\n",
      "Epoch [149/200], Step [4801/149], D Loss: 1.3610150814056396, G Loss: 0.6981949806213379\n",
      "Epoch [149/200], Step [6401/149], D Loss: 1.4139819145202637, G Loss: 0.6841332912445068\n",
      "Epoch [149/200], Step [8001/149], D Loss: 1.4323813915252686, G Loss: 0.634964108467102\n",
      "Epoch [150/200], Step [1/149], D Loss: 1.3904484510421753, G Loss: 0.42782992124557495\n",
      "Epoch [150/200], Step [1601/149], D Loss: 1.4380594491958618, G Loss: 0.5920750498771667\n",
      "Epoch [150/200], Step [3201/149], D Loss: 1.3559966087341309, G Loss: 0.9659725427627563\n",
      "Epoch [150/200], Step [4801/149], D Loss: 1.3862953186035156, G Loss: 0.8727011680603027\n",
      "Epoch [150/200], Step [6401/149], D Loss: 1.228927731513977, G Loss: 0.9256681799888611\n",
      "Epoch [150/200], Step [8001/149], D Loss: 1.2898142337799072, G Loss: 0.8741762042045593\n",
      "Epoch [151/200], Step [1/149], D Loss: 2.029825210571289, G Loss: 0.8824774622917175\n",
      "Epoch [151/200], Step [1601/149], D Loss: 1.3618413209915161, G Loss: 0.5410447120666504\n",
      "Epoch [151/200], Step [3201/149], D Loss: 1.1552660465240479, G Loss: 0.6177089810371399\n",
      "Epoch [151/200], Step [4801/149], D Loss: 1.336185336112976, G Loss: 0.6902063488960266\n",
      "Epoch [151/200], Step [6401/149], D Loss: 1.3924819231033325, G Loss: 0.6838905811309814\n",
      "Epoch [151/200], Step [8001/149], D Loss: 1.3670167922973633, G Loss: 0.6726164817810059\n",
      "Epoch [152/200], Step [1/149], D Loss: 1.0381486415863037, G Loss: 0.5992238521575928\n",
      "Epoch [152/200], Step [1601/149], D Loss: 1.3417080640792847, G Loss: 0.473049134016037\n",
      "Epoch [152/200], Step [3201/149], D Loss: 1.5352652072906494, G Loss: 0.685521125793457\n",
      "Epoch [152/200], Step [4801/149], D Loss: 1.5751781463623047, G Loss: 1.0006035566329956\n",
      "Epoch [152/200], Step [6401/149], D Loss: 1.3006536960601807, G Loss: 1.1057896614074707\n",
      "Epoch [152/200], Step [8001/149], D Loss: 1.2469019889831543, G Loss: 1.0961977243423462\n",
      "Epoch [153/200], Step [1/149], D Loss: 1.7744892835617065, G Loss: 1.5030169486999512\n",
      "Epoch [153/200], Step [1601/149], D Loss: 1.4043477773666382, G Loss: 0.5786117315292358\n",
      "Epoch [153/200], Step [3201/149], D Loss: 1.320563793182373, G Loss: 0.546718180179596\n",
      "Epoch [153/200], Step [4801/149], D Loss: 1.4002264738082886, G Loss: 0.5960095524787903\n",
      "Epoch [153/200], Step [6401/149], D Loss: 1.3913639783859253, G Loss: 0.629992663860321\n",
      "Epoch [153/200], Step [8001/149], D Loss: 1.3874568939208984, G Loss: 0.6521503925323486\n",
      "Epoch [154/200], Step [1/149], D Loss: 1.1262524127960205, G Loss: 0.6514195203781128\n",
      "Epoch [154/200], Step [1601/149], D Loss: 1.0853359699249268, G Loss: 0.665901243686676\n",
      "Epoch [154/200], Step [3201/149], D Loss: 1.201176404953003, G Loss: 0.6660782098770142\n",
      "Epoch [154/200], Step [4801/149], D Loss: 1.4446712732315063, G Loss: 0.7887811660766602\n",
      "Epoch [154/200], Step [6401/149], D Loss: 1.4364142417907715, G Loss: 0.7565627098083496\n",
      "Epoch [154/200], Step [8001/149], D Loss: 1.4664607048034668, G Loss: 0.7419495582580566\n",
      "Epoch [155/200], Step [1/149], D Loss: 1.3757282495498657, G Loss: 1.6978955268859863\n",
      "Epoch [155/200], Step [1601/149], D Loss: 1.3449337482452393, G Loss: 0.6906505227088928\n",
      "Epoch [155/200], Step [3201/149], D Loss: 1.3203246593475342, G Loss: 0.6889079809188843\n",
      "Epoch [155/200], Step [4801/149], D Loss: 1.2989106178283691, G Loss: 0.6719754934310913\n",
      "Epoch [155/200], Step [6401/149], D Loss: 1.269113302230835, G Loss: 0.702732503414154\n",
      "Epoch [155/200], Step [8001/149], D Loss: 1.451125144958496, G Loss: 0.6751378774642944\n",
      "Epoch [156/200], Step [1/149], D Loss: 1.5536136627197266, G Loss: 0.7592363357543945\n",
      "Epoch [156/200], Step [1601/149], D Loss: 1.2399611473083496, G Loss: 0.6387345790863037\n",
      "Epoch [156/200], Step [3201/149], D Loss: 1.0796000957489014, G Loss: 0.7100476622581482\n",
      "Epoch [156/200], Step [4801/149], D Loss: 1.3416671752929688, G Loss: 0.7968057990074158\n",
      "Epoch [156/200], Step [6401/149], D Loss: 1.435164213180542, G Loss: 0.6999459266662598\n",
      "Epoch [156/200], Step [8001/149], D Loss: 1.3921523094177246, G Loss: 0.6583111882209778\n",
      "Epoch [157/200], Step [1/149], D Loss: 1.2747092247009277, G Loss: 0.4337705373764038\n",
      "Epoch [157/200], Step [1601/149], D Loss: 1.3663570880889893, G Loss: 0.5253058075904846\n",
      "Epoch [157/200], Step [3201/149], D Loss: 1.609816312789917, G Loss: 0.6847988367080688\n",
      "Epoch [157/200], Step [4801/149], D Loss: 1.652254581451416, G Loss: 0.8666442036628723\n",
      "Epoch [157/200], Step [6401/149], D Loss: 1.3330703973770142, G Loss: 1.01834237575531\n",
      "Epoch [157/200], Step [8001/149], D Loss: 1.1514103412628174, G Loss: 1.2628967761993408\n",
      "Epoch [158/200], Step [1/149], D Loss: 1.6711503267288208, G Loss: 2.114932060241699\n",
      "Epoch [158/200], Step [1601/149], D Loss: 1.4382094144821167, G Loss: 0.538999617099762\n",
      "Epoch [158/200], Step [3201/149], D Loss: 1.437331199645996, G Loss: 0.5551830530166626\n",
      "Epoch [158/200], Step [4801/149], D Loss: 1.394667148590088, G Loss: 0.5364895462989807\n",
      "Epoch [158/200], Step [6401/149], D Loss: 1.3087077140808105, G Loss: 0.6226628422737122\n",
      "Epoch [158/200], Step [8001/149], D Loss: 1.4734314680099487, G Loss: 0.6477660536766052\n",
      "Epoch [159/200], Step [1/149], D Loss: 1.4617784023284912, G Loss: 0.7772051095962524\n",
      "Epoch [159/200], Step [1601/149], D Loss: 1.2269389629364014, G Loss: 0.6522673964500427\n",
      "Epoch [159/200], Step [3201/149], D Loss: 1.0778329372406006, G Loss: 0.7185092568397522\n",
      "Epoch [159/200], Step [4801/149], D Loss: 1.3049514293670654, G Loss: 0.803794264793396\n",
      "Epoch [159/200], Step [6401/149], D Loss: 1.429222583770752, G Loss: 0.689563512802124\n",
      "Epoch [159/200], Step [8001/149], D Loss: 1.4001219272613525, G Loss: 0.625781774520874\n",
      "Epoch [160/200], Step [1/149], D Loss: 1.2999376058578491, G Loss: 0.4319440722465515\n",
      "Epoch [160/200], Step [1601/149], D Loss: 1.4282429218292236, G Loss: 0.5794506072998047\n",
      "Epoch [160/200], Step [3201/149], D Loss: 1.5197877883911133, G Loss: 0.8822230100631714\n",
      "Epoch [160/200], Step [4801/149], D Loss: 1.4210115671157837, G Loss: 1.078531265258789\n",
      "Epoch [160/200], Step [6401/149], D Loss: 1.0782499313354492, G Loss: 1.305497646331787\n",
      "Epoch [160/200], Step [8001/149], D Loss: 1.1473560333251953, G Loss: 1.0222578048706055\n",
      "Epoch [161/200], Step [1/149], D Loss: 2.3866052627563477, G Loss: 1.2970179319381714\n",
      "Epoch [161/200], Step [1601/149], D Loss: 1.5115880966186523, G Loss: 0.5043588876724243\n",
      "Epoch [161/200], Step [3201/149], D Loss: 1.2861193418502808, G Loss: 0.5247254967689514\n",
      "Epoch [161/200], Step [4801/149], D Loss: 1.3541412353515625, G Loss: 0.6046945452690125\n",
      "Epoch [161/200], Step [6401/149], D Loss: 1.4121170043945312, G Loss: 0.6311485171318054\n",
      "Epoch [161/200], Step [8001/149], D Loss: 1.33091402053833, G Loss: 0.6769436001777649\n",
      "Epoch [162/200], Step [1/149], D Loss: 1.0476140975952148, G Loss: 0.5831514596939087\n",
      "Epoch [162/200], Step [1601/149], D Loss: 1.0226035118103027, G Loss: 0.6910876035690308\n",
      "Epoch [162/200], Step [3201/149], D Loss: 1.2816165685653687, G Loss: 0.714482307434082\n",
      "Epoch [162/200], Step [4801/149], D Loss: 1.7484647035598755, G Loss: 0.761603832244873\n",
      "Epoch [162/200], Step [6401/149], D Loss: 1.5781383514404297, G Loss: 0.7975826263427734\n",
      "Epoch [162/200], Step [8001/149], D Loss: 1.3825414180755615, G Loss: 1.027979850769043\n",
      "Epoch [163/200], Step [1/149], D Loss: 1.415762186050415, G Loss: 1.7500243186950684\n",
      "Epoch [163/200], Step [1601/149], D Loss: 1.353295087814331, G Loss: 0.7153149843215942\n",
      "Epoch [163/200], Step [3201/149], D Loss: 1.3849270343780518, G Loss: 0.691326916217804\n",
      "Epoch [163/200], Step [4801/149], D Loss: 1.1863750219345093, G Loss: 0.7684001326560974\n",
      "Epoch [163/200], Step [6401/149], D Loss: 1.0086300373077393, G Loss: 0.9395423531532288\n",
      "Epoch [163/200], Step [8001/149], D Loss: 1.2024686336517334, G Loss: 0.8486272096633911\n",
      "Epoch [164/200], Step [1/149], D Loss: 2.224592924118042, G Loss: 1.1166677474975586\n",
      "Epoch [164/200], Step [1601/149], D Loss: 1.6510093212127686, G Loss: 0.4864184260368347\n",
      "Epoch [164/200], Step [3201/149], D Loss: 1.3998563289642334, G Loss: 0.4846147894859314\n",
      "Epoch [164/200], Step [4801/149], D Loss: 1.3400795459747314, G Loss: 0.5976575016975403\n",
      "Epoch [164/200], Step [6401/149], D Loss: 1.36495840549469, G Loss: 0.6449886560440063\n",
      "Epoch [164/200], Step [8001/149], D Loss: 1.3144145011901855, G Loss: 0.6795281171798706\n",
      "Epoch [165/200], Step [1/149], D Loss: 1.0055302381515503, G Loss: 0.6280866861343384\n",
      "Epoch [165/200], Step [1601/149], D Loss: 0.9394527673721313, G Loss: 0.7515057325363159\n",
      "Epoch [165/200], Step [3201/149], D Loss: 1.240149974822998, G Loss: 0.8213763236999512\n",
      "Epoch [165/200], Step [4801/149], D Loss: 1.7595751285552979, G Loss: 0.8737564086914062\n",
      "Epoch [165/200], Step [6401/149], D Loss: 1.5477559566497803, G Loss: 0.9314626455307007\n",
      "Epoch [165/200], Step [8001/149], D Loss: 1.2869831323623657, G Loss: 1.1260910034179688\n",
      "Epoch [166/200], Step [1/149], D Loss: 1.5144658088684082, G Loss: 1.6558414697647095\n",
      "Epoch [166/200], Step [1601/149], D Loss: 1.3613953590393066, G Loss: 0.6225827932357788\n",
      "Epoch [166/200], Step [3201/149], D Loss: 1.351794719696045, G Loss: 0.6631631851196289\n",
      "Epoch [166/200], Step [4801/149], D Loss: 1.2249624729156494, G Loss: 0.709032416343689\n",
      "Epoch [166/200], Step [6401/149], D Loss: 1.084407091140747, G Loss: 0.8462032079696655\n",
      "Epoch [166/200], Step [8001/149], D Loss: 1.3576362133026123, G Loss: 0.7043427228927612\n",
      "Epoch [167/200], Step [1/149], D Loss: 2.154151439666748, G Loss: 0.7919964790344238\n",
      "Epoch [167/200], Step [1601/149], D Loss: 1.517371654510498, G Loss: 0.5206314921379089\n",
      "Epoch [167/200], Step [3201/149], D Loss: 1.224091649055481, G Loss: 0.5845313668251038\n",
      "Epoch [167/200], Step [4801/149], D Loss: 1.3163752555847168, G Loss: 0.6478283405303955\n",
      "Epoch [167/200], Step [6401/149], D Loss: 1.3748517036437988, G Loss: 0.6565910577774048\n",
      "Epoch [167/200], Step [8001/149], D Loss: 1.3295693397521973, G Loss: 0.6712782382965088\n",
      "Epoch [168/200], Step [1/149], D Loss: 1.0495494604110718, G Loss: 0.5935916304588318\n",
      "Epoch [168/200], Step [1601/149], D Loss: 1.1649155616760254, G Loss: 0.6420648097991943\n",
      "Epoch [168/200], Step [3201/149], D Loss: 1.249441146850586, G Loss: 0.8881733417510986\n",
      "Epoch [168/200], Step [4801/149], D Loss: 1.6253401041030884, G Loss: 0.8290155529975891\n",
      "Epoch [168/200], Step [6401/149], D Loss: 1.349408507347107, G Loss: 1.0766735076904297\n",
      "Epoch [168/200], Step [8001/149], D Loss: 1.1168111562728882, G Loss: 1.2828887701034546\n",
      "Epoch [169/200], Step [1/149], D Loss: 1.673029899597168, G Loss: 1.5600731372833252\n",
      "Epoch [169/200], Step [1601/149], D Loss: 1.408604383468628, G Loss: 0.5566179156303406\n",
      "Epoch [169/200], Step [3201/149], D Loss: 1.3886423110961914, G Loss: 0.5474516749382019\n",
      "Epoch [169/200], Step [4801/149], D Loss: 1.3870702981948853, G Loss: 0.571884036064148\n",
      "Epoch [169/200], Step [6401/149], D Loss: 1.323956847190857, G Loss: 0.6402301788330078\n",
      "Epoch [169/200], Step [8001/149], D Loss: 1.4214165210723877, G Loss: 0.6640730500221252\n",
      "Epoch [170/200], Step [1/149], D Loss: 1.3950769901275635, G Loss: 0.7407583594322205\n",
      "Epoch [170/200], Step [1601/149], D Loss: 1.2034003734588623, G Loss: 0.6357139348983765\n",
      "Epoch [170/200], Step [3201/149], D Loss: 1.115614414215088, G Loss: 0.6653251051902771\n",
      "Epoch [170/200], Step [4801/149], D Loss: 1.3392882347106934, G Loss: 0.7244613170623779\n",
      "Epoch [170/200], Step [6401/149], D Loss: 1.4023597240447998, G Loss: 0.7067433595657349\n",
      "Epoch [170/200], Step [8001/149], D Loss: 1.3780252933502197, G Loss: 0.671574056148529\n",
      "Epoch [171/200], Step [1/149], D Loss: 1.241652250289917, G Loss: 0.47600066661834717\n",
      "Epoch [171/200], Step [1601/149], D Loss: 1.3340394496917725, G Loss: 0.5534284710884094\n",
      "Epoch [171/200], Step [3201/149], D Loss: 1.5027530193328857, G Loss: 0.7040554285049438\n",
      "Epoch [171/200], Step [4801/149], D Loss: 1.5742590427398682, G Loss: 0.8866270184516907\n",
      "Epoch [171/200], Step [6401/149], D Loss: 1.3094968795776367, G Loss: 1.0674821138381958\n",
      "Epoch [171/200], Step [8001/149], D Loss: 1.213376760482788, G Loss: 1.2507351636886597\n",
      "Epoch [172/200], Step [1/149], D Loss: 1.5254693031311035, G Loss: 2.0036754608154297\n",
      "Epoch [172/200], Step [1601/149], D Loss: 1.3752148151397705, G Loss: 0.595358669757843\n",
      "Epoch [172/200], Step [3201/149], D Loss: 1.384169578552246, G Loss: 0.6271603107452393\n",
      "Epoch [172/200], Step [4801/149], D Loss: 1.3252313137054443, G Loss: 0.5908604264259338\n",
      "Epoch [172/200], Step [6401/149], D Loss: 1.230172872543335, G Loss: 0.6611357927322388\n",
      "Epoch [172/200], Step [8001/149], D Loss: 1.5469883680343628, G Loss: 0.6377816796302795\n",
      "Epoch [173/200], Step [1/149], D Loss: 1.5131055116653442, G Loss: 0.7600141167640686\n",
      "Epoch [173/200], Step [1601/149], D Loss: 1.173673391342163, G Loss: 0.7007914185523987\n",
      "Epoch [173/200], Step [3201/149], D Loss: 1.1234153509140015, G Loss: 0.6782518029212952\n",
      "Epoch [173/200], Step [4801/149], D Loss: 1.3873875141143799, G Loss: 0.7311667799949646\n",
      "Epoch [173/200], Step [6401/149], D Loss: 1.4864344596862793, G Loss: 0.6733046174049377\n",
      "Epoch [173/200], Step [8001/149], D Loss: 1.4395735263824463, G Loss: 0.6308287382125854\n",
      "Epoch [174/200], Step [1/149], D Loss: 1.390350580215454, G Loss: 0.44200587272644043\n",
      "Epoch [174/200], Step [1601/149], D Loss: 1.4368224143981934, G Loss: 0.6192857623100281\n",
      "Epoch [174/200], Step [3201/149], D Loss: 1.4909100532531738, G Loss: 0.7580231428146362\n",
      "Epoch [174/200], Step [4801/149], D Loss: 1.4140199422836304, G Loss: 0.9121521711349487\n",
      "Epoch [174/200], Step [6401/149], D Loss: 1.211308479309082, G Loss: 1.0100950002670288\n",
      "Epoch [174/200], Step [8001/149], D Loss: 1.1732068061828613, G Loss: 1.0954128503799438\n",
      "Epoch [175/200], Step [1/149], D Loss: 1.7145320177078247, G Loss: 1.625045657157898\n",
      "Epoch [175/200], Step [1601/149], D Loss: 1.3636375665664673, G Loss: 0.6024821400642395\n",
      "Epoch [175/200], Step [3201/149], D Loss: 1.3902950286865234, G Loss: 0.5379073023796082\n",
      "Epoch [175/200], Step [4801/149], D Loss: 1.4275652170181274, G Loss: 0.5312217473983765\n",
      "Epoch [175/200], Step [6401/149], D Loss: 1.3914823532104492, G Loss: 0.5856413245201111\n",
      "Epoch [175/200], Step [8001/149], D Loss: 1.401904821395874, G Loss: 0.6265345811843872\n",
      "Epoch [176/200], Step [1/149], D Loss: 1.1928393840789795, G Loss: 0.6642091870307922\n",
      "Epoch [176/200], Step [1601/149], D Loss: 1.1651852130889893, G Loss: 0.6565651893615723\n",
      "Epoch [176/200], Step [3201/149], D Loss: 1.2805886268615723, G Loss: 0.651151716709137\n",
      "Epoch [176/200], Step [4801/149], D Loss: 1.3892954587936401, G Loss: 0.7786241769790649\n",
      "Epoch [176/200], Step [6401/149], D Loss: 1.3610422611236572, G Loss: 0.7836401462554932\n",
      "Epoch [176/200], Step [8001/149], D Loss: 1.353925108909607, G Loss: 0.8099698424339294\n",
      "Epoch [177/200], Step [1/149], D Loss: 1.3879657983779907, G Loss: 0.7369569540023804\n",
      "Epoch [177/200], Step [1601/149], D Loss: 1.3894472122192383, G Loss: 0.7390270233154297\n",
      "Epoch [177/200], Step [3201/149], D Loss: 1.4221217632293701, G Loss: 0.7658315300941467\n",
      "Epoch [177/200], Step [4801/149], D Loss: 1.2591452598571777, G Loss: 0.7873676419258118\n",
      "Epoch [177/200], Step [6401/149], D Loss: 1.1921786069869995, G Loss: 0.7664356231689453\n",
      "Epoch [177/200], Step [8001/149], D Loss: 1.484224796295166, G Loss: 0.6382029056549072\n",
      "Epoch [178/200], Step [1/149], D Loss: 1.5448198318481445, G Loss: 0.9795523881912231\n",
      "Epoch [178/200], Step [1601/149], D Loss: 1.3354742527008057, G Loss: 0.5674468278884888\n",
      "Epoch [178/200], Step [3201/149], D Loss: 1.346092700958252, G Loss: 0.6042435169219971\n",
      "Epoch [178/200], Step [4801/149], D Loss: 1.369267225265503, G Loss: 0.7270563840866089\n",
      "Epoch [178/200], Step [6401/149], D Loss: 1.418737530708313, G Loss: 0.6697144508361816\n",
      "Epoch [178/200], Step [8001/149], D Loss: 1.4562327861785889, G Loss: 0.6484857797622681\n",
      "Epoch [179/200], Step [1/149], D Loss: 1.3564791679382324, G Loss: 0.6544223427772522\n",
      "Epoch [179/200], Step [1601/149], D Loss: 1.3156155347824097, G Loss: 0.7771636843681335\n",
      "Epoch [179/200], Step [3201/149], D Loss: 1.3618721961975098, G Loss: 0.7039855718612671\n",
      "Epoch [179/200], Step [4801/149], D Loss: 1.4024218320846558, G Loss: 0.6668887734413147\n",
      "Epoch [179/200], Step [6401/149], D Loss: 1.365938425064087, G Loss: 0.6860692501068115\n",
      "Epoch [179/200], Step [8001/149], D Loss: 1.3915454149246216, G Loss: 0.7100757360458374\n",
      "Epoch [180/200], Step [1/149], D Loss: 1.4552743434906006, G Loss: 0.8264774680137634\n",
      "Epoch [180/200], Step [1601/149], D Loss: 1.2987492084503174, G Loss: 0.5821760296821594\n",
      "Epoch [180/200], Step [3201/149], D Loss: 1.3164246082305908, G Loss: 0.6481547355651855\n",
      "Epoch [180/200], Step [4801/149], D Loss: 1.3946682214736938, G Loss: 0.721903920173645\n",
      "Epoch [180/200], Step [6401/149], D Loss: 1.4051474332809448, G Loss: 0.6979987621307373\n",
      "Epoch [180/200], Step [8001/149], D Loss: 1.3939263820648193, G Loss: 0.7206480503082275\n",
      "Epoch [181/200], Step [1/149], D Loss: 1.3058770895004272, G Loss: 0.9266110062599182\n",
      "Epoch [181/200], Step [1601/149], D Loss: 1.283656120300293, G Loss: 0.6482083797454834\n",
      "Epoch [181/200], Step [3201/149], D Loss: 1.3627341985702515, G Loss: 0.6524797677993774\n",
      "Epoch [181/200], Step [4801/149], D Loss: 1.3908541202545166, G Loss: 0.7090155482292175\n",
      "Epoch [181/200], Step [6401/149], D Loss: 1.3708034753799438, G Loss: 0.7154086828231812\n",
      "Epoch [181/200], Step [8001/149], D Loss: 1.3819396495819092, G Loss: 0.7262921333312988\n",
      "Epoch [182/200], Step [1/149], D Loss: 1.3308515548706055, G Loss: 0.8677037358283997\n",
      "Epoch [182/200], Step [1601/149], D Loss: 1.3041822910308838, G Loss: 0.6005755662918091\n",
      "Epoch [182/200], Step [3201/149], D Loss: 1.3414270877838135, G Loss: 0.6664484143257141\n",
      "Epoch [182/200], Step [4801/149], D Loss: 1.392020344734192, G Loss: 0.7228964567184448\n",
      "Epoch [182/200], Step [6401/149], D Loss: 1.3795361518859863, G Loss: 0.7179160118103027\n",
      "Epoch [182/200], Step [8001/149], D Loss: 1.3793492317199707, G Loss: 0.7232709527015686\n",
      "Epoch [183/200], Step [1/149], D Loss: 1.3507177829742432, G Loss: 0.887525200843811\n",
      "Epoch [183/200], Step [1601/149], D Loss: 1.323362112045288, G Loss: 0.5812017917633057\n",
      "Epoch [183/200], Step [3201/149], D Loss: 1.3498461246490479, G Loss: 0.6787168383598328\n",
      "Epoch [183/200], Step [4801/149], D Loss: 1.3802566528320312, G Loss: 0.7327798008918762\n",
      "Epoch [183/200], Step [6401/149], D Loss: 1.3908171653747559, G Loss: 0.7118712663650513\n",
      "Epoch [183/200], Step [8001/149], D Loss: 1.3800206184387207, G Loss: 0.7294942736625671\n",
      "Epoch [184/200], Step [1/149], D Loss: 1.3391549587249756, G Loss: 0.8513335585594177\n",
      "Epoch [184/200], Step [1601/149], D Loss: 1.319923758506775, G Loss: 0.5951387286186218\n",
      "Epoch [184/200], Step [3201/149], D Loss: 1.387796401977539, G Loss: 0.6927099227905273\n",
      "Epoch [184/200], Step [4801/149], D Loss: 1.3775910139083862, G Loss: 0.7616289854049683\n",
      "Epoch [184/200], Step [6401/149], D Loss: 1.3696355819702148, G Loss: 0.7351524829864502\n",
      "Epoch [184/200], Step [8001/149], D Loss: 1.369685173034668, G Loss: 0.7318260073661804\n",
      "Epoch [185/200], Step [1/149], D Loss: 1.3653290271759033, G Loss: 0.9900040030479431\n",
      "Epoch [185/200], Step [1601/149], D Loss: 1.377732515335083, G Loss: 0.5622003674507141\n",
      "Epoch [185/200], Step [3201/149], D Loss: 1.3903666734695435, G Loss: 0.6461412906646729\n",
      "Epoch [185/200], Step [4801/149], D Loss: 1.3765989542007446, G Loss: 0.7031793594360352\n",
      "Epoch [185/200], Step [6401/149], D Loss: 1.3675036430358887, G Loss: 0.7073460221290588\n",
      "Epoch [185/200], Step [8001/149], D Loss: 1.379277229309082, G Loss: 0.7151813507080078\n",
      "Epoch [186/200], Step [1/149], D Loss: 1.3339390754699707, G Loss: 0.8185531497001648\n",
      "Epoch [186/200], Step [1601/149], D Loss: 1.3602690696716309, G Loss: 0.5890309810638428\n",
      "Epoch [186/200], Step [3201/149], D Loss: 1.3864591121673584, G Loss: 0.673701286315918\n",
      "Epoch [186/200], Step [4801/149], D Loss: 1.368988275527954, G Loss: 0.7226144075393677\n",
      "Epoch [186/200], Step [6401/149], D Loss: 1.3734182119369507, G Loss: 0.7047165036201477\n",
      "Epoch [186/200], Step [8001/149], D Loss: 1.3680858612060547, G Loss: 0.7363961935043335\n",
      "Epoch [187/200], Step [1/149], D Loss: 1.370455026626587, G Loss: 0.9402897953987122\n",
      "Epoch [187/200], Step [1601/149], D Loss: 1.2991969585418701, G Loss: 0.6037304997444153\n",
      "Epoch [187/200], Step [3201/149], D Loss: 1.3492202758789062, G Loss: 0.700671911239624\n",
      "Epoch [187/200], Step [4801/149], D Loss: 1.380779504776001, G Loss: 0.7551110982894897\n",
      "Epoch [187/200], Step [6401/149], D Loss: 1.3882966041564941, G Loss: 0.7069845795631409\n",
      "Epoch [187/200], Step [8001/149], D Loss: 1.3629405498504639, G Loss: 0.7536851167678833\n",
      "Epoch [188/200], Step [1/149], D Loss: 1.3439182043075562, G Loss: 1.136172890663147\n",
      "Epoch [188/200], Step [1601/149], D Loss: 1.2993825674057007, G Loss: 0.5786016583442688\n",
      "Epoch [188/200], Step [3201/149], D Loss: 1.3893887996673584, G Loss: 0.6261497139930725\n",
      "Epoch [188/200], Step [4801/149], D Loss: 1.3848025798797607, G Loss: 0.7369783520698547\n",
      "Epoch [188/200], Step [6401/149], D Loss: 1.3799771070480347, G Loss: 0.7104730010032654\n",
      "Epoch [188/200], Step [8001/149], D Loss: 1.368715763092041, G Loss: 0.7243896722793579\n",
      "Epoch [189/200], Step [1/149], D Loss: 1.3358700275421143, G Loss: 1.0073707103729248\n",
      "Epoch [189/200], Step [1601/149], D Loss: 1.3371037244796753, G Loss: 0.6041918992996216\n",
      "Epoch [189/200], Step [3201/149], D Loss: 1.352254033088684, G Loss: 0.6804114580154419\n",
      "Epoch [189/200], Step [4801/149], D Loss: 1.3800092935562134, G Loss: 0.7095045447349548\n",
      "Epoch [189/200], Step [6401/149], D Loss: 1.380487322807312, G Loss: 0.7146241068840027\n",
      "Epoch [189/200], Step [8001/149], D Loss: 1.3318411111831665, G Loss: 0.7498573064804077\n",
      "Epoch [190/200], Step [1/149], D Loss: 1.3049581050872803, G Loss: 0.9587290287017822\n",
      "Epoch [190/200], Step [1601/149], D Loss: 1.3533843755722046, G Loss: 0.6104710698127747\n",
      "Epoch [190/200], Step [3201/149], D Loss: 1.3731234073638916, G Loss: 0.7094276547431946\n",
      "Epoch [190/200], Step [4801/149], D Loss: 1.380552887916565, G Loss: 0.6834204196929932\n",
      "Epoch [190/200], Step [6401/149], D Loss: 1.379953145980835, G Loss: 0.6889439821243286\n",
      "Epoch [190/200], Step [8001/149], D Loss: 1.4012686014175415, G Loss: 0.689131498336792\n",
      "Epoch [191/200], Step [1/149], D Loss: 1.3150498867034912, G Loss: 0.828190803527832\n",
      "Epoch [191/200], Step [1601/149], D Loss: 1.3030402660369873, G Loss: 0.6306692361831665\n",
      "Epoch [191/200], Step [3201/149], D Loss: 1.390455722808838, G Loss: 0.722197949886322\n",
      "Epoch [191/200], Step [4801/149], D Loss: 1.3601150512695312, G Loss: 0.715742826461792\n",
      "Epoch [191/200], Step [6401/149], D Loss: 1.3664742708206177, G Loss: 0.6952915191650391\n",
      "Epoch [191/200], Step [8001/149], D Loss: 1.3706560134887695, G Loss: 0.7058296203613281\n",
      "Epoch [192/200], Step [1/149], D Loss: 1.311823844909668, G Loss: 0.8162811398506165\n",
      "Epoch [192/200], Step [1601/149], D Loss: 1.3892438411712646, G Loss: 0.6086477041244507\n",
      "Epoch [192/200], Step [3201/149], D Loss: 1.2505277395248413, G Loss: 0.9019176363945007\n",
      "Epoch [192/200], Step [4801/149], D Loss: 1.2023788690567017, G Loss: 0.8699194192886353\n",
      "Epoch [192/200], Step [6401/149], D Loss: 1.4814858436584473, G Loss: 0.6130626201629639\n",
      "Epoch [192/200], Step [8001/149], D Loss: 1.3799159526824951, G Loss: 0.7139134407043457\n",
      "Epoch [193/200], Step [1/149], D Loss: 1.3754217624664307, G Loss: 0.7034408450126648\n",
      "Epoch [193/200], Step [1601/149], D Loss: 1.409338116645813, G Loss: 0.6302924156188965\n",
      "Epoch [193/200], Step [3201/149], D Loss: 1.4211506843566895, G Loss: 0.7180426120758057\n",
      "Epoch [193/200], Step [4801/149], D Loss: 1.3708628416061401, G Loss: 0.6640900373458862\n",
      "Epoch [193/200], Step [6401/149], D Loss: 1.4114940166473389, G Loss: 0.6519863605499268\n",
      "Epoch [193/200], Step [8001/149], D Loss: 1.3989157676696777, G Loss: 0.6758337020874023\n",
      "Epoch [194/200], Step [1/149], D Loss: 1.2647278308868408, G Loss: 0.7649058103561401\n",
      "Epoch [194/200], Step [1601/149], D Loss: 1.4158012866973877, G Loss: 0.5920727849006653\n",
      "Epoch [194/200], Step [3201/149], D Loss: 1.391921043395996, G Loss: 0.7252548933029175\n",
      "Epoch [194/200], Step [4801/149], D Loss: 1.4036403894424438, G Loss: 0.6671801209449768\n",
      "Epoch [194/200], Step [6401/149], D Loss: 1.4008034467697144, G Loss: 0.6809220314025879\n",
      "Epoch [194/200], Step [8001/149], D Loss: 1.3913686275482178, G Loss: 0.6853044033050537\n",
      "Epoch [195/200], Step [1/149], D Loss: 1.390347957611084, G Loss: 0.6469766497612\n",
      "Epoch [195/200], Step [1601/149], D Loss: 1.407766342163086, G Loss: 0.6504600048065186\n",
      "Epoch [195/200], Step [3201/149], D Loss: 1.3879774808883667, G Loss: 0.7242841124534607\n",
      "Epoch [195/200], Step [4801/149], D Loss: 1.3742671012878418, G Loss: 0.6493844985961914\n",
      "Epoch [195/200], Step [6401/149], D Loss: 1.3770091533660889, G Loss: 0.689078688621521\n",
      "Epoch [195/200], Step [8001/149], D Loss: 1.3876068592071533, G Loss: 0.6874963641166687\n",
      "Epoch [196/200], Step [1/149], D Loss: 1.3709908723831177, G Loss: 0.6219145655632019\n",
      "Epoch [196/200], Step [1601/149], D Loss: 1.3824061155319214, G Loss: 0.6419731378555298\n",
      "Epoch [196/200], Step [3201/149], D Loss: 1.3940171003341675, G Loss: 0.7320810556411743\n",
      "Epoch [196/200], Step [4801/149], D Loss: 1.3678441047668457, G Loss: 0.6842984557151794\n",
      "Epoch [196/200], Step [6401/149], D Loss: 1.3605663776397705, G Loss: 0.6959428787231445\n",
      "Epoch [196/200], Step [8001/149], D Loss: 1.3905528783798218, G Loss: 0.7016974687576294\n",
      "Epoch [197/200], Step [1/149], D Loss: 1.4250967502593994, G Loss: 0.5969142913818359\n",
      "Epoch [197/200], Step [1601/149], D Loss: 1.377941608428955, G Loss: 0.6874367594718933\n",
      "Epoch [197/200], Step [3201/149], D Loss: 1.386670708656311, G Loss: 0.7336288094520569\n",
      "Epoch [197/200], Step [4801/149], D Loss: 1.39034903049469, G Loss: 0.6755034327507019\n",
      "Epoch [197/200], Step [6401/149], D Loss: 1.3994340896606445, G Loss: 0.716684103012085\n",
      "Epoch [197/200], Step [8001/149], D Loss: 1.3419710397720337, G Loss: 0.7430890798568726\n",
      "Epoch [198/200], Step [1/149], D Loss: 1.2790195941925049, G Loss: 0.7574992179870605\n",
      "Epoch [198/200], Step [1601/149], D Loss: 1.4064133167266846, G Loss: 0.5941261649131775\n",
      "Epoch [198/200], Step [3201/149], D Loss: 1.3711943626403809, G Loss: 0.7119978666305542\n",
      "Epoch [198/200], Step [4801/149], D Loss: 1.3794047832489014, G Loss: 0.7071874141693115\n",
      "Epoch [198/200], Step [6401/149], D Loss: 1.357161283493042, G Loss: 0.7033067345619202\n",
      "Epoch [198/200], Step [8001/149], D Loss: 1.383958101272583, G Loss: 0.6966620683670044\n",
      "Epoch [199/200], Step [1/149], D Loss: 1.3239176273345947, G Loss: 0.8703011274337769\n",
      "Epoch [199/200], Step [1601/149], D Loss: 1.3989415168762207, G Loss: 0.635703444480896\n",
      "Epoch [199/200], Step [3201/149], D Loss: 1.436768889427185, G Loss: 0.7518615126609802\n",
      "Epoch [199/200], Step [4801/149], D Loss: 1.3901910781860352, G Loss: 0.627927839756012\n",
      "Epoch [199/200], Step [6401/149], D Loss: 1.380645513534546, G Loss: 0.6960902810096741\n",
      "Epoch [199/200], Step [8001/149], D Loss: 1.3312568664550781, G Loss: 0.7367592453956604\n",
      "Epoch [200/200], Step [1/149], D Loss: 1.5876884460449219, G Loss: 0.3587929904460907\n",
      "Epoch [200/200], Step [1601/149], D Loss: 1.3734278678894043, G Loss: 0.7392945289611816\n",
      "Epoch [200/200], Step [3201/149], D Loss: 1.4005399942398071, G Loss: 0.6839228272438049\n",
      "Epoch [200/200], Step [4801/149], D Loss: 1.3950990438461304, G Loss: 0.7328085899353027\n",
      "Epoch [200/200], Step [6401/149], D Loss: 1.4309380054473877, G Loss: 0.6755259037017822\n",
      "Epoch [200/200], Step [8001/149], D Loss: 1.3582838773727417, G Loss: 0.7065359354019165\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "n_epochs = 200  # ADJUST\n",
    "batch_size = 64  # ADJUST\n",
    "noise_dim = 2  # Size of noise vector\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(data_tensor), batch_size):\n",
    "        # Prepare real data batch\n",
    "        real_data = data_tensor[i:i+batch_size, 1:]  # Exclude timestamp from training\n",
    "        real_labels = torch.ones(real_data.size(0), 1)\n",
    "        fake_labels = torch.zeros(real_data.size(0), 1)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real data loss\n",
    "        real_output = discriminator(real_data)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Generate fake data\n",
    "        noise = torch.randn(real_data.size(0), noise_dim)\n",
    "        # Conditional input, for now, let's use a random slice from the best_ask_price as an example\n",
    "        conditional_input = real_data[:, 0].unsqueeze(1)  # This should be modified based on your specific conditional input\n",
    "\n",
    "\n",
    "        # Right before generator(noise, conditional_input) call\n",
    "        # print(\"Conditional input shape before generator:\", conditional_input.shape)\n",
    "\n",
    "        fake_data = generator(noise, conditional_input)\n",
    "        \n",
    "        # Fake data loss\n",
    "        fake_output = discriminator(fake_data.detach())  # Detach to avoid training generator on these labels\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # Combine loss and update discriminator\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Trick discriminator into thinking the generated data is real\n",
    "        output = discriminator(fake_data)\n",
    "        g_loss = criterion(output, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if i % 100 == 0:  # Adjust printing frequency based on your preference\n",
    "            print(f\"Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{len(data_tensor)//batch_size}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator state has been saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the generator's state dictionary\n",
    "torch.save(generator.state_dict(), '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/Saved_Generator_States/generator_state_dict.pth')\n",
    "print(\"Generator state has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Generate Example Orderbook Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Order Book Snapshot: [[2.1985203e+03 4.8230276e+00 2.1969265e+03 4.6494180e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a sample order book snapshot\n",
    "with torch.no_grad():\n",
    "    test_noise = torch.randn(1, noise_dim)\n",
    "    test_price_level = torch.tensor([[0.5]])  # Example price level, normalized\n",
    "    generated_snapshot = generator(test_noise, test_price_level)\n",
    "    inverse_transformed_snapshot = scaler.inverse_transform(generated_snapshot.numpy())\n",
    "    print(\"Generated Order Book Snapshot:\", inverse_transformed_snapshot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
