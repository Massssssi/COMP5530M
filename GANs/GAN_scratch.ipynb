{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Development - Kochems Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from data_reader import DataReader\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ask_bid_int(dataset):\n",
    "    mask_ask = np.char.endswith(dataset[:,:,2], 'ask')\n",
    "    mask_bid = np.char.endswith(dataset[:,:,2], 'bid')\n",
    "    dataset[:,:,2][mask_ask] = '1'\n",
    "    dataset[:,:,2][mask_bid] = '0'\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "def get_dataset_max_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = 0\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2-(level))\n",
    "    last_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    max_val = np.max(last_row_prices)\n",
    "    return max_val\n",
    "\n",
    "def get_dataset_min_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = -1\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2+(level-1))\n",
    "    first_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    min_val = np.min(first_row_prices)\n",
    "    return min_val\n",
    "\n",
    "def make_histogram_from_dataset(dataset, rows_per_orderbook = 100, bin_width = 0.5, level = -1):\n",
    "    X_train = []\n",
    "    hist_max = get_dataset_max_price(dataset, rows_per_orderbook, level)\n",
    "    hist_min = get_dataset_min_price(dataset, rows_per_orderbook, level)\n",
    "    print(\"range: \", hist_min, \" \", hist_max)\n",
    "    num_bins = int(np.ceil((hist_max-hist_min) / bin_width))\n",
    "    bins = np.linspace(hist_min, hist_max, num_bins)\n",
    "    for i in range(len(dataset)):\n",
    "        orderbook = dataset[i]\n",
    "        price = orderbook[:,0]\n",
    "        quantity = orderbook[:,1]\n",
    "        quantity[orderbook[:, 2] == 0] *= -1\n",
    "        hist, bin_edges = np.histogram(price, bins=bins, weights=quantity)\n",
    "        X_train.append(hist)\n",
    "    X_train = np.array(X_train)\n",
    "    return X_train, hist_min, hist_max, bins\n",
    "\n",
    "def make_centred_LOB_snapshots(histograms, level = 1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(histograms)-1):\n",
    "        current_OB = histograms[i]\n",
    "        next_OB = histograms[i+1]\n",
    "        j = -1\n",
    "        while j < len(current_OB)-1 and not (current_OB[j] < 0 and current_OB[j+1] > 0): j+=1\n",
    "        j+=1\n",
    "        \n",
    "        current_start_index = j-level\n",
    "        current_subarray_size = 2 * level\n",
    "        current_centre_LOB_snapshot = current_OB[current_start_index: current_start_index + current_subarray_size]\n",
    "        \n",
    "        next_subarray_size = 2 * current_subarray_size\n",
    "        next_centre_LOB_snapshot = next_OB[current_start_index - level: (current_start_index - level) + next_subarray_size]\n",
    "        \n",
    "        if np.any(next_centre_LOB_snapshot == 0):\n",
    "            continue\n",
    "        \n",
    "        X_train.append(current_centre_LOB_snapshot)\n",
    "        y_train.append(next_centre_LOB_snapshot)\n",
    "    \n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.vstack(y_train)\n",
    "    \n",
    "    return X_train, y_train\n",
    "        \n",
    "def normalization(X_train, c=8):\n",
    "    sqrt_abs_over_c = np.sqrt(np.abs(X_train)) / c\n",
    "    with_sign = np.sign(X_train) * sqrt_abs_over_c\n",
    "    return with_sign\n",
    "\n",
    "def get_centre_of_LOB(X_t_delta_t, level=3):\n",
    "    index = -1\n",
    "    for i in range(len(X_t_delta_t)-1):\n",
    "        if X_t_delta_t[i] < 0 and X_t_delta_t[i+1]>0: index = i+1\n",
    "    return X_t_delta_t[(index-level): (index-level) + 2 * level]\n",
    "\n",
    "def get_centre_of_LOB_dataset(X_t_delta_t_dataset, level = 3):\n",
    "    centres = []\n",
    "    for i in range(len(X_t_delta_t_dataset)):\n",
    "        if np.any(X_t_delta_t_dataset[i] == 0): continue\n",
    "        centre = get_centre_of_LOB(X_t_delta_t_dataset[i], level)\n",
    "        if centre.size != 6: continue\n",
    "        centres.append(centre)\n",
    "    np.array(centres)\n",
    "    return centres\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to post-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_normalization(X_train_normalized, c=8):\n",
    "    if not isinstance(X_train_normalized, np.ndarray) and X_train_normalized.device.type == 'cuda':\n",
    "        X_train_normalized = X_train_normalized.cpu()\n",
    "    unscaled_data = X_train_normalized * c\n",
    "    X_train = np.sign(X_train_normalized) * (unscaled_data ** 2)\n",
    "    return X_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_avg_centre_comparison(X_t_delta_t_generated_dataset, real_dataset, level = 3):\n",
    "    generated_dataset_centres = get_centre_of_LOB_dataset(X_t_delta_t_generated_dataset, level)\n",
    "    mean_real = np.mean(real_dataset, axis=0)\n",
    "    mean_fake = np.mean(generated_dataset_centres, axis=0)\n",
    "    interleaved_array = np.empty(mean_real.size + mean_fake.size, dtype=mean_real.dtype)\n",
    "    interleaved_array[0::2] = mean_real\n",
    "    interleaved_array[1::2] = mean_fake\n",
    "    indices = np.arange(len(interleaved_array))\n",
    "    return indices, interleaved_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = DataReader(\"./orderbook_snapshots.csv\", rows_per_orderbook=100)\n",
    "data_reader.read_csv()\n",
    "X_train_raw = data_reader.get_data()\n",
    "X_train_raw = convert_ask_bid_int(X_train_raw)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prices = (X_train_raw[:, 49, 0] + X_train_raw[:, 50, 0]) /2\n",
    "best_ask_quantities = X_train_raw[:, 49, 1]\n",
    "best_bid_quantities = np.abs(X_train_raw[:, 50, 1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(best_prices, label='Prices')\n",
    "axes[1].plot(best_ask_quantities, label='Ask Quantities')\n",
    "axes[1].plot(best_bid_quantities, label='Bid Quantities')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms, price_min, price_max, bins = make_histogram_from_dataset(X_train_raw, rows_per_orderbook=100, bin_width=0.5, level=-1)\n",
    "print(histograms.shape)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Marginal Histogram of all Orderbook States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_not_processed, y_train_not_processed = make_centred_LOB_snapshots(histograms, level=3)\n",
    "print(X_train_not_processed.shape,\" \",y_train_not_processed.shape)\n",
    "print(X_train_not_processed[0])\n",
    "print(y_train_not_processed[0])\n",
    "all_orders = X_train_not_processed.ravel()\n",
    "all_orders_next = y_train_not_processed.ravel()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(all_orders, bins=100, density=True)\n",
    "axes[1].hist(all_orders_next, bins=100, density=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "S_t_dataset = normalization(X_train=X_train_not_processed,c=8)\n",
    "X_t_delta_t_dataset = normalization(X_train=y_train_not_processed,c=8)\n",
    "print(S_t_dataset.shape,\" \", X_t_delta_t_dataset.shape)\n",
    "print(S_t_dataset[0])\n",
    "print(X_t_delta_t_dataset[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(S_t_dataset.ravel(), bins=100, density=True)\n",
    "axes[1].hist(X_t_delta_t_dataset.ravel(), bins=100, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Distributions (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asks_1 = X_t_delta_t_dataset[:, 6]\n",
    "asks_2 = X_t_delta_t_dataset[:, 7]\n",
    "asks_3 = X_t_delta_t_dataset[:, 8]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(asks_1, bins=100, density=True)\n",
    "sns.kdeplot(asks_1, ax=axes[0], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[1].hist(asks_2, bins=100, density=True)\n",
    "sns.kdeplot(asks_2, ax=axes[1], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[2].hist(asks_3, bins=100, density=True)\n",
    "sns.kdeplot(asks_3, ax=axes[2], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "bids_1 = X_t_delta_t_dataset[:, 5]\n",
    "bids_2 = X_t_delta_t_dataset[:, 4]\n",
    "bids_3 = X_t_delta_t_dataset[:, 3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(bids_1, bins=100, density=True)\n",
    "sns.kdeplot(bids_1, ax=axes[0], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[1].hist(bids_2, bins=100, density=True)\n",
    "sns.kdeplot(bids_2, ax=axes[1], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[2].hist(bids_3, bins=100, density=True)\n",
    "sns.kdeplot(bids_3, ax=axes[2], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, interleaved_array = draw_avg_centre_comparison(X_t_delta_t_dataset, S_t_dataset, level = 3)\n",
    "plt.bar(indices, interleaved_array, color=['orange', 'blue'] * (len(interleaved_array) // 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t_dataset_tensor = torch.tensor(S_t_dataset, dtype=torch.float32).to(device)\n",
    "X_t_delta_t_dataset_tensor = torch.tensor(X_t_delta_t_dataset, dtype=torch.float32).to(device)\n",
    "dataset = TensorDataset(X_t_delta_t_dataset_tensor, S_t_dataset_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# First, select the relevant slices of the array for bid_1, bid_2, bid_3, ask_1, ask_2, and ask_3\n",
    "data = X_t_delta_t_dataset[:, [6, 5, 7, 4, 8, 3]]\n",
    "\n",
    "# Convert this data into a pandas DataFrame\n",
    "columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Plotting the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar=True, vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Order Book Levels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Critic and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=6, n=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.h_11 = nn.Linear(latent_dim,32).to(device)\n",
    "        self.h_12 = nn.Linear(n,32).to(device)\n",
    "        self.h_21 = nn.Linear(32,32).to(device)\n",
    "        self.h_22 = nn.Linear(32,32).to(device)\n",
    "        self.h_32 = nn.Linear(64, 64).to(device)\n",
    "        self.h_33 = nn.Linear(64, 12).to(device)\n",
    "\n",
    "    def forward(self, Z_t, S_t):\n",
    "        Z_t = Z_t.to(device)  # Move input tensor to the device\n",
    "        S_t = S_t.to(device)  # Move input tensor to the device\n",
    "\n",
    "        h_11_output = torch.relu(self.h_11(Z_t))\n",
    "        h_21_output = torch.relu(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(S_t))\n",
    "        h_22_output = torch.relu(self.h_22(h_12_output))\n",
    "        \n",
    "        # Concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "        \n",
    "        h_32_output = torch.relu(self.h_32(h_31_output))\n",
    "        h_33_output = self.h_33(h_32_output)\n",
    "        return h_33_output\n",
    "\n",
    "'''\n",
    "# Markovian Setting\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_1=12, n_2=6):\n",
    "        super(Critic, self).__init__()\n",
    "        self.h_11 = nn.Linear(n_1,32).to(device)\n",
    "        self.h_12 = nn.Linear(n_2,32).to(device)\n",
    "        self.h_21 = nn.Linear(32,32).to(device)\n",
    "        self.h_22 = nn.Linear(32,32).to(device)\n",
    "        self.h_32 = nn.Linear(64, 64).to(device)\n",
    "        self.h_33 = nn.Linear(64, 1).to(device)\n",
    "\n",
    "    def forward(self, X_t_delta_t, S_t):\n",
    "        X_t_delta_t = X_t_delta_t.to(device)  # Move input tensor to the device\n",
    "        S_t = S_t.to(device)  # Move input tensor to the device\n",
    "\n",
    "        h_11_output = torch.relu(self.h_11(X_t_delta_t))\n",
    "        h_21_output = torch.relu(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(S_t))\n",
    "        h_22_output = torch.relu(self.h_22(h_12_output))\n",
    "        \n",
    "        # Concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "        \n",
    "        h_32_output = torch.relu(self.h_32(h_31_output))\n",
    "        h_33_output = self.h_33(h_32_output)\n",
    "        return h_33_output\n",
    "'''\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_1=12, n_2=6):\n",
    "        super(Critic, self).__init__()\n",
    "        # Increasing the depth and width of the network\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_1 + n_2, 64).to(device),\n",
    "            nn.LeakyReLU(0.2).to(device),\n",
    "            nn.Linear(64, 128).to(device),\n",
    "            nn.LeakyReLU(0.2).to(device),\n",
    "            nn.Linear(128, 256).to(device),\n",
    "            nn.LeakyReLU(0.2).to(device),\n",
    "            nn.Dropout(0.3).to(device),\n",
    "            nn.Linear(256, 1).to(device)\n",
    "        )\n",
    "\n",
    "    def forward(self, X_t_delta_t, S_t):\n",
    "        X_t_delta_t = X_t_delta_t.to(device)  # Move input tensor to the device\n",
    "        S_t = S_t.to(device)  # Move input tensor to the device        \n",
    "        # Concatenation of inputs\n",
    "        combined_input = torch.cat((X_t_delta_t, S_t), dim=1)\n",
    "        validity = self.model(combined_input)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "n_critic = 5\n",
    "z_t_dim = 6\n",
    "LAMBDA = 10\n",
    "epochs = 10000\n",
    "\n",
    "# Instantiate the generator and discriminator with device adaptation\n",
    "generator = Generator(latent_dim=z_t_dim, n=6).to(device)\n",
    "critic = Critic(n_1=12, n_2=6).to(device)\n",
    "\n",
    "# Define the optimisers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.00001, betas=(0, 0.9))\n",
    "optimizer_D = optim.Adam(critic.parameters(), lr=0.00001, betas=(0, 0.9))\n",
    "\n",
    "avg_d_loss_list = []\n",
    "avg_g_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard run location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SummaryWriter\n",
    "writelocation = \"runs/\" + datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "writer = SummaryWriter(writelocation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_marginal_distributions_to_tensorboard(epoch, X_t_delta_t_generated):\n",
    "    # Function to create and log a KDE plot\n",
    "    def log_kde_plot(data, title, epoch, writer):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(data.cpu().numpy(), bins=100, density=True)\n",
    "        sns.kdeplot(data.cpu().numpy(), color='red', fill=True, alpha=0.5)\n",
    "        plt.title(title)\n",
    "        # Save the plot to a buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        plt.close()\n",
    "        buf.seek(0)\n",
    "        # Use add_image to log the plot\n",
    "        writer.add_image(title, plt.imread(buf), epoch, dataformats='HWC')\n",
    "    # Indices for asks and bids might need adjustment based on dataset structure\n",
    "    asks_bids_indices = {\n",
    "        'Ask 1': 6,\n",
    "        'Ask 2': 7,\n",
    "        'Ask 3': 8,\n",
    "        'Bid 1': 5,\n",
    "        'Bid 2': 4,\n",
    "        'Bid 3': 3,\n",
    "    }\n",
    "\n",
    "    for title, index in asks_bids_indices.items():\n",
    "        data = X_t_delta_t_generated[:, index]\n",
    "        log_kde_plot(data, title, epoch, writer)\n",
    "        \n",
    "        \n",
    "def write_covariance_matrix_of_generated_data_to_tensorboard(epoch, X_t_delta_t_generated):\n",
    "    best_k_ask_bids = X_t_delta_t_generated[:, [6, 5, 7, 4, 8, 3]].cpu().numpy()\n",
    "    columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "    df = pd.DataFrame(best_k_ask_bids, columns=columns)\n",
    "    correlation_matrix = df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar=True, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Order Book Levels')\n",
    "    # Save the plot to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    # Use add_image to log the plot\n",
    "    writer.add_image('Correlation Matrix of Order Book Levels', plt.imread(buf), epoch, dataformats='HWC')\n",
    "    \n",
    "def write_avg_LOB_snapshot_to_tensorboard(epoch, X_t_delta_t_generated):\n",
    "    X_t_delta_t_generated = X_t_delta_t_generated.cpu().numpy()\n",
    "    indices, interleaved_array = draw_avg_centre_comparison(X_t_delta_t_generated, S_t_dataset, level = 3)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.bar(indices, interleaved_array, color=['orange', 'blue'] * (len(interleaved_array) // 2))\n",
    "    plt.title('Average LOB shape real and fake')\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    writer.add_image('Average LOB shape real(orange) and fake(blue)', plt.imread(buf), epoch, dataformats='HWC')\n",
    "    \n",
    "def write_metrics_to_tensorboard(epoch):\n",
    "    with torch.no_grad():\n",
    "        z_t = torch.randn(S_t_dataset.shape[0], z_t_dim)\n",
    "        X_t_delta_t_generated = generator(z_t, S_t_dataset_tensor)\n",
    "    write_marginal_distributions_to_tensorboard(epoch, X_t_delta_t_generated)\n",
    "    write_covariance_matrix_of_generated_data_to_tensorboard(epoch, X_t_delta_t_generated)\n",
    "    write_avg_LOB_snapshot_to_tensorboard(epoch, X_t_delta_t_generated)\n",
    "\n",
    "        \n",
    "def write_loss_functions_to_tensorboard(avg_d_loss, avg_g_loss,epoch):\n",
    "    writer.add_scalar('Loss/Discriminator', avg_d_loss, epoch)\n",
    "    writer.add_scalar('Loss/Generator', avg_g_loss, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(C, real_samples, fake_samples, batch_S_t):\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Ensure alpha is shaped correctly for broadcasting\n",
    "    alpha = torch.rand(batch_size, 1, device=device)\n",
    "    alpha = alpha.expand(batch_size, real_samples.nelement() // batch_size).contiguous().view(batch_size, -1)\n",
    "\n",
    "    # Calculate interpolates\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = C(interpolates, batch_S_t)\n",
    "    \n",
    "    fake = Variable(torch.Tensor(batch_size , 1).fill_(1.0), requires_grad=False).to(device)\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    # Flatten the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop of WGAN with Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to track progress\n",
    "avg_d_loss = 0\n",
    "avg_g_loss = 0\n",
    "n_batches = len(data_loader)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Variables per epoch\n",
    "    total_d_loss_epoch = 0\n",
    "    total_g_loss_epoch = 0\n",
    "    \n",
    "    for i, (batch_X_t_delta_t, batch_S_t) in enumerate(data_loader):\n",
    "        batch_size = batch_S_t.shape[0]\n",
    "        \n",
    "        batch_S_t = batch_S_t.to(device)\n",
    "        batch_X_t_delta_t = batch_X_t_delta_t.to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        total_d_loss_batch = 0\n",
    "        for _ in range(n_critic):  # Update the discriminator n_critic times\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            batch_Z_t = torch.randn(batch_size, z_t_dim, device=device)  # Ensure noise_dim matches generator input\n",
    "\n",
    "            # Generate a batch of images\n",
    "            batch_X_t_delta_t_generated = generator(batch_Z_t, batch_S_t)\n",
    "\n",
    "            # Real images\n",
    "            real_validity = critic(batch_X_t_delta_t, batch_S_t)\n",
    "            # Fake images\n",
    "            fake_validity = critic(batch_X_t_delta_t_generated, batch_S_t)\n",
    "            # Gradient penalty\n",
    "            gp = gradient_penalty(critic, batch_X_t_delta_t.data, batch_X_t_delta_t_generated.data, batch_S_t.data)\n",
    "            \n",
    "            # Wasserstein GAN loss w/ gradient penalty\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + LAMBDA * gp\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            total_d_loss_batch += d_loss.item()\n",
    "            \n",
    "        d_loss_batch = total_d_loss_batch / n_critic\n",
    "        total_d_loss_epoch += d_loss_batch # Average over the n_critic updates\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Resample noise (optional but often leads to better training stability)\n",
    "        batch_Z_t = torch.randn(batch_size, z_t_dim, device=device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        batch_X_t_delta_t_generated = generator(batch_Z_t, batch_S_t)\n",
    "        fake_validity = critic(batch_X_t_delta_t_generated, batch_S_t)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        total_g_loss_epoch += g_loss.item()\n",
    "\n",
    "    # Prints average loss per epoch\n",
    "    avg_d_loss = total_d_loss_epoch / n_batches\n",
    "    avg_g_loss = total_g_loss_epoch / n_batches\n",
    "    avg_d_loss_list.append(avg_d_loss)\n",
    "    avg_g_loss_list.append(avg_g_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        write_metrics_to_tensorboard(epoch)\n",
    "        \n",
    "    write_loss_functions_to_tensorboard(avg_d_loss, avg_g_loss,epoch)\n",
    "    # Resets average losses for the next epoch\n",
    "    avg_d_loss = 0\n",
    "    avg_g_loss = 0\n",
    "\n",
    "# Close the writer after the training loop\n",
    "writer.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(avg_d_loss_list, avg_g_loss_list):\n",
    "    epochs = range(1, len(avg_d_loss_list) + 1)\n",
    "    plt.plot(epochs, avg_d_loss_list, label='Discriminator Loss')\n",
    "    plt.plot(epochs, avg_g_loss_list, label='Generator Loss')\n",
    "    plt.title('Average Losses per Epoch')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')        \n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "# Now, call the function with your lists\n",
    "plot_losses(avg_d_loss_list, avg_g_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generator's state dictionary for python\n",
    "torch.save(generator.state_dict(), './saved_models/python/BatchSize64Base1000Epochs.pth')\n",
    "# Save the generator's state dictionary for c++\n",
    "z_t = torch.randn(1,z_t_dim)\n",
    "s_t = torch.tensor(S_t_dataset[0], dtype=torch.float32).unsqueeze(0)\n",
    "traced_script_generator = torch.jit.trace(generator, (z_t, s_t))\n",
    "traced_script_generator.save('./saved_models/cpp/BatchSize64Base1000Epochs.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 1 Example Orderbook Snapshot where the price has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample order book snapshot\n",
    "i=0\n",
    "counter = 0\n",
    "while (True):\n",
    "    with torch.no_grad():\n",
    "        if counter == 1: break\n",
    "        i += 1\n",
    "        z_t = torch.randn(1,z_t_dim)\n",
    "        s_t = torch.tensor(S_t_dataset[0], dtype=torch.float32).unsqueeze(0)\n",
    "        X_t_delta_t_example = generator(z_t, s_t)\n",
    "        if reverse_normalization(X_t_delta_t_example)[0, 5] > 0 or reverse_normalization(X_t_delta_t_example)[0, 6] < 0:\n",
    "            print(i)\n",
    "            print(\"Current Order Book Snapshot:\\t\", reverse_normalization(s_t))\n",
    "            print(\"Generated Order Book Snapshot:\\t\", reverse_normalization(X_t_delta_t_example))\n",
    "            counter += 1\n",
    "            \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
