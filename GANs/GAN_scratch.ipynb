{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Development - Kochems Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from data_reader import DataReader\n",
    "from torch.autograd import Variable, grad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ask_bid_int(dataset):\n",
    "    mask_ask = np.char.endswith(dataset[:,:,2], 'ask')\n",
    "    mask_bid = np.char.endswith(dataset[:,:,2], 'bid')\n",
    "    dataset[:,:,2][mask_ask] = '1'\n",
    "    dataset[:,:,2][mask_bid] = '0'\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "def get_dataset_max_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = 0\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2-(level))\n",
    "    last_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    max_val = np.max(last_row_prices)\n",
    "    return max_val\n",
    "\n",
    "def get_dataset_min_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = -1\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2+(level-1))\n",
    "    first_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    min_val = np.min(first_row_prices)\n",
    "    return min_val\n",
    "\n",
    "def make_histogram_from_dataset(dataset, rows_per_orderbook = 100, bin_width = 0.5, level = -1):\n",
    "    X_train = []\n",
    "    hist_max = get_dataset_max_price(dataset, rows_per_orderbook, level)\n",
    "    hist_min = get_dataset_min_price(dataset, rows_per_orderbook, level)\n",
    "    print(\"range: \", hist_min, \" \", hist_max)\n",
    "    num_bins = int(np.ceil((hist_max-hist_min) / bin_width))\n",
    "    bins = np.linspace(hist_min, hist_max, num_bins)\n",
    "    for i in range(len(dataset)):\n",
    "        orderbook = dataset[i]\n",
    "        price = orderbook[:,0]\n",
    "        quantity = orderbook[:,1]\n",
    "        quantity[orderbook[:, 2] == 0] *= -1\n",
    "        hist, bin_edges = np.histogram(price, bins=bins, weights=quantity)\n",
    "        X_train.append(hist)\n",
    "    X_train = np.array(X_train)\n",
    "    return X_train, hist_min, hist_max, bins\n",
    "\n",
    "def make_centred_LOB_snapshots(histograms, level = 1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(histograms)-1):\n",
    "        current_OB = histograms[i]\n",
    "        next_OB = histograms[i+1]\n",
    "        j = -1\n",
    "        while j < len(current_OB)-1 and not (current_OB[j] < 0 and current_OB[j+1] > 0): j+=1\n",
    "        j+=1\n",
    "        \n",
    "        current_start_index = j-level\n",
    "        current_subarray_size = 2 * level\n",
    "        current_centre_LOB_snapshot = current_OB[current_start_index: current_start_index + current_subarray_size]\n",
    "        current_np_before_after = np.zeros((level,))\n",
    "        \n",
    "        k = -1\n",
    "        while k < len(next_OB)-1 and not (next_OB[k] < 0 and next_OB[k+1] > 0): k+=1\n",
    "        k+=1\n",
    "        \n",
    "        next_start_index = k-level\n",
    "        next_subarray_size = 2 * level\n",
    "        next_centre_LOB_snapshot = next_OB[next_start_index: next_start_index + next_subarray_size]\n",
    "        \n",
    "        jk_diff = j-k\n",
    "        if (abs(jk_diff) > level): continue\n",
    "        \n",
    "        next_np_before = np.zeros((level+jk_diff,))\n",
    "        next_np_after = np.zeros((level-jk_diff,))\n",
    "        current_centre_LOB_snapshot = np.concatenate((current_np_before_after, current_centre_LOB_snapshot, current_np_before_after))\n",
    "        next_OB_transition = np.concatenate((next_np_before, next_centre_LOB_snapshot, next_np_after))\n",
    "        X_train.append(current_centre_LOB_snapshot)\n",
    "        y_train.append(next_OB_transition)\n",
    "        \n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.vstack(y_train)\n",
    "    \n",
    "    return X_train, y_train\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = DataReader(\"orderbook_snapshots100.csv\", rows_per_orderbook=100)\n",
    "data_reader.read_csv()\n",
    "X_train_raw = data_reader.get_data()\n",
    "X_train_raw = convert_ask_bid_int(X_train_raw)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms, price_min, price_max, bins = make_histogram_from_dataset(X_train_raw, rows_per_orderbook=100, bin_width=0.5, level=-1)\n",
    "print(histograms.shape)\n",
    "print(histograms)\n",
    "# print(X_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_centred_LOB_snapshots(histograms, level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_asks = X_train[:, 8]\n",
    "best_bids = X_train[:, 3]\n",
    "\n",
    "plt.hist(best_bids, bins=30, density=True)\n",
    "plt.show()\n",
    "plt.hist(best_asks, bins=30, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Critic and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=5, n=12):\n",
    "        super(Generator, self).__init__()\n",
    "        self.h_11 = nn.Linear(latent_dim,32)\n",
    "        self.h_12 = nn.Linear(n,32)\n",
    "        self.h_21 = nn.Linear(32,32)\n",
    "        self.h_22 = nn.Linear(32,32)\n",
    "        self.h_32 = nn.Linear(64, 64)\n",
    "        self.h_33 = nn.Linear(64, n)\n",
    "\n",
    "    def forward(self, noise, previous_state):\n",
    "        h_11_output = torch.relu(self.h_11(noise))\n",
    "        h_21_output = torch.relu(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(previous_state))\n",
    "        h_22_output = torch.relu(self.h_22(h_12_output))\n",
    "        \n",
    "        #concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "        \n",
    "        h_32_output = torch.relu(self.h_32(h_31_output))\n",
    "        h_33_output = self.h_33(h_32_output)\n",
    "        return h_33_output\n",
    "\n",
    "#Markovian setting\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n=32):\n",
    "        super(Critic, self).__init__()\n",
    "        self.h_11 = nn.Linear(n,32)\n",
    "        self.h_12 = nn.Linear(n,32)\n",
    "        self.h_21 = nn.Linear(32,32)\n",
    "        self.h_22 = nn.Linear(32,32)\n",
    "        self.h_32 = nn.Linear(64, 64)\n",
    "        self.h_33 = nn.Linear(64, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, previous_state, current_state):\n",
    "        h_11_output = torch.relu(self.h_11(current_state))\n",
    "        h_21_output = torch.relu(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(previous_state))\n",
    "        h_22_output = torch.relu(self.h_22(h_12_output))\n",
    "        \n",
    "        #concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "        \n",
    "        h_32_output = torch.relu(self.h_32(h_31_output))\n",
    "        h_33_output = self.h_33(h_32_output)\n",
    "        return h_33_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(D, real_samples, fake_samples):\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Ensure alpha is shaped correctly for broadcasting\n",
    "    alpha = torch.rand(batch_size, 1, device=real_samples.device)\n",
    "    alpha = alpha.expand(batch_size, real_samples.nelement() // batch_size).contiguous().view(batch_size, -1)\n",
    "\n",
    "    # Ensure real_samples and fake_samples are flat\n",
    "    real_samples_flat = real_samples.view(batch_size, -1)\n",
    "    fake_samples_flat = fake_samples.view(batch_size, -1)\n",
    "\n",
    "    # Calculate interpolates\n",
    "    interpolates = (alpha * real_samples_flat + (1 - alpha) * fake_samples_flat).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    \n",
    "    fake = Variable(torch.Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False).to(real_samples.device)\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    # Flatten the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)  \n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop of WGAN with Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to track progress\n",
    "avg_d_loss = 0\n",
    "avg_g_loss = 0\n",
    "n_batches = len(data_loader)\n",
    "\n",
    "# Define the number of critic updates per generator update\n",
    "n_critic = 5  \n",
    "\n",
    "for epoch in range(200):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.view(imgs.size(0), -1)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        for _ in range(n_critic):  # Update the discriminator n_critic times\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = torch.randn(imgs.shape[0], 100)  # Ensure noise_dim matches generator input\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "\n",
    "            # Real images\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            # Fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(discriminator, real_imgs.data, fake_imgs.data)\n",
    "            \n",
    "            # Wasserstein GAN loss w/ gradient penalty\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            avg_d_loss += d_loss.item() / n_critic  # Average over the n_critic updates\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        if i % n_critic == 0:  # Update the generator every n_critic steps\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = -torch.mean(discriminator(gen_imgs))\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            avg_g_loss += g_loss.item()\n",
    "\n",
    "        # Prints progress within the epoch\n",
    "        if (i+1) % 100 == 0:  # Print every 100 steps\n",
    "            print(f\"Epoch [{epoch+1}/{200}], Step [{i+1}/{n_batches}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "    \n",
    "    # Prints average loss per epoch\n",
    "    avg_d_loss /= n_batches\n",
    "    avg_g_loss /= n_batches\n",
    "    print(f\"Epoch [{epoch+1}/{200}] completed. Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "    # Resets average losses for the next epoch\n",
    "    avg_d_loss = 0\n",
    "    avg_g_loss = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Instantiate Models & Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the generator and discriminator\n",
    "generator = Generator()\n",
    "critic = Critic()\n",
    "\n",
    "# Define the optimisers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_D = torch.optim.Adam(critic.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "n_epochs = 200  # ADJUST\n",
    "batch_size = 64  # ADJUST\n",
    "noise_dim = 2  # Size of noise vector\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(data_tensor), batch_size):\n",
    "        # Prepare real data batch\n",
    "        real_data = data_tensor[i:i+batch_size, 1:]  # Exclude timestamp from training\n",
    "        real_labels = torch.ones(real_data.size(0), 1)\n",
    "        fake_labels = torch.zeros(real_data.size(0), 1)\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real data loss\n",
    "        real_output = discriminator(real_data)\n",
    "        d_loss_real = criterion(real_output, real_labels)\n",
    "        \n",
    "        # Generate fake data\n",
    "        noise = torch.randn(real_data.size(0), noise_dim)\n",
    "        # Conditional input, for now, let's use a random slice from the best_ask_price as an example\n",
    "        conditional_input = real_data[:, 0].unsqueeze(1)  # This should be modified based on your specific conditional input\n",
    "\n",
    "\n",
    "        # Right before generator(noise, conditional_input) call\n",
    "        # print(\"Conditional input shape before generator:\", conditional_input.shape)\n",
    "\n",
    "        fake_data = generator(noise, conditional_input)\n",
    "        \n",
    "        # Fake data loss\n",
    "        fake_output = discriminator(fake_data.detach())  # Detach to avoid training generator on these labels\n",
    "        d_loss_fake = criterion(fake_output, fake_labels)\n",
    "        \n",
    "        # Combine loss and update discriminator\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Trick discriminator into thinking the generated data is real\n",
    "        output = discriminator(fake_data)\n",
    "        g_loss = criterion(output, real_labels)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        if i % 100 == 0:  # Adjust printing frequency based on your preference\n",
    "            print(f\"Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{len(data_tensor)//batch_size}], D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generator's state dictionary\n",
    "torch.save(generator.state_dict(), '/Users/sina/Downloads/SCRATCH_GENERATIVE_ADV_NET/Saved_Generator_States/generator_state_dict.pth')\n",
    "print(\"Generator state has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Generate Example Orderbook Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample order book snapshot\n",
    "with torch.no_grad():\n",
    "    test_noise = torch.randn(1, noise_dim)\n",
    "    test_price_level = torch.tensor([[0.5]])  # Example price level, normalized\n",
    "    generated_snapshot = generator(test_noise, test_price_level)\n",
    "    inverse_transformed_snapshot = scaler.inverse_transform(generated_snapshot.numpy())\n",
    "    print(\"Generated Order Book Snapshot:\", inverse_transformed_snapshot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
