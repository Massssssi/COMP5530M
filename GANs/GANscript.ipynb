{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing a Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, ones, expand_dims, asarray\n",
    "from numpy.random import randn, randint\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras import initializers, Sequential\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import os\n",
    "from keras.callbacks import Callback\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float32) / 127.5 - 1\n",
    "X_train = np.expand_dims(X_train, axis=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(input_dim=(28,28,1)):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_dim),\n",
    "        Flatten(),\n",
    "        Dense(units=784, activation='relu'),\n",
    "        Dense(units=128, activation='relu'),\n",
    "        Dense(units=64, activation='relu'),\n",
    "        Dense(units=1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim = 64, input_dim=(28,28,1)):\n",
    "    model = Sequential([\n",
    "        Input(shape=(latent_dim,)),\n",
    "        Dense(units=784, activation='relu'),\n",
    "        Dense(units=256, activation='relu'),\n",
    "        Dense(units=input_dim[0] * input_dim[1] * input_dim[2], activation='tanh'),\n",
    "        Reshape(target_shape=input_dim)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining GAN Training Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel(Model):\n",
    "    def __init__(self, generator, discriminator, generator_latent_dim, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator\n",
    "        self.generator_latent_dim = generator_latent_dim\n",
    "        \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, n_critic, LAMBDA, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss\n",
    "        self.n_critic = n_critic\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def get_generator(self):\n",
    "        return self.generator\n",
    "    \n",
    "    # returns 2D array\n",
    "    # n_samples number of rows with each row having latent_dim number of random noise.\n",
    "    def generate_latent_points(self, n_samples):\n",
    "        # Returns a np array of dimension (X,) meaning 1D array. \n",
    "        x_input = randn(self.generator_latent_dim * n_samples)\n",
    "        \n",
    "        # Returns a 2D np array. \n",
    "        # Divides 1D array such that for each n_samples, there are latent_dim random numbers\n",
    "        z_input = x_input.reshape(n_samples, self.generator_latent_dim)\n",
    "        return z_input\n",
    "    \n",
    "    def gradient_penalty(self, real_images, fake_images):\n",
    "        batch_size = real_images.shape[0]\n",
    "        epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0.0, maxval=1.0)\n",
    "        interpolated_images = epsilon * tf.dtypes.cast(real_images, tf.float32) + ((1 - epsilon) * fake_images)\n",
    "        \n",
    "        with tf.GradientTape() as penalty_tape:\n",
    "            penalty_tape.watch(interpolated_images)\n",
    "            yhat_interpolated = self.discriminator(interpolated_images, training=True)\n",
    "            \n",
    "        p_grad = penalty_tape.gradient(yhat_interpolated, interpolated_images)\n",
    "        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(p_grad), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(grad_norms-1.0))\n",
    "        return gradient_penalty\n",
    "        \n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        real_images = batch\n",
    "        fake_images = self.generator(self.generate_latent_points(batch.shape[0]), training=False)\n",
    "        \n",
    "        for _ in range(self.n_critic):\n",
    "            # Train the discriminator\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                yhat_real = self.discriminator(real_images, training=True) \n",
    "                yhat_fake = self.discriminator(fake_images, training=True)\n",
    "                yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "                y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "                \n",
    "                gradient_penalty = self.gradient_penalty(real_images, fake_images)\n",
    "                # Calculate loss - BINARYCROSS \n",
    "                # total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "                total_d_loss = (tf.reduce_mean(yhat_fake) - tf.reduce_mean(yhat_real)) + (self.LAMBDA * gradient_penalty)\n",
    "                \n",
    "            # Apply backpropagation - nn learn \n",
    "            d_grad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "            self.d_opt.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            gen_images = self.generator(self.generate_latent_points(batch.shape[0]), training=True)\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            # total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels)\n",
    "            total_g_loss = -tf.reduce_mean(predicted_labels)\n",
    "            \n",
    "        # Apply backprop\n",
    "        g_grad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns 2D array\n",
    "# n_samples number of rows with each row having latent_dim number of random noise.\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Returns a np array of dimension (X,) meaning 1D array. \n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    \n",
    "    # Returns a 2D np array. \n",
    "    # Divides 1D array such that for each n_samples, there are latent_dim random numbers\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return z_input\n",
    "\n",
    "# Chooses n_samples number of samples from training set\n",
    "# Gets labels alongside with same dimension.\n",
    "def generate_real_samples(X_train, n_samples):\n",
    "    #Returns a np array of size n_samples repr. indices of chosen elements for next batch\n",
    "    ix = randint(0, X_train.shape[0], n_samples)\n",
    "    X = X_train[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    \n",
    "    # X is of dimension (n_samples, 28, 28, 1)\n",
    "    # y is of dimension (n_samples, 1)\n",
    "    return X, y\n",
    "\n",
    "# generates n_samples from generator\n",
    "# takes in 2D array of latent points aswell\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    outputs = generator.predict(z_input)  \n",
    "    y = zeros((n_samples, 1))\n",
    "    return outputs, y\n",
    "\n",
    "def get_GAN_training_network(generator, discriminator, latent_dim, generator_learning_rate = 0.002, discriminator_learning_rate = 0.002, n_critic=1, LAMBDA=1):\n",
    "    gan_model = GANModel(generator=generator, discriminator=discriminator, generator_latent_dim=latent_dim)\n",
    "    g_opt = Adam(learning_rate=generator_learning_rate, beta_1=0.5)\n",
    "    d_opt = Adam(learning_rate=discriminator_learning_rate, beta_1=0.5)\n",
    "    g_loss = BinaryCrossentropy()\n",
    "    d_loss = BinaryCrossentropy()\n",
    "    gan_model.compile(g_opt, d_opt, g_loss, d_loss, n_critic=n_critic, LAMBDA=LAMBDA)\n",
    "    return gan_model\n",
    "\n",
    "def get_generator_and_discriminator(latent_dim, input_dim):\n",
    "    discriminator = define_discriminator(input_dim=input_dim)\n",
    "    generator = define_generator(latent_dim=latent_dim, input_dim=input_dim)\n",
    "    return discriminator, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator, generator = get_generator_and_discriminator(latent_dim=100, input_dim=(28,28,1))\n",
    "gan_model = get_GAN_training_network(generator=generator, \n",
    "                                     discriminator=discriminator, \n",
    "                                     latent_dim=100, \n",
    "                                     generator_learning_rate=0.00005, \n",
    "                                     discriminator_learning_rate=0.00005,\n",
    "                                     n_critic=5,\n",
    "                                     LAMBDA=1\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = gan_model.fit(X_train, epochs=100, batch_size=1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.suptitle('Loss')\n",
    "pyplot.plot(hist.history['d_loss'], label='d_loss')\n",
    "pyplot.plot(hist.history['g_loss'], label='g_loss')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the generator from the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gan_model.get_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print resulting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = generate_real_samples(X_train=X_train, n_samples=64)\n",
    "X_fake, y_fake = generate_fake_samples(generator=generator, latent_dim=100, n_samples=64)\n",
    "for i in range(64):\n",
    "    pyplot.imshow(X_real[i])\n",
    "    pyplot.show()\n",
    "    pyplot.imshow(X_fake[i])\n",
    "    pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
