{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing a Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and loading in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import zeros, ones, expand_dims, asarray\n",
    "from numpy.random import randn, randint\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
    "from keras.layers import LeakyReLU, Dropout, Embedding\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras import initializers, Sequential\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import os\n",
    "from keras.callbacks import Callback\n",
    "from data_reader import DataReader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train = X_train.astype(np.float32) / 127.5 - 1\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Orderbook Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5876, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "data_reader_instance = DataReader(\"orderbook_snapshots.csv\", rows_per_orderbook=2)\n",
    "data_reader_instance.read_csv()\n",
    "X_train = data_reader_instance.get_data()\n",
    "X_train_raw = X_train.astype(np.float32)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5876, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_min_price(dataset):\n",
    "    first_row_prices = dataset[:, -1, 0, 0]\n",
    "    min_val = np.min(first_row_prices)\n",
    "    return min_val\n",
    "\n",
    "def get_dataset_max_price(dataset):\n",
    "    last_row_prices = dataset[:, 0, 0, 0]\n",
    "    max_val = np.max(last_row_prices)\n",
    "    return max_val\n",
    "\n",
    "def get_prices_range(dataset):\n",
    "    min_val = get_dataset_min_price(dataset)\n",
    "    max_val = get_dataset_max_price(dataset)\n",
    "    range_vals = np.array([min_val, max_val])\n",
    "    return range_vals\n",
    "\n",
    "def get_dataset_min_max_quantity(dataset):\n",
    "    all_quantities = dataset[:, :, 1, 0]\n",
    "    max_val = np.max(all_quantities)\n",
    "    min_val = np.min(all_quantities)\n",
    "    return min_val, max_val\n",
    "\n",
    "def get_quantity_range(dataset):\n",
    "    min_val, max_val = get_dataset_min_max_quantity(dataset)\n",
    "    range_vals = np.array([min_val, max_val])\n",
    "    return range_vals\n",
    "\n",
    "def scale_prices(dataset):\n",
    "    range_vals_price = get_prices_range(dataset)\n",
    "    range_vals_quantity = get_quantity_range(dataset)\n",
    "    price_scaler = MinMaxScaler()\n",
    "    quantity_scaler = MinMaxScaler()\n",
    "    price_scaler.fit(range_vals_price[:, np.newaxis])\n",
    "    quantity_scaler.fit(range_vals_quantity[:, np.newaxis])\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        prices_col = dataset[i, :, 0]\n",
    "        quantity_col = dataset[i, :, 1]\n",
    "        prices_transformed = price_scaler.transform(prices_col)\n",
    "        quantity_transformed = quantity_scaler.transform(quantity_col)\n",
    "        dataset[i, :, 0] = prices_transformed\n",
    "        dataset[i, :, 1] = quantity_transformed    \n",
    "    return dataset\n",
    "        \n",
    "\n",
    "X_train = scale_prices(X_train_raw)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_discriminator(input_dim=(28,28,1)):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=input_dim),\n",
    "#         Flatten(),\n",
    "#         Dense(units=512),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=512),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=512),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=512),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=512),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=1, activation='linear')\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "# def define_generator(latent_dim = 64, input_dim=(28,28,1)):\n",
    "#     model = Sequential([\n",
    "#         Input(shape=(latent_dim,)),\n",
    "#         Dense(units=256),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=256),\n",
    "#         LeakyReLU(alpha=0.2),\n",
    "#         # Dense(units=256),\n",
    "#         # LeakyReLU(alpha=0.2),\n",
    "#         # Dense(units=256),\n",
    "#         # LeakyReLU(alpha=0.2),\n",
    "#         # Dense(units=256),\n",
    "#         # LeakyReLU(alpha=0.2),\n",
    "#         Dense(units=input_dim[0] * input_dim[1] * input_dim[2], activation='linear'),\n",
    "#         Reshape(target_shape=input_dim)\n",
    "#     ])\n",
    "#     return model\n",
    "\n",
    "def define_discriminator(input_dim=(28,28,1)):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_dim),\n",
    "        Flatten(),\n",
    "        Dense(units=4, activation='relu'),\n",
    "        Dense(units=4, activation='relu'),\n",
    "        Dense(units=4, activation='relu'),\n",
    "        Dense(units=1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def define_generator(latent_dim = 64, input_dim=(28,28,1)):\n",
    "    model = Sequential([\n",
    "        Input(shape=(latent_dim,)),\n",
    "        Dense(units=latent_dim, activation='relu'),\n",
    "        Dense(units=input_dim[0] * input_dim[1], activation='relu'),\n",
    "        Dense(units=input_dim[0] * input_dim[1] * input_dim[2], activation='tanh'),\n",
    "        # Dense(units=input_dim[0] * input_dim[1] * input_dim[2], activation='linear'),\n",
    "        Reshape(target_shape=input_dim)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining GAN Training Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel(Model):\n",
    "    def __init__(self, generator, discriminator, generator_latent_dim, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for gen and disc\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator\n",
    "        self.generator_latent_dim = generator_latent_dim\n",
    "        \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, n_critic, LAMBDA, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.n_critic = n_critic\n",
    "        self.LAMBDA = LAMBDA\n",
    "        \n",
    "    def get_generator(self):\n",
    "        return self.generator\n",
    "    \n",
    "    def get_discriminator(self):\n",
    "        return self.discriminator\n",
    "    \n",
    "    # returns 2D array\n",
    "    # n_samples number of rows with each row having latent_dim number of random noise.\n",
    "    def generate_latent_points(self, n_samples):\n",
    "        # Returns a np array of dimension (X,) meaning 1D array. \n",
    "        x_input = randn(self.generator_latent_dim * n_samples)\n",
    "        \n",
    "        # Returns a 2D np array. \n",
    "        # Divides 1D array such that for each n_samples, there are latent_dim random numbers\n",
    "        z_input = x_input.reshape(n_samples, self.generator_latent_dim)\n",
    "        return z_input\n",
    "    \n",
    "    def gradient_penalty(self, real_images, fake_images):\n",
    "        batch_size = real_images.shape[0]\n",
    "        epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0.0, maxval=1.0)\n",
    "        interpolated_images = epsilon * tf.dtypes.cast(real_images, tf.float32) + ((1 - epsilon) * fake_images)\n",
    "        \n",
    "        with tf.GradientTape() as penalty_tape:\n",
    "            penalty_tape.watch(interpolated_images)\n",
    "            yhat_interpolated = self.discriminator(interpolated_images, training=True)\n",
    "            \n",
    "        p_grad = penalty_tape.gradient(yhat_interpolated, interpolated_images)\n",
    "        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(p_grad), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(grad_norms-1.0))\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def rule_penalty(self, fake_images):\n",
    "        rule_penalty = tf.reduce_mean(tf.nn.relu(fake_images[:, 1, 0] - fake_images[:,0,0]))\n",
    "        return rule_penalty\n",
    "    \n",
    "    def wasserstein_loss_discriminator(self, fake_pred, real_pred):\n",
    "        return -(tf.reduce_mean(real_pred) - tf.reduce_mean(fake_pred))\n",
    "    \n",
    "    def wasserstein_loss_generator(self, fake_pred):\n",
    "        return -tf.reduce_mean(fake_pred)\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        batch_size = batch.shape[0]\n",
    "        real_images = batch\n",
    "        fake_images = self.generator(self.generate_latent_points(batch_size), training=False)\n",
    "        \n",
    "        for _ in range(self.n_critic):\n",
    "            # Train the discriminator\n",
    "            with tf.GradientTape() as d_tape:\n",
    "                yhat_real = self.discriminator(real_images, training=True) \n",
    "                yhat_fake = self.discriminator(fake_images, training=True)\n",
    "                gradient_penalty = self.gradient_penalty(real_images, fake_images)\n",
    "                # rule_penalty = self.rule_penalty(fake_images)\n",
    "                \n",
    "                # Calculate loss - Wassertstein\n",
    "                total_d_loss = self.wasserstein_loss_discriminator(yhat_fake, yhat_real) + self.LAMBDA * gradient_penalty\n",
    "                \n",
    "            # Apply backpropagation to weights\n",
    "            d_grad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "            self.d_opt.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            gen_images = self.generator(self.generate_latent_points(batch_size), training=True)\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.wasserstein_loss_generator(predicted_labels)\n",
    "            \n",
    "        # Apply backpropagation to weights\n",
    "        g_grad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# returns 2D array\n",
    "# n_samples number of rows with each row having latent_dim number of random noise.\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # Returns a np array of dimension (X,) meaning 1D array. \n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    \n",
    "    # Returns a 2D np array. \n",
    "    # Divides 1D array such that for each n_samples, there are latent_dim random numbers\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return z_input\n",
    "\n",
    "# Chooses n_samples number of samples from training set\n",
    "# Gets labels alongside with same dimension.\n",
    "def generate_real_samples(X_train, n_samples):\n",
    "    #Returns a np array of size n_samples repr. indices of chosen elements for next batch\n",
    "    ix = randint(0, X_train.shape[0], n_samples)\n",
    "    X = X_train[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    \n",
    "    # X is of dimension (n_samples, 28, 28, 1)\n",
    "    # y is of dimension (n_samples, 1)\n",
    "    return X, y\n",
    "\n",
    "# generates n_samples from generator\n",
    "# takes in 2D array of latent points aswell\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    outputs = generator.predict(z_input)  \n",
    "    y = zeros((n_samples, 1))\n",
    "    return outputs, y\n",
    "\n",
    "def get_GAN_training_network(generator, discriminator, latent_dim, generator_learning_rate = 0.002, discriminator_learning_rate = 0.002, n_critic=1, adam_beta_1 = 0, adam_beta_2 = 0.9, LAMBDA=1):\n",
    "    gan_model = GANModel(generator=generator, discriminator=discriminator, generator_latent_dim=latent_dim)\n",
    "    g_opt = Adam(learning_rate=generator_learning_rate, beta_1=adam_beta_1, beta_2=adam_beta_2)\n",
    "    d_opt = Adam(learning_rate=discriminator_learning_rate, beta_1=adam_beta_1, beta_2=adam_beta_2)\n",
    "    gan_model.compile(g_opt, d_opt, n_critic=n_critic, LAMBDA=LAMBDA)\n",
    "    return gan_model\n",
    "\n",
    "def get_generator_and_discriminator(latent_dim, input_dim):\n",
    "    discriminator = define_discriminator(input_dim=input_dim)\n",
    "    generator = define_generator(latent_dim=latent_dim, input_dim=input_dim)\n",
    "    return discriminator, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Discriminator and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator, generator = get_generator_and_discriminator(latent_dim=100, input_dim=(2,2,1))\n",
    "gan_model = get_GAN_training_network(generator=generator, \n",
    "                                     discriminator=discriminator, \n",
    "                                     latent_dim=100,\n",
    "                                     generator_learning_rate=0.0001, \n",
    "                                     discriminator_learning_rate=0.0001,\n",
    "                                     n_critic=5,\n",
    "                                     adam_beta_1=0,\n",
    "                                     adam_beta_2=0.9,\n",
    "                                     LAMBDA=10\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "Discriminator Gradients:\n",
      "dense_341/kernel:0: Tensor(\"Mean_16:0\", shape=(), dtype=float32)\n",
      "dense_341/bias:0: Tensor(\"Mean_17:0\", shape=(), dtype=float32)\n",
      "dense_342/kernel:0: Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "dense_342/bias:0: Tensor(\"Mean_19:0\", shape=(), dtype=float32)\n",
      "dense_343/kernel:0: Tensor(\"Mean_20:0\", shape=(), dtype=float32)\n",
      "dense_343/bias:0: Tensor(\"Mean_21:0\", shape=(), dtype=float32)\n",
      "dense_344/kernel:0: Tensor(\"Mean_22:0\", shape=(), dtype=float32)\n",
      "dense_344/bias:0: Tensor(\"Mean_23:0\", shape=(), dtype=float32)\n",
      "Generator Gradients:\n",
      "dense_345/kernel:0: Tensor(\"Mean_24:0\", shape=(), dtype=float32)\n",
      "dense_345/bias:0: Tensor(\"Mean_25:0\", shape=(), dtype=float32)\n",
      "dense_346/kernel:0: Tensor(\"Mean_26:0\", shape=(), dtype=float32)\n",
      "dense_346/bias:0: Tensor(\"Mean_27:0\", shape=(), dtype=float32)\n",
      "dense_347/kernel:0: Tensor(\"Mean_28:0\", shape=(), dtype=float32)\n",
      "dense_347/bias:0: Tensor(\"Mean_29:0\", shape=(), dtype=float32)\n",
      "Discriminator Gradients:\n",
      "dense_341/kernel:0: Tensor(\"Mean_16:0\", shape=(), dtype=float32)\n",
      "dense_341/bias:0: Tensor(\"Mean_17:0\", shape=(), dtype=float32)\n",
      "dense_342/kernel:0: Tensor(\"Mean_18:0\", shape=(), dtype=float32)\n",
      "dense_342/bias:0: Tensor(\"Mean_19:0\", shape=(), dtype=float32)\n",
      "dense_343/kernel:0: Tensor(\"Mean_20:0\", shape=(), dtype=float32)\n",
      "dense_343/bias:0: Tensor(\"Mean_21:0\", shape=(), dtype=float32)\n",
      "dense_344/kernel:0: Tensor(\"Mean_22:0\", shape=(), dtype=float32)\n",
      "dense_344/bias:0: Tensor(\"Mean_23:0\", shape=(), dtype=float32)\n",
      "Generator Gradients:\n",
      "dense_345/kernel:0: Tensor(\"Mean_24:0\", shape=(), dtype=float32)\n",
      "dense_345/bias:0: Tensor(\"Mean_25:0\", shape=(), dtype=float32)\n",
      "dense_346/kernel:0: Tensor(\"Mean_26:0\", shape=(), dtype=float32)\n",
      "dense_346/bias:0: Tensor(\"Mean_27:0\", shape=(), dtype=float32)\n",
      "dense_347/kernel:0: Tensor(\"Mean_28:0\", shape=(), dtype=float32)\n",
      "dense_347/bias:0: Tensor(\"Mean_29:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 21:55:49.620017: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 17s 87ms/step - d_loss: nan - g_loss: nan\n",
      "Epoch 2/500\n",
      "113/113 [==============================] - 6s 52ms/step - d_loss: nan - g_loss: nan\n",
      "Epoch 3/500\n",
      "113/113 [==============================] - 6s 52ms/step - d_loss: nan - g_loss: nan\n",
      "Epoch 4/500\n",
      "113/113 [==============================] - 6s 56ms/step - d_loss: nan - g_loss: nan\n",
      "Epoch 5/500\n",
      " 42/113 [==========>...................] - ETA: 3s - d_loss: nan - g_loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mgan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m52\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/venv-metal/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = gan_model.fit(X_train, epochs=500, batch_size=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.suptitle('Loss')\n",
    "pyplot.plot(hist.history['d_loss'], label='d_loss')\n",
    "pyplot.plot(hist.history['g_loss'], label='g_loss')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the generator from the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = gan_model.get_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print resulting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = generate_real_samples(X_train=X_train, n_samples=64)\n",
    "X_fake, y_fake = generate_fake_samples(generator=generator, latent_dim=100, n_samples=64)\n",
    "for i in range(64):\n",
    "    print(\"real image from dataset\")\n",
    "    pyplot.imshow(X_real[i])\n",
    "    pyplot.show()\n",
    "    print(\"generated image of number\")\n",
    "    pyplot.imshow(X_fake[i])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_real, y_real = generate_real_samples(X_train=X_train, n_samples=1000)\n",
    "X_fake, y_fake = generate_fake_samples(generator=generator, latent_dim=100, n_samples=1000)\n",
    "\n",
    "def printOB(OB):\n",
    "    print(\"ask: \", OB[0][0][0], \" \", OB[0][1][0])\n",
    "    print(\"bid: \", OB[1][0][0], \" \", OB[1][1][0])\n",
    "    print()\n",
    "    \n",
    "asks_fake = X_fake[:, 0, 0, 0]\n",
    "asks_real = X_real[:, 0, 0, 0]\n",
    "bids_fake = X_fake[:, 1, 0, 0]\n",
    "bids_real = X_real[:, 1, 0, 0]\n",
    "asks_quantity_real = X_real[:, 0, 1, 0]\n",
    "bids_quantity_real = X_real[:, 1, 1, 0]\n",
    "\n",
    "positions = [1, 2, 3, 4]  # X-coordinates for the boxplots\n",
    "plt.boxplot([asks_fake, asks_real, bids_fake, bids_real], positions=positions, labels=['fake asks', 'real asks', 'fake bids', 'real bids'])\n",
    "plt.show()\n",
    "positions = [1, 2]\n",
    "plt.boxplot([asks_real, bids_real], positions=positions, labels=['real asks', 'real bids'])\n",
    "plt.show()\n",
    "plt.boxplot([asks_quantity_real, bids_quantity_real], positions=positions, labels=['real asks quantity', 'real bids quantity'])\n",
    "plt.show()\n",
    "plt.hist(asks_real)\n",
    "plt.show()\n",
    "plt.hist(bids_real)\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(X_fake)):\n",
    "    print(\"fake\")\n",
    "    printOB(X_fake[i])\n",
    "    print(\"real\")\n",
    "    printOB(X_real[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming X is your 4-dimensional array with shape (1000, 2, 2, 1)\n",
    "# Replace this with your actual data\n",
    "X = np.random.rand(1000, 2, 2, 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "for i in range(len(X)):\n",
    "    ascolumns = X[i].reshape(4,1)\n",
    "    t = scaler.fit_transform(ascolumns)\n",
    "    transformed = t.reshape((2,2,1))\n",
    "    X[i] = transformed\n",
    "    \n",
    "transformed = scaler.fit_transform(X_train[:, 0, 0, 0].reshape(-1,1))\n",
    "original_restored = scaler.inverse_transform(transformed)\n",
    "\n",
    "print(X_train[:, 0, 0, 0])\n",
    "print(transformed.flatten())\n",
    "print(original_restored.flatten())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
